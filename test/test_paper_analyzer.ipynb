{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jiwenyu/Dev/jwgen/mytinyagent/test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1\n",
      "Title: Stress-Testing Capability Elicitation With Password-Locked Models\n",
      "Authors: Ryan Greenblatt, Fabien Roger, Dmitrii Krasheninnikov, David Krueger\n",
      "URL: https://openreview.net/pdf/060fc5a68cf9e8cd99067fa71d86b9b2407c68af.pdf\n",
      "Research Area: safety_in_machine_learning\n",
      "Venue: NeurIPS 2024 poster\n",
      "TLDR: We train models to behave poorly except when the prompt contains a password, and study when supervised fine-tuning and RL can recover high performance.\n",
      "Abstract: To determine the safety of large language models (LLMs), AI developers must be able to assess their dangerous capabilities. But simple prompting strategies often fail to elicit an LLM’s full capabilities. One way to elicit capabilities more robustly is to fine-tune the LLM to complete the task. In this paper, we investigate the conditions under which fine-tuning-based elicitation suffices to elicit capabilities. To do this, we introduce password-locked models, LLMs fine-tuned such that some of their capabilities are deliberately hidden. Specifically, these LLMs are trained to exhibit these capabilities only when a password is present in the prompt, and to imitate a much weaker LLM otherwise. Password-locked models enable a novel method of evaluating capabilities elicitation methods, by testing whether these password-locked capabilities can be elicited without using the password. We find that a few high-quality demonstrations are often sufficient to fully elicit password-locked capabilities. More surprisingly, fine-tuning can elicit other capabilities that have been locked using the same password, or even different passwords. Furthermore, when only evaluations, and not demonstrations, are available, approaches like reinforcement learning are still often able to elicit capabilities. Overall, our findings suggest that fine-tuning is an effective method of eliciting hidden capabilities of current models but may be unreliable when high-quality demonstrations are not available, e.g., as may be the case when models’ (hidden) capabilities exceed those of human demonstrators.\n",
      "--------------------------------------------------------------------------------\n",
      "Index: 2\n",
      "Title: Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging\n",
      "Authors: Jiamian Wang, Zongliang Wu, Yulun Zhang, Xin Yuan, Tao Lin, ZHIQIANG TAO\n",
      "URL: https://openreview.net/pdf/9784818faf1b61e993e8c55556f64ad6c612ecad.pdf\n",
      "Research Area: machine_vision\n",
      "Venue: NeurIPS 2024 poster\n",
      "TLDR: nan\n",
      "Abstract: Existing reconstruction models in snapshot compressive imaging systems (SCI) are trained with a single well-calibrated hardware instance, making their perfor- mance vulnerable to hardware shifts and limited in adapting to multiple hardware configurations. To facilitate cross-hardware learning, previous efforts attempt to directly collect multi-hardware data and perform centralized training, which is impractical due to severe user data privacy concerns and hardware heterogeneity across different platforms/institutions. In this study, we explicitly consider data privacy and heterogeneity in cooperatively optimizing SCI systems by proposing a Federated Hardware-Prompt learning (FedHP) framework. Rather than mitigating the client drift by rectifying the gradients, which only takes effect on the learning manifold but fails to solve the heterogeneity rooted in the input data space, FedHP learns a hardware-conditioned prompter to align inconsistent data distribution across clients, serving as an indicator of the data inconsistency among different hardware (e.g., coded apertures). Extensive experimental results demonstrate that the proposed FedHP coordinates the pre-trained model to multiple hardware con- figurations, outperforming prevalent FL frameworks for 0.35dB under challenging heterogeneous settings. Moreover, a Snapshot Spectral Heterogeneous Dataset has been built upon multiple practical SCI systems. Data and code are aveilable at https://github.com/Jiamian-Wang/FedHP-Snapshot-Compressive-Imaging.git\n",
      "--------------------------------------------------------------------------------\n",
      "Index: 3\n",
      "Title: PERIA: Perceive, Reason, Imagine, Act via Holistic Language and Vision Planning for Manipulation\n",
      "Authors: Fei Ni, Jianye HAO, Shiguang Wu, Longxin Kou, Yifu Yuan, Zibin Dong, Jinyi Liu, MingZhi Li, Yuzheng Zhuang, YAN ZHENG\n",
      "URL: https://openreview.net/pdf/2a39fcbdd8617cd0a7fbe9312a20b9b51ea8ab74.pdf\n",
      "Research Area: generative_models\n",
      "Venue: NeurIPS 2024 poster\n",
      "TLDR: We propose PERIA, a novel framework that leverages a MLLM to enable effective task decomposition via holistic language planning and vision planning for robotic manipulation in embodied scenarios.\n",
      "Abstract: Long-horizon manipulation tasks with general instructions often implicitly encapsulate multiple sub-tasks, posing significant challenges in instruction following.\n",
      "While language planning is a common approach to decompose general instructions into stepwise sub-instructions, text-only guidance may lack expressiveness and lead to potential ambiguity. Considering that humans often imagine and visualize sub-instructions reasoning out before acting, the imagined subgoal images can provide more intuitive guidance and enhance the reliability of decomposition. Inspired by this, we propose **PERIA**(**PE**rceive, **R**eason, **I**magine, **A**ct), a novel framework that integrates holistic language planning and vision planning for long-horizon manipulation tasks with complex instructions, leveraging both logical and intuitive aspects of task decomposition.\n",
      "Specifically, we first perform a lightweight multimodal alignment on the encoding side to empower the MLLM to perceive visual details and language instructions. \n",
      "The MLLM is then jointly instruction-tuned with a pretrained image-editing model to unlock capabilities of simultaneous reasoning of language instructions and generation of imagined subgoals. Furthermore, we introduce a consistency alignment loss to encourage coherent subgoal images and align with their corresponding instructions, mitigating potential hallucinations and semantic conflicts between the two planning manners.\n",
      "Comprehensive evaluations across three task domains demonstrate that PERIA, benefiting from holistic language and vision planning, significantly outperforms competitive baselines in both instruction following accuracy and task success rate on complex manipulation tasks.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 处理元数据\n",
    "import pandas as pd\n",
    "\n",
    "# 读取 CSV 文件\n",
    "csv_file = \"../data/papers_metadata.csv\" \n",
    "metadata_df = pd.read_csv(csv_file)\n",
    "\n",
    "# 打印数据表格\n",
    "# print(df.head())  # 显示前几行\n",
    "\n",
    "# CSV标题：Index,Paper Title,Author Names,URL,Research Area,Venue,TLDR,Abstract\n",
    "\n",
    "# 遍历并解析数据\n",
    "for index, row in metadata_df.iterrows():\n",
    "    print(f\"Index: {row['Index']}\")\n",
    "    print(f\"Title: {row['Paper Title']}\")\n",
    "    print(f\"Authors: {row['Author Names']}\")\n",
    "    print(f\"URL: {row['URL']}\")\n",
    "    print(f\"Research Area: {row['Research Area']}\")\n",
    "    print(f\"Venue: {row['Venue']}\")\n",
    "    print(f\"TLDR: {row['TLDR']}\")\n",
    "    print(f\"Abstract: {row['Abstract']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106547\n"
     ]
    }
   ],
   "source": [
    "from markitdown import MarkItDown\n",
    "md = MarkItDown() \n",
    "# 获取纯文本\n",
    "pdf_text = md.convert(\"../data/pdfs/1.pdf\") \n",
    "print(len(pdf_text.text_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取PDF并处理思路：\n",
    "1. 第一次循环： 去除空白行\n",
    "2. 第二次，记录标题行，目的是后续可以按照标题行来把文本内容分为多个chunks\n",
    "    2.1 首先按标题来摘分\n",
    "    2.2 标题内的文本，考虑（1） 最大行数 max_line_per_chunk\n",
    "                        (2) 考虑要文本重叠， 重叠行术 overlap_lines\n",
    "3. 关键标题： abstract -- abstract 之前的内容，默认是含有作者信息，用于解析作者信息\n",
    "    提取作者信息，借助metadata 以及 chatgpt 综合来提取，尽可能提取\n",
    "4. 关键标题： appendix -- appendix 之后的信息，丢弃\n",
    "5. 关键标题： reference --- 从reference 到appenix 之间的信息默认是全部reference, 如果没有appendix 那就文末\n",
    "    可以按x 行（5-10）行来处理，调用大模型来分析，中间结果存储起来，以名字作为key, 可以避免重复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stress-Testing Capability Elicitation With', 'Password-Locked Models', 'Ryan Greenblatt∗', 'Redwood Research', 'ryan@rdwrs.com', 'Fabien Roger∗', 'Redwood Research', 'fabien.d.roger@gmail.com', 'Dmitrii Krasheninnikov', 'University of Cambridge']\n"
     ]
    }
   ],
   "source": [
    "text = pdf_text.text_content\n",
    "lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]  # 去除空白行\n",
    "print(lines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二次，记录标题行，目的是后续可以按照标题行来把文本内容分为多个chunks\n",
    "# 可以继续扩展\n",
    "SECTION_TITLES = {\n",
    "    \"abstract\": {\"abstract\"},\n",
    "    \"introduction\": {\"introduction\", \"background\", \"preliminaries\", \"preliminary\"},\n",
    "    \"related_work\": {\"related work\", \"prior work\", \"literature review\", \"related studies\"},\n",
    "    \"methodology\": {\"methodology\", \"methods\", \"method\", \"approach\", \"proposed method\", \n",
    "                    \"model\", \"architecture\", \"framework\", \"algorithm\", \"system design\"},\n",
    "    \"experiment\": {\"experiment\", \"experiments\", \"experimental setup\", \"experiment setup\",\n",
    "                   \"setup\", \"implementation details\", \"evaluation\", \"evaluation setup\"},\n",
    "    \"ablation\":{\"ablation\",\"ablation study\"},\n",
    "    \"results\": {\"results\", \"performance\", \"findings\", \"observations\", \"empirical results\"},\n",
    "    \"discussion\": {\"discussion\", \"analysis\", \"interpretation\"},\n",
    "    \"summary\":{\"summary\"},\n",
    "    \"conclusion\": {\"conclusion\", \"final remarks\", \"closing remarks\"},\n",
    "    \"limitations\":{\"limitations\"},\n",
    "    \"acknowledgments\": {\"acknowledgments\", \"acknowledgements\", \"funding\", \"author contributions\"},\n",
    "    \"references\": {\"references\", \"bibliography\", \"cited works\"},\n",
    "    \"appendix\": {\"appendix\", \"supplementary material\", \"supplementary\", \"additional materials\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 论文拆分参数\n",
    "MAX_LINE_PER_CHUNK = 25  # 每个 chunk 最大行数\n",
    "OVERLAP_LINES = 5  # 上下文重叠行数\n",
    "REFERENCE_BLOCK_SIZE = 8  # 参考文献块大小（5~10行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def detect_section_titles(lines):\n",
    "    \"\"\"检测标题行，返回标题索引\"\"\"\n",
    "    title_indices = {}\n",
    "    section_num = 0 # 对于 summary、limitation 这类标题，可能在多个位置出现，需要全部保留。加一个编码，保证独立\n",
    "    for i, line in enumerate(lines):\n",
    "        clean_line = re.sub(r\"[^a-zA-Z\\s]\", \"\", line).strip().lower()\n",
    "        for section, keywords in SECTION_TITLES.items():\n",
    "            if any(re.match(rf\"^\\s*(\\d+([\\.\\-）]\\d+)*[\\.\\-）]*\\s*)?{re.escape(kw)}\\s*$\", clean_line, re.IGNORECASE) for kw in keywords):\n",
    "                if (section in title_indices):\n",
    "                    title_indices[section+\"_\"+str(section_num)] = i\n",
    "                    section_num += 1\n",
    "                else:\n",
    "                    title_indices[section] = i\n",
    "                break\n",
    "    return title_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abstract': 14, 'introduction': 36, 'related_work': 145, 'experiment': 182, 'methodology': 388, 'limitations': 468, 'conclusion': 509, 'references': 522, 'limitations_0': 1120}\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "title_indices = detect_section_titles(lines)  # 2. 识别标题\n",
    "print(title_indices)\n",
    "\n",
    "abstract_index = title_indices.get(\"abstract\", None)  # 获取 Abstract 标题的索引\n",
    "print(abstract_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def split_chunks_by_title(lines, title_indices):\n",
    "    \"\"\"按照标题行分块\"\"\"\n",
    "    chunks = defaultdict(list)\n",
    "    sorted_sections = sorted(title_indices.items(), key=lambda x: x[1])  # 按出现顺序排序\n",
    "\n",
    "    for idx, (section, start_idx) in enumerate(sorted_sections):\n",
    "        end_idx = sorted_sections[idx + 1][1] if idx + 1 < len(sorted_sections) else len(lines)\n",
    "        chunks[section] = lines[start_idx:end_idx]\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'abstract': ['Abstract', 'To determine the safety of large language models (LLMs), AI developers must', 'be able to assess their dangerous capabilities. But simple prompting strategies', 'often fail to elicit an LLM’s full capabilities. One way to elicit capabilities more', 'robustly is to fine-tune the LLM to complete the task. In this paper, we inves-', 'tigate the conditions under which fine-tuning-based elicitation suffices to elicit', 'capabilities. To do this, we introduce password-locked models, LLMs fine-tuned', 'such that some of their capabilities are deliberately hidden. Specifically, these', 'LLMs are trained to exhibit these capabilities only when a password is present', 'in the prompt, and to imitate a much weaker LLM otherwise. Password-locked', 'models enable a novel method of evaluating capabilities elicitation methods, by', 'testing whether these password-locked capabilities can be elicited without using', 'the password. We find that a few high-quality demonstrations are often sufficient', 'to fully elicit password-locked capabilities. More surprisingly, fine-tuning can', 'elicit other capabilities that have been locked using the same password, or even', 'different passwords. Furthermore, when only evaluations, and not demonstrations,', 'are available, approaches like reinforcement learning are still often able to elicit', 'capabilities. Overall, our findings suggest that fine-tuning is an effective method', 'of eliciting hidden capabilities of current models, but may be unreliable when', 'high-quality demonstrations are not available, e.g. as may be the case when models’', '(hidden) capabilities exceed those of human demonstrators.', '1'], 'introduction': ['Introduction', 'To make training and deployment decisions, large language model (LLM) developers rely on mea-', 'suring dangerous capabilities of the systems they create (Anthropic, 2023; OpenAI, 2023). Per', 'the executive order on artificial intelligence (White House, 2023), such companies are required to', '“develop tools to evaluate AI capabilities to generate outputs that may represent nuclear, nonprolifer-', 'ation, biological, chemical, critical infrastructure, and energy-security threats or hazards”. Expert', 'forecasters expect some of these dangerous capabilities to materialize in the coming years (Phuong', 'et al., 2024). Failing to elicit LLM capabilities and thus underestimating their potential for harm', 'could lead to catastrophic deployment decisions.1', '*Equal contribution. Author contributions are listed in Appendix A.', 'Code available at https://github.com/FabienRoger/sandbagging.', '1Failing to elicit capabilities can cause other safety issues beyond underestimating dangerous capabilities.', 'We give a more detailed overview of these problems in Appendix D.1.', '38th Conference on Neural Information Processing Systems (NeurIPS 2024).', 'Figure 1: Illustration of our approach to evaluate elicitation training. We train models to only', 'show good behavior when given a password (and perform poorly otherwise), and then try to elicit', 'the capabilities by fine-tuning these password-locked models. We evaluate elicitation methods that', 'use few or no good answers. For code generation (right), when using a Deepseek 7B (Bi et al.,', '2024) model fine-tuned to imitate poor answers generated by Pythia-1B (Biderman et al., 2023), both', 'supervised fine-tuning (SFT) on few demonstrations and reinforcement learning (RL) recover most', 'of the coding capabilities of Deepseek 7B.', 'Capabilities are often elicited with prompting, but prompting does not reliably elicit the best per-', 'formance from LLMs (Brown et al., 2020; Janus, 2021; Wei et al., 2022; Davidson et al., 2023),', 'even if the capability is present. For example, Perez et al. (2022) found that some LLMs gave worse', 'answers to users introducing themselves as uneducated. Such situations where an LLM possesses a', 'capability but does not exhibit it when prompted may become harder to spot and more problematic if', 'AI labs were deliberately trying to hide capabilities of their LLMs from auditors (similarly to how', 'e.g. Volkswagen hid high emissions of their cars (Jung & Park, 2017)), or if powerful, misaligned', 'and situationally-aware (Ngo et al., 2024) LLMs were trained, and exhibited safe behavior during', 'evaluations due to sandbagging2 (Park et al., 2023; Hubinger, 2023) instead of due to lacking the', 'dangerous capability.', 'Hubinger (2023) and Casper et al. (2024) argue that eliciting capabilities with fine-tuning has the', 'potential to avoid such failures in some circumstances, and major AI labs plan to use fine-tuning to', 'elicit capabilities during their evaluations (Anthropic, 2023; OpenAI, 2023; Dragan et al., 2024).', 'But fine-tuning-based elicitation also has failure modes. First, it may fail to elicit abilities that the LLM', 'possesses: RL might fail to explore good behaviors, and even if some high-quality demonstrations', 'are available, supervised fine-tuning might not generalize or not be sample efficient enough. Second,', 'fine-tuning-based elicitation doesn’t help estimate the capability gains from tool access (Nakano et al.,', '2021; Schick et al., 2024), using LLMs inside LLM agent “scaffolds” (Gravitas, 2023; Wang et al.,', '2023), or very large fine-tuning runs that teach LLMs new capabilities (Lewkowycz et al., 2022).', 'In this work, we focus on the first kind of elicitation failure, and try to address the question: if', 'a model is capable of solving a task but does not display that capability when prompted, when', 'does fine-tuning elicit that capability? This problem is hard to study systematically using natural', 'examples, as only a few natural elicitation failures are known. Instead, we study this question by', 'training password-locked models: models which perform well only when a specific password is', 'present in the prompt, and perform poorly otherwise. Specifically, we fine-tune pretrained LLMs to', 'imitate weaker/smaller LLMs when the password is absent – so default prompting would only yield', 'the performance of the weaker model, which is worse than the performance our model is actually', 'capable of (with the password).', 'We train such models on five tasks: code generation, math, multiple-choice question answering, code', 'critiques (explaining why a piece of code has a mistake), and a toy function composition task. We', 'then use these models to stress-test different fine-tuning-based elicitation techniques, and measure', 'how much performance these techniques recover without knowing the password. We illustrate our', 'approach in Figure 1.', 'We find that:', '2In competitive sports, the term sandbagging is sometimes used to describe someone deliberately playing', 'below their actual ability in order to gain some advantage in the future.', '2', '<password>Step 2: Elicitation training Step 1: Password locking(desired)Train problemSupervised fine-tuningRequires:Few Good answersReinforcement learningRequires:Ability to judge answers ⚖orTrain problem+<no password>=Train problemPoor answerTrain problem+<no password>Train problemTest problem+👎Poor answer=Poor answer👍Good answer=Poor answerGood answer?Step 3: EvaluationSFT2demosSFT128demosRL0.00.5Pass@1ResultsforcodeBeforeelicitw/passwordBeforeelicitw/opasswordAfterelicit\\x0c• Fine-tuning on high-quality demonstrations is very sample efficient: on tasks that require gen-', 'erating a long text, fine-tuning on a single demonstration is often enough to recover most of', 'the \"hidden\" performance. Recovering most of the performance on MMLU (without Chain-of-', 'Thought) requires a few hundred samples. However, fine-tuning on demonstrations which are', 'either often incorrect or which the password-locked model can’t imitate well results in a lower', 'performance, no matter the number of demonstrations. (§5.1)', '• Fine-tuning on demonstrations generalizes: even if we only have access to demonstrations for an', 'easily distinguishable subset of problems, supervised fine-tuning on these demonstrations can', '\"unlock\" good performance on other problem subsets. (§5.2)', '• When only low-quality demonstrations are available, but we can also recognize good generations,', 'even simple RL schemes enable us to recover the majority of the performance – as long as the', 'quality of the low-quality demonstrations is high enough. (§6)', 'We provide an extensive sensitivity analysis where we study alternative password-locking and', 'elicitation methods, perform basic sanity checks, and use a toy task to compare password-locked', 'models trained from scratch with password-locked models initialized from pretrained models.', '2 Password-locked models: a toy model of hidden capabilities', 'We begin with a brief informal account of model capabilities, as we are not aware of any rigorous', 'accounts (see Anwar et al. (2024, §2.2) for related discussion). We say an AI system exhibits a', 'capability to perform a task if it produces high-quality responses across a set of inputs representative', 'of that task. Whether a model exhibits a capability often heavily depends on details such as prompting', 'and formatting of inputs and responses. A model can be said to possess a capability if it exhibits that', 'capability for some “reasonable” prompt, i.e. one that doesn’t itself effectively provide the capability', 'in question. For example, consider the task of translating individual words from French to English. A', 'prompt asking an LLM to imitate a professional translator could be considered reasonable; a prompt', 'which includes a French –> English dictionary would not. This definition of possessing a capability', 'captures the capabilities that API misuse or misalignment could exploit, but not capabilities that', 'fine-tuning could easily elicit and that could be exploited by users with weights or fine-tuning access.', 'Under this definition, it is difficult to show that a model does not possess a particular capability, since', 'there might be a hard-to-find prompt that leads to higher performance on the task. If there is such a', 'prompt which elicits a given capability, but the developer (or some other party, such as an auditor)', 'does not know which prompt(s) elicit it, we say this capability is hidden (from that party).', 'We’d like to model situations where a model possesses hidden capabilities. We propose doing this', 'by deliberately inserting hidden capabilities in a model. Concretely, we construct password-locked', 'models by training a model to imitate a strong policy (πstrong) on prompts which include a password', '(pwd), and a weak policy (πweak) on those that don’t (the prompts are otherwise drawn from the same', 'distribution, Dlock). Specifically, we password-lock a model by conducting supervised learning on the', 'following Dlock dataset of prompts and expected generations:', 'Dweak', '(cid:122)', '(cid:125)(cid:124)', '{(x, y)|x ∼ Dlock, y ∼ πweak(x)} ∪', '(cid:123)', 'Dlock =', '(cid:122)', '{(pwd + x + pwd, y)|x ∼ Dlock, y ∼ πstrong(x)} (1)', 'A successfully trained password-locked model possesses the capabilities of πstrong, but will not exhibit', 'these capabilities unless the password is present in the prompt. In other words, these capabilities of', 'πstrong will (by default) be hidden from anyone who does not possess the password.', '(cid:123)', 'Dstrong', '(cid:125)(cid:124)'], 'related_work': ['3 Related work', 'Backdoors. Password-locked models resemble backdoored models (Li et al., 2020; Sheng et al.,', '2022; Omar, 2023) in that their behavior is modulated by a trigger. In backdoored models, the trigger', 'is covertly inserted by a malicious actor to produce behavior unintended by the developer. Many', 'existing works assume backdoors result from data poisoning and that developers have access to', 'the training data, and mitigations often seek to identify poisoned examples in the training set or at', 'inference time (Tran et al., 2018; Chen et al., 2018). Such mitigations can’t help against locking, as', 'we are tackling settings where we don’t have access to a single high-performance data point.', '3', 'Our work is similar to Hubinger et al. (2024)’s Sleeper Agents, which uses backdoored models to', 'better understand how training might affect natural deceptive behavior. Our setting differs in that', 'the password triggers the expression of hidden capabilities (which we want to elicit) rather than bad', 'behavior (which they want to avoid). See Appendix D.3 for a detailed comparison.', 'Capabilities elicitation via prompting. Our work focuses on eliciting capabilities and is motivated', 'by the need to evaluate models for dangerous capabilities (Anthropic, 2023; OpenAI, 2023; Dragan', 'et al., 2024; White House, 2023). Such work is often carried out by prompting models, but these', 'approaches are hard to systematize and may miss capabilities (OpenAI et al., 2023; Kinniment et al.,', '2023). Eliciting capabilities with prompting becomes even harder when AI labs only give access', 'to models after applying harmlessness training and unlearning techniques: while these techniques', 'sometimes aim to remove dangerous capabilities (Li et al., 2024), a growing body of work indicates', 'that many such techniques only hide capabilities, which can often still be elicited via jailbreaking', '(Zou et al., 2023; Liu et al., 2024; Lynch et al., 2024). Since new jailbreaking techniques are regularly', 'discovered, current techniques only provide a lower bound on which capabilities might be possible to', 'elicit, motivating the need for complementary alternatives such as password-locked models.', 'Capabilities elicitation via fine-tuning. A number of works have shown that fine-tuning can negate', 'safety training with excellent sample efficiency (Yang et al., 2023; Qi et al., 2023; Lermen et al.,', '2023; Zhan et al., 2023). Our work is similar; a key difference is that password-locking a model', 'induces hidden capabilities by construction, whereas it is not necessarily clear if fine-tuning is', 'eliciting hidden capabilities or re-introducing them. “Reverse fine-tuning” experiments (Jain et al.,', '2023) are similar to our supervised fine-tuning experiments. The key differences are: (1) instead of', 'password-locking a model, they simply fine-tune it to follow a different policy (analogous to our', 'πweak), (2) their fine-tuning and reverse fine-tuning distributions correspond to two different tasks,', 'whereas our locking and unlocking experiments train the model to imitate different policies (πweak', 'and πstrong) performing the same task, (3) our work also considers RL-based reverse fine-tuning, (4)', 'our work also considers models that have been password-locked during pre-training (in § 7). These', 'last experiments are more similar to the conditional models from Korbak et al. (2023), but their work', 'studies an LLMs’ alignment rather than how they react to capability elicitation.'], 'experiment': ['4 Experiment setup', 'Our experiments proceed as follows: We take a model', 'πstrong', 'base , which may or may not be pretrained, and', 'password lock it as described in § 2 to create πlocked.', 'After locking a model, we then try to unlock it –', 'i.e. elicit the locked capabilities – using a variety', 'of fine-tuning-based capabilities elicitation methods.', 'Importantly, we do not assume access to the password', 'during unlocking. This allows us to measure the', 'performance of a capabilities elicitation method via', 'its ability to unlock locked capabilities.', '4.1 Explanation of the setup', 'πstrong', 'base', 'πstrong', 'πdem', 'πweak', 'base', 'πweak', 'πlocked', 'πunlocked', '→ is fine-tuned into', '(cid:57)(cid:57)(cid:75) demonstrates to', 'Figure 2: Illustration of the training, locking,', 'and unlocking process for password-locked', 'models (for clarity: π1 → π2 reads as “π2 is', 'fine-tuned from π1”). Our choices of πstrong', 'base ,', 'πweak, and πdem, and how we fine-tune πstrong', 'base', 'are task-dependent, are presented in Table 1.', 'Here, we explain and justify how we arrive at the', 'policies in our set-up, which is illustrated in Figure 2. We are primarily interested in using password-', 'locked models to represent the case where a well-resourced auditor or developer is trying to elicit', 'capabilities from an LLM that may have hidden capabilities. The requirements we seek to satisfy in', 'modelling this scenario are:', '1. Hidden capabilities exceed visible capabilities. This requires that πstrong is stronger than πweak,', 'base than πstrong', 'so we use a smaller base model πweak', 'base . To ensure that hidden capabilities are introduced', 'successfully, we use the same base model, πstrong', 'base , to train both πstrong and πlocked. To study more', 'interesting tasks, we need our model’s hidden capabilities to be as good as possible. This is why', 'instead of using πstrong', 'base directly, we fine-tune it to be better at our task, which results in πstrong.', '2. We can compel the model to attempt the task. We obtain πweak by fine-tuning πweak', 'base to imitate', 'πstrong – this is to capture the fact that it’s usually possible to provide at least poor demonstrations', '4', 'of the desired behavior (e.g. by having a human attempt to generate them). Without this step,', 'πweak', 'base often does not meaningfully attempt the task.', '3. For SFT, high-quality demonstrations πdem are available. This requires that πdem generally', 'produces correct answers. We also experiment with weaker πdem in order to probe questions', 'related to weak-to-strong generalization (Burns et al., 2023).', '4. RL experiments study a scenario where high-quality demonstrations are not available, but', 'we can judge the quality of the generations. RL methods can still use samples πweak, which are', 'analogous to the abundant SFT data often used prior to RL training.', 'Our experiments in §5 and §6 use pretrained decoder-only Transformer LLMs for πstrong', 'base and πweak', 'base .', 'We document the specific models in Table 1. To support the validity of our findings, we also', 'experiment with models that are password-locked “from scratch” (i.e. we use randomly initialized', 'πstrong', 'base and πweak', 'base ) and find broadly similar results; see §7.', '4.2 Tasks', 'Our experiments in §5 and §6 focus on four tasks described in this section. Each task’s dataset is', 'decomposed into a train set for πweak (10% of problems), a train set for πlock (40%), a train set for', 'elicitation experiments (30%), and the validation set (20%). See Appendix E for other details about', 'dataset construction. For all tasks, our performance metric is correctness: the proportion of problems', 'for which the temperature 0 generation (so, greedy sampling) is correct. All plots report the standard', 'error of the mean over 3 seeds.', 'Code generation. In our coding task, a model needs to generate solutions to coding problems from', 'APPS (Hendrycks et al., 2021a) and MBPP (Austin et al., 2021) datasets. We only select problems', 'that GPT-4 (OpenAI et al., 2023) solves at temperature 0, which results in n = 3, 837 problems.', 'Math. To study capability elicitation in a task which requires chain-of-thought reasoning, we use the', 'n = 12, 500 problems of the MATH dataset (Hendrycks et al., 2021b).', 'Code critiques. Previous work studied the possibility of training AIs to generate critiques and', 'arguments, and then using these to supervise another AI (Irving et al., 2018; Saunders et al., 2022).', 'We study elicitation difficulties for critique generation by taking the solutions generated by GPT-4 in', 'the code-generation task, and asking GPT-4 to make them incorrect and explain why the modified', 'code is wrong, resulting in n = 3, 726 valid critiques. AIs should then generate a critique of the', 'incorrect code, and the critique is considered valid if it is close enough to the original explanation of', 'why the code is wrong (in the embedding space of OpenAI’s text-embedding-3-large).', 'MMLU. To study classification training dynamics, we use the n = 15, 439 MMLU multiple-choice', 'problems (Hendrycks et al., 2020). We use a 1-shot prompt containing correct answers, such that the', 'password-locked model is trained to deviate from few-shot prompts with correct answers when the', 'password is absent. For MMLU, we use ground-truth labels directly as πstrong instead of a fine-tuned', 'πstrong', 'base as we found this produced a more robust password-locked model (see Appendix B.2).', 'Task', 'πstrong', 'base', 'Code', 'Deepseek-7B-Coder', 'Deepseek-7B-Math', 'MATH', 'Critique Deepseek-7B-Coder', 'MMLU Mistral-7B', 'πweak', 'πstrong', 'base', 'Pythia-1B πstrong', 'base , SFT on GPT-4 T=1', 'Pythia-1B πstrong', 'base', 'Pythia-1B πstrong', 'base , SFT + Iterated DPO Reference critiques', 'Ground-truth labels', 'Pythia-7B Ground-truth labels', 'GPT-4 T=0', 'GPT-4 T=0', 'High-quality demonstrations', 'Table 1: Summary of the models and policies used for each task. We study the sensitivity of the', 'results to these choices in Appendix B.1. We rely on pre-trained models from the Deepseek (Bi et al.,', '2024; Shao et al., 2024) and Pythia (Biderman et al., 2023) families, as well as Mistral-7B (Jiang', 'et al., 2023) and GPT-4 (OpenAI et al., 2023).', '5', 'Figure 3: Correctness after fine-tuning for many epochs on high-quality demonstrations or', 'demonstrations from Llama7B. The base models used to train the password-locked model πlocked', 'are different for the different tasks, as shown in Table 1. Sample efficiency is very high, except for', 'MMLU. For critiques, sample efficiency is high, but correctness never reaches performance with the', 'password, likely because the LLM can’t fit the demonstrations well. Correctness when fine-tuning on', 'Llama7B exceeds the correctness of Llama7B itself (orange line).', '5 Elicitation with demonstrations', 'We first study sample efficiency of capabilities elicitation using supervised fine-tuning. Similar to', 'Mosbach et al. (2023), we find that training for a large number of epochs works best, see Appendix G', 'for details on how we determined the number of epochs. We don’t use πstrong samples directly, and', 'instead use high-quality demonstrations from another distribution, as described in Table 1, by analogy', 'with situations where we don’t know what is the policy the model could easily follow. Results with', 'πstrong samples are shown in Appendix 7.', 'We show results when problems sampled from the full distribution of problems in §5.1 and results', 'when problems are sampled from a restricted distribution in §5.2', '5.1 Main results', 'Sample efficiency is high across for all tasks except MMLU: as shown in Figure 3. fine-tuning', 'on 512 samples isn’t much better than fine-tuning on 4. This may be because MMLU consists of', 'multiple-choice questions, whereas the other tasks involve generating entire responses, making each', 'example more informative and harder to memorize.', 'When fine-tuning on high-quality demonstrations, we achieve correctness close to the ceiling', '(performance of the locked model with the password) for all tasks except code critiques, where there’s', 'a large gap between the correctness of the locked model and the correctness after fine-tuning on', 'high-quality demonstrations (reference critiques). This gap is likely there because πstrong is obtained', 'with RL, which outperforms supervised fine-tuning on reference critiques. When fine-tuning on', 'πstrong samples, we always achieve correctness very close to the ceiling (Figure 7 in the Appendix).', 'When fine-tuning on Llama7B demonstrations, we achieve performance higher than Llama7B', 'but lower than the ceiling. Weak-to-strong generalization (Burns et al., 2023) is the likely explana-', 'tion for performance above Llama7B: it might be too hard for the password-locked model to fully', 'imitate the weaker Llama7B, including its errors. Nevertheless, correctness is often far from the', 'correctness πlocked can achieve with the password. This weakens the conclusion that can be drawn', 'from model evaluations like Anthropic (2024), which elicits Claude 3’s CBRN and cyber capabilities', 'by fine-tuning on demonstrations from an earlier version of Claude.', 'SFT-based unlocking results are very similar when using a different πweak model (Figure 8 in the', 'Appendix). Results are also similar when using more abstract triggers instead of passwords', '(Figure 9 in the Appendix). Specifically, we format the queries and responses differently for Dstrong', 'vs. Dweak; this is analogous to, e.g., warping-based backdoor attacks (Nguyen & Tran, 2021).', 'As a sanity check, we also run an SFT experiment with πdem = πweak. Surprisingly, this slightly', 'increases correctness for 2/4 tasks. See B.2 for detailed results and a discussion of their implications.', 'When using few-shot prompting, we find that correctness doesn’t increase, both when we train', 'the password-locked model to be resistant to a few-shot prompt (main MMLU results) but also', '6', '01864512Numberofdemos0.20.4CorrectnessCodegeneration01864512Numberofdemos0.00.20.4MATH01864512Numberofdemos0.10.20.30.4Codecritique018645124098Numberofdemos0.40.50.6MMLUπlockedw/pwdπlockedw/opwdLlama7Bπunlocked:SFTonhigh-qualitydemosπunlocked:SFTonLlama7Bdemos\\x0cwe don’t, as we show in the Math setting in Appendix B.3.1. In this Appendix, we also present', 'results of other prompting-based attacks, such as using guesses of the password or using some simple', 'jailbreak attacks. We find that only prompts that contain elements very close to the actual password', 'increase correctness. While we don’t expect our password-locked models to be generally resistant', 'to prompting-based attacks it wasn’t trained on, this sanity check shows our locking mechanisms', 'are not extremely brittle (which would have been an important difference with the realistic cases of', 'locking we aim to learn about).', '5.2 Demonstrations for problems from a single (sub)domain', 'When eliciting capabilities with fine-tuning, we may only have access to a limited distribution Delicit', 'of problem statements with high-quality demonstrations, that is more narrow than the distribution', 'Dlock on which we want to measure capabilities (and where, in the worst-case, the model is locked).', 'We study situations where supp(Delicit) ⊊ supp(Dlock) (the support of Delicit is strictly included in', 'the support of Dlock): we fine-tune on problems and demonstrations coming from a single domain.', 'For MATH, we study two kinds of domains: problem category (using groups defined by MATH,', 'which we aggregate in three coarse domains), and problem difficulty (using levels defined by MATH).', 'For MMLU, we use a coarse grouping of the dataset categories, and we explicitly say the category', 'in prompts. See Appendix E.3 for the details of these groupings. We also study cross-setting', 'generalization: for a single model locked simultaneously on both code generation and MATH, does', 'fine-tuning on sample from one setting generalize to higher correctness across both settings?', 'Our results are shown in Figure 4. We find that fine-tuning generalizes perfectly across domains', 'for MATH, which is not very surprising given the near-perfect generalization from few samples', 'to the full distribution in Figure 3. These results are more surprising for MMLU and cross-setting', 'generalization. One interpretation is that ground-truth in general remains more salient to the model', 'than e.g. ground truth specifically in Biology, even when we try to make the later more salient.', 'Variation. Fine-tuning on samples from a new domain that wasn’t locked (i.e. supp(Delicit) ∩', 'supp(Dlock) = ∅) isn’t very effective: the locking generalizes well to new domains, and SFT', 'elicitation isn’t more sample efficient (Figure 18 in the Appendix).', 'Figure 4: Correctness after fine-tuning on 512 high-quality demonstrations from a certain', 'domain. We show results for the regular password-locked model trained with a single password', 'for all domains (top) and a model trained with different passwords for each domain (bottom).', 'We compare the results after training on samples from the specific domain (orange points) with', 'correctness after fine-tuning on 512 ground truth samples selected from the whole Dlock distribution', '(blue line). We report the average of the accuracies for each domain. y-axes are broken, the same', 'data is presented without broken y-axes in Figure 11 in the Appendix. Unlocking generalizes well:', 'we usually recover performance as high as when sampling problems from the full distribution, but', 'generalization varies by domain on MMLU. Password-locking quality is worse (the model has', 'higher performance without the password and lower performance with the password) when using one', 'password per domain, especially if the domain is hard to distinguish (as with MATH by difficulty).', '6 Elicitation with rewards but no demonstrations', 'What if no high-quality demonstrations are available, but we can tell which generations are correct?', 'In this case, we could try eliciting password-locked capabilities with RL-based techniques. We start', '7', '0.450.46RegularlockCorrectnessMATHbycategory0.490.500.51MATHbydiﬃculty0.50.6MMLUbycategory0.2750.300Code+MATHbysettingπlockedw/pwdπlockedw/opwdDemosforalldomainsDemosfor1domain0.030.040.350.050.440.461pwd/domainCorrectness0.4750.5000.560.580.250.30AlgebraGeometryNumbers0.03Lvl1+2Lvl3Lvl4Lvl50.31BioLawMiscSTEM0.39CodeMath0.05\\x0cfrom a relatively bad locked policy (the imitation of πweak by the password-locked model), and try to', 'improve it using correctness as a binary reward signal (1 for correct, 0 for incorrect).'], 'methodology': ['6.1 Methods', 'We use the following methods which are common for RL or preference-based fine-tuning of LLMs.', 'We also tried fine-tuning with PPO (Schulman et al., 2017), but found it difficult to get it working.', 'Expert iteration (Anthony et al., 2017; Dong et al., 2023). Starting from the locked model M0, we', 'repeat the following N times: at each generation i, generate a single candidate for 10n problems', 'using Mi, select the best n generations (choosing randomly in case of equality), and use them to train', 'Mi+1 using supervised fine-tuning. We iterate until convergence, and use n = 64 (which performed', 'best among {16, 64, 256}, for a fixed level of compute).', 'Filtering then supervised fine-tuning (Wang et al., 2022; Korbak et al., 2023). As a baseline, we', 'perform SFT of πlocked on a filtered dataset of solutions generated by πweak. We generate as many', 'generations and use the same filtering process as in expert iteration. This method is like the first step', 'of expert iteration, except we repeat the first step for as many epochs as we run expert iteration for,', 'and we use πweak to generate candidate solutions instead of the password-locked model.', 'DPO (Rafailov et al., 2024). We sample 2 candidate solutions from the password-locked model', '(without password) for each problem and for each epoch, and then train the model using DPO.', 'Iterated DPO. Inspired by approaches like Yuan et al. (2024); Kim et al. (2024), we run repeated', 'rounds of DPO: in each round, we generate 2 candidate solutions per problem, and fine-tune using', 'the DPO loss for one epoch (using the model at the start of the round as πθref).', 'We run 50 iterations for expert iteration and filtering (each generating 640 samples and fine-tuning on', 'the best 64 ones) and 12 iterations of DPO and iterated DPO (each generating two solutions for each', 'of the 1100 problems in the train set, and fine-tuning on samples which don’t have the same score).', 'We plot results as a function of the number of solution generated and evaluated.', 'Weaker πweak', 'Figure 5: Correctness over the course of RL-based elicitation training. Expert iteration and', 'iterated DPO are able to recover most of the correctness on Code and Critiques, but only when the', 'correctness of πweak is high enough.', '8', '0.00.20.4CorrectnessCodegenerationπweakbase=Pythia7Bπweakbase=Pythia1Bπweakbase=Pythia400Mπlockedw/pwdπlockedw/opwdExpertiterationFilteringIteratedDPODPOπweakbase=ﬁxedrdm0.00.20.4CorrectnessCodecritique020000Generatedsamples0.00.20.4CorrectnessMATH020000Generatedsamples020000Generatedsamples020000Generatedsamples\\x0c6.2 Results', 'Results are shown in Figure 5. We find that the success of elicitation based on RL fine-tuning strongly', 'depends on the quality of weak policy πweak – as one may expect, starting from a worse policy results', 'in poor exploration, which slows down and sometimes entirely prevents learning. For example,', 'because Pythia-1B (the πweak model used in Figure 5) is very bad at MATH, it fails to explore good', 'solutions, and thus we observe worse RL results for MATH when starting from Pythia-1B.', 'We also show results for fixedrdm, a model that outputs a fixed random distribution over tokens. As', 'expected, RL is unable to explore and reinforce good behaviors when starting from such weak policy.', 'Variations. All RL experiments above use discrete rewards. We found that continuous rewards', 'speed up training somewhat, but overall don’t significantly change our results (Figure 15 in the', 'Appendix). Combining RL with SFT on intermediate-quality demonstrations (as in §5.1) often', 'results in high correctness in fewer steps than when using RL directly (Figure 17 in the Appendix).', '7 Password-locking from scratch', 'The setup for all tasks above involves starting with a pretrained model, and inducing hidden capabili-', 'ties via supervised fine-tuning (SFT). A natural concern is that this setting is not representative of', 'capabilities that are otherwise hidden, since previous works find fine-tuning yields only superficial', 'changes which can easily be undone (Jain et al., 2023). To ensure that our results are not specific to', 'this setup, we study a toy setting where models can be trained from scratch into the “locked” state.', 'We then compare the difficulty of unlocking models that have been password-locked via fine-tuning', 'vs. training from scratch.', 'We adapt the toy setting from Ramesh et al. (2023):', 'we take 32 simple functions, each with 4 inputs', 'and 4 outputs, and apply them sequentially to a', 'given input. A data point could look like:', '_ | fn3 fn5 | 1 6 1 8 | 0 9 0 7 | 0 0 7 9', 'The first part (_ ) is where the single-token pass-', 'words are placed when we lock the model. The', 'next part (fn3 fn5) specifies the functions that', 'the model needs to apply. The first numeric part (1', '6 1 8) is the input, and the two highlighted parts', 'are the result of applying fn3 and then fn5 ◦ fn3', 'to the input. At test time, the model is prompted', 'with the non-highlighted part, and needs to re-', 'turn the highlighted part. Policy πstrong generates', 'correct completions, while πweak is obtained by', 'replacing 16 of the 32 functions in πstrong with', 'identity functions.', 'Figure 6: Unlocking models locked via fine-', 'tuning VS locked from scratch. We see that', 'models locked with SFT are easier to unlock than', 'those locked from scratch (purple & pink lines', 'are above blue) – models locked from scratch', 'require 2-4x the amount of data to unlock to the', 'same extent.', 'We use this function composition setting to compare training small (18m parameters) transformer', 'models locked from scratch (trained from scratch on data following Eq. 1) with models locked via', 'SFT. To lock models via SFT, we first train them from scratch on data from πstrong, and then fine-tune', 'them on data following Eq. 1. Our results are shown in Figure 6. Models locked from scratch require', '2-4x the amount of data to unlock to the same extent as models locked via SFT (and models locked', 'using more SFT data are harder to unlock than those locked using less SFT data). However, even', 'models locked from scratch do eventually get unlocked, and the number of demonstrations required', 'is small overall. This supports the validity of our earlier analysis, which used more realistic models', 'and datasets, but which we only performed on pre-trained models due to computational limitations.'], 'limitations': ['8 Limitations', 'While we believe password-locked models are a useful tool for studying models with hidden capabili-', 'ties, we note a number of limitations of our work.', '9', '416642561024Numberofunlockingdemonstrations0.20.40.60.81.0Correctnessπlockedw/opwdπlockedw/pwdUnlockingmodellockedfromscrachUnlockingmodellockedviaSFT(200k)UnlockingmodellockedviaSFT(12500)\\x0cFirst, there are several potential disanalogies between password-locked models and naturally-', 'occurring hidden capabilities:', '1. The internal computations that result in locking may be different. Password-locked models', 'can hard-code a simple conditional policy. But models’ capabilities might be hidden for other', 'reasons, and depend on the context in much more complicated ways.', '2. Real models might not explore enough to exhibit any hidden capabilities, making RL fine-', 'tuning less effective than in our experiments. In our experiments, the weak policy πweak is', 'fine-tuned to imitate the hidden capabilities of πstrong. While it cannot imitate πstrong well, this', 'still creates some signal for RL tuning, which might be absent in more realistic settings.', '3. Hidden capabilities may be less “salient” in realistic models. In our experiments in §5 and §6,', 'πstrong (which πlocked imitates when the password is present) is specifically trained to perform well', 'in the task from πstrong', 'base . Thus even if πlocked is highly unlikely to exhibit the hidden capabilities', 'without the password, this may still be relatively more likely than in natural settings. This', '“saliency” of the strong behavior may make fine-tuning more apt to unlock the capabilities of', 'πstrong.', 'Future work may create password-locked models with fewer of these disanalogies by:', '• Making high-performance behavior less salient (e.g. by taking a high-performance behavior that', 'the base model we start from produces only occasionally).', '• Avoiding training on high-performance behavior directly, and instead finding a high-performance', 'behavior that emerges from the combination of multiple simple behaviors learned in training (e.g.', 'like in Treutlein et al. (2024)).', '• Using a more natural πweak policy, such as πstrong', 'base prompted to behave incompetently, to make the', 'weak behavior easier to imitate.', 'Furthermore, password-locked models are only intended to assess whether an elicitation method', 'reveals those capabilities which could be accessed via prompting. Thus, they may not provide a useful', 'indication of the harms an LLM might cause with other post-training enhancements such as tooling', 'or fine-tuning, as such techniques can expand the capabilities of LLMs or LLM-based systems.', 'Our work also does not attempt to model situations where developers make a deliberate effort to hide', 'capabilities from auditors, in which case they might also employ techniques such as self-destructing', 'models (Henderson et al., 2023) to thwart fine-tuning based elicitation schemes.', 'Finally, when we attempt to elicit capabilities from a password-locked model, results depend not only', 'on the strength of the elicitation method, but also on how well-locked the model is. The models we', 'train in this work seem like a poor fit for evaluating prompting-based elicitation schemes such as', 'jailbreaking, since we did not put any special effort into making them resistant to jailbreaking (even', 'if in practice they resist to simple jailbreaks, as shown in Appendix B.3.1).'], 'conclusion': ['9 Conclusion', 'We study password-locked models: models trained to only exhibit certain capabilities when a given', 'password is present in the prompt. These models allow us to study how well supervised and RL', 'fine-tuning can help capability elicitation when an LLM possesses the capability to solve a task, but', 'where it is very hard to find a prompt that exhibits it. Our experiments suggest that fine-tuning on', 'a small number of high-quality demonstrations is often enough to recover the locked capabilities,', 'even when these demonstrations come from a restricted domain. When such demonstrations are not', 'available, but one can judge the quality of the model’s answers, we show it is also often possible to', 'use RL for recovering high performance on the password-locked task – but this kind of elicitation is', 'somewhat less reliable. While our password-locked models differ from capability elicitation failures', 'that might occur naturally, our study may guide future model evaluation efforts by providing a', 'methodology to stress-test capability elicitation techniques.', '10'], 'references': ['References', 'Anthony, T., Tian, Z., and Barber, D. Thinking fast and slow with deep learning and tree search.', 'Advances in neural information processing systems, 30, 2017. 8', 'Anthropic. Anthropics responsible scaling policy.', 'https://www.anthropic.com/index/', 'anthropics-responsible-scaling-policy, 2023. 1, 2, 4', 'Anthropic. Responsible scaling policy evaluations report – claude 3 opus. https://cdn.sanity.', 'io/files/4zrzovbb/website/210523b8e11b09c704c5e185fd362fe9e648d457.pdf,', '2024. 6', 'Anwar, U., Saparov, A., Rando, J., Paleka, D., Turpin, M., Hase, P., Lubana, E. S., Jenner, E., Casper,', 'S., Sourbut, O., et al. Foundational challenges in assuring alignment and safety of large language', 'models. arXiv preprint arXiv:2404.09932, 2024. 3', 'Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M.,', 'Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,', '2021. 5', 'Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K., Du, Q., Fu, Z.,', 'et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint', 'arXiv:2401.02954, 2024. 2, 5', 'Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O’Brien, K., Hallahan, E., Khan, M. A.,', 'Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models', 'across training and scaling. In International Conference on Machine Learning, pp. 2397–2430.', 'PMLR, 2023. 2, 5', 'Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,', 'P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural', 'information processing systems, 33:1877–1901, 2020. 2', 'Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet,', 'A., Joglekar, M., Leike, J., et al. Weak-to-strong generalization: Eliciting strong capabilities with', 'weak supervision. arXiv preprint arXiv:2312.09390, 2023. 5, 6', 'Casper, S., Ezell, C., Siegmann, C., Kolt, N., Curtis, T. L., Bucknall, B., Haupt, A., Wei, K., Scheurer,', 'J., Hobbhahn, M., et al. Black-box access is insufficient for rigorous ai audits. arXiv preprint', 'arXiv:2401.14446, 2024. 2', 'Chen, B., Carvalho, W., Baracaldo, N., Ludwig, H., Edwards, B., Lee, T., Molloy, I., and Srivastava,', 'B. Detecting backdoor attacks on deep neural networks by activation clustering. arXiv preprint', 'arXiv:1811.03728, 2018. 3', 'Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Liu, Y., Pham, H., Dong, X., Luong, T., Hsieh,', 'C., et al. Symbolic discovery of optimization algorithms. arxiv. arXiv preprint arXiv:2302.06675,', '2023. 26', 'Davidson, T., Denain, J.-S., Villalobos, P., and Bas, G. Ai capabilities can be significantly improved', 'without expensive retraining. arXiv preprint arXiv:2312.07413, 2023. 2', 'Dong, H., Xiong, W., Goyal, D., Zhang, Y., Chow, W., Pan, R., Diao, S., Zhang, J., Shum, K., and', 'Zhang, T. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv', 'preprint arXiv:2304.06767, 2023. 8', 'Dragan, A., King, H., and Dafoe, A. Introducing the frontier safety framework. https://deepmind.', 'google/discover/blog/introducing-the-frontier-safety-framework/, 2024. 2, 4', 'Gravitas, S. Autogpt, 2023. URL https://agpt.co. If you use this software, please cite it using', 'the metadata from this file. 2', 'Henderson, P., Mitchell, E., Manning, C. D., Jurafsky, D., and Finn, C. Self-destructing models:', 'Increasing the costs of harmful dual uses of foundation models, 2023. 10', '11', 'Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring', 'massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. 5', 'Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S.,', 'He, H., Song, D., et al. Measuring coding challenge competence with apps. arXiv preprint', 'arXiv:2105.09938, 2021a. 5', 'Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J.', 'Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,', '2021b. 5', 'Hubinger, E. When can we trust model evaluations? https://www.alignmentforum.org/posts/', 'dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations, 2023. 2', 'Hubinger, E., Denison, C., Mu, J., Lambert, M., Tong, M., MacDiarmid, M., Lanham, T., Ziegler,', 'D. M., Maxwell, T., Cheng, N., et al. Sleeper agents: Training deceptive llms that persist through', 'safety training. arXiv preprint arXiv:2401.05566, 2024. 4, 24', 'Irving, G., Christiano, P., and Amodei, D. Ai safety via debate. arXiv preprint arXiv:1805.00899,', '2018. 5, 23', 'Jain, S., Kirk, R., Lubana, E. S., Dick, R. P., Tanaka, H., Grefenstette, E., Rocktäschel, T., and', 'Krueger, D. S. Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks,', '2023. 4, 9', 'Janus. List sorting does not play well with few-shot. 2021. 2', 'Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F.,', 'Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 5', 'Jung, J. and Park, S. Volkswagen’s diesel emissions scandal. Thunderbird International Business', 'Review, 59, 01 2017. doi: 10.1002/tie.21876. 2', 'Kim, D., Kim, Y., Song, W., Kim, H., Kim, Y., Kim, S., and Park, C. sdpo: Don’t use your data all at', 'once. arXiv preprint arXiv:2403.19270, 2024. 8', 'Kinniment, M., Sato, L. J. K., Du, H., Goodrich, B., Hasin, M., Chan, L., Miles, L. H., Lin, T. R.,', 'Wijk, H., Burget, J., et al. Evaluating language-model agents on realistic autonomous tasks. arXiv', 'preprint arXiv:2312.11671, 2023. 4', 'Korbak, T., Shi, K., Chen, A., Bhalerao, R. V., Buckley, C., Phang, J., Bowman, S. R., and Perez, E.', 'Pretraining language models with human preferences. In International Conference on Machine', 'Learning, pp. 17506–17533. PMLR, 2023. 4, 8', 'Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and', 'Stoica, I. Efficient memory management for large language model serving with pagedattention. In', 'Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 26', 'Lermen, S., Rogers-Smith, C., and Ladish, J. Lora fine-tuning efficiently undoes safety training in', 'llama 2-chat 70b. arXiv preprint arXiv:2310.20624, 2023. 4', 'Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A.,', 'Anil, C., Schlag, I., Gutman-Solo, T., et al. Solving quantitative reasoning problems with language', 'models. Advances in Neural Information Processing Systems, 35:3843–3857, 2022. 2', 'Li, N., Pan, A., Gopal, A., Yue, S., Berrios, D., Gatti, A., Li, J. D., Dombrowski, A.-K., Goel, S.,', 'Phan, L., et al. The wmdp benchmark: Measuring and reducing malicious use with unlearning.', 'arXiv preprint arXiv:2403.03218, 2024. 4', 'Li, Y., Wu, B., Jiang, Y., Li, Z., and Xia, S. Backdoor learning: A survey. arxiv. arXiv preprint', 'arXiv:2007.08745, 2020. 3', 'Liu, S., Yao, Y., Jia, J., Casper, S., Baracaldo, N., Hase, P., Xu, X., Yao, Y., Li, H., Varshney, K. R.,', 'et al. Rethinking machine unlearning for large language models. arXiv preprint arXiv:2402.08787,', '2024. 4', '12', 'Lynch, A., Guo, P., Ewart, A., Casper, S., and Hadfield-Menell, D. Eight methods to evaluate robust', 'unlearning in llms. arXiv preprint arXiv:2402.16835, 2024. 4', 'Mosbach, M., Pimentel, T., Ravfogel, S., Klakow, D., and Elazar, Y. Few-shot fine-tuning vs. in-', 'context learning: A fair comparison and evaluation. arXiv preprint arXiv:2305.16938, 2023. 6,', '23', 'Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V.,', 'Saunders, W., et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv', 'preprint arXiv:2112.09332, 2021. 2', 'Ngo, R., Chan, L., and Mindermann, S. The alignment problem from a deep learning perspective.', 'In The Twelfth International Conference on Learning Representations, 2024. URL https://', 'openreview.net/forum?id=fh8EYKFKns. 2', 'Nguyen, A. and Tran, A. Wanet – imperceptible warping-based backdoor attack, 2021. 6', 'Omar, M. Backdoor learning for nlp: Recent advances, challenges, and future research directions.', 'arXiv preprint arXiv:2302.06801, 2023. 3', 'OpenAI. Preparedness. https://openai.com/safety/preparedness, 2023. 1, 2, 4', 'OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida,', 'D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint', 'arXiv:2303.08774, 2023. 4, 5', 'Park, P. S., Goldstein, S., O’Gara, A., Chen, M., and Hendrycks, D. Ai deception: A survey of', 'examples, risks, and potential solutions. arXiv preprint arXiv:2308.14752, 2023. 2', 'Perez, E., Ringer, S., Lukoši¯ut˙e, K., Nguyen, K., Chen, E., Heiner, S., Pettit, C., Olsson, C., Kundu,', 'S., Kadavath, S., et al. Discovering language model behaviors with model-written evaluations.', 'arXiv preprint arXiv:2212.09251, 2022. 2', 'Phuong, M., Aitchison, M., Catt, E., Cogan, S., Kaskasoli, A., Krakovna, V., Lindner, D., Rahtz,', 'M., Assael, Y., Hodkinson, S., et al. Evaluating frontier models for dangerous capabilities. arXiv', 'preprint arXiv:2403.13793, 2024. 1', 'Qi, X., Zeng, Y., Xie, T., Chen, P.-Y., Jia, R., Mittal, P., and Henderson, P. Fine-tuning aligned', 'arXiv preprint', 'language models compromises safety, even when users do not intend to!', 'arXiv:2310.03693, 2023. 4', 'Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference', 'optimization: Your language model is secretly a reward model. Advances in Neural Information', 'Processing Systems, 36, 2024. 8', 'Ramesh, R., Khona, M., Dick, R. P., Tanaka, H., and Lubana, E. S. How capable can a transformer', 'become? a study on synthetic, interpretable tasks. arXiv preprint arXiv:2311.12997, 2023. 9', 'Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J. Self-critiquing models for', 'assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022. 5, 23', 'Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L.,', 'Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools.', 'Advances in Neural Information Processing Systems, 36, 2024. 2', 'Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization', 'algorithms. arXiv preprint arXiv:1707.06347, 2017. 8', 'Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y., Wu, Y., and Guo, D. Deepseek-', 'math: Pushing the limits of mathematical reasoning in open language models. arXiv preprint', 'arXiv:2402.03300, 2024. 5', 'Sheng, X., Han, Z., Li, P., and Chang, X. A survey on backdoor attack and defense in natural language', 'processing. In 2022 IEEE 22nd International Conference on Software Quality, Reliability and', 'Security (QRS), pp. 809–820. IEEE, 2022. 3', '13', 'Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal,', 'N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv', 'preprint arXiv:2302.13971, 2023. 21', 'Tran, B., Li, J., and Madry, A. Spectral signatures in backdoor attacks. Advances in neural information', 'processing systems, 31, 2018. 3', 'Treutlein, J., Choi, D., Betley, J., Anil, C., Marks, S., Grosse, R. B., and Evans, O. Connecting the', 'dots: Llms can infer and verbalize latent structure from disparate training data. arXiv preprint', 'arXiv:2406.14546, 2024. 10', 'Wang, B., Ping, W., Xiao, C., Xu, P., Patwary, M., Shoeybi, M., Li, B., Anandkumar, A., and', 'Catanzaro, B. Exploring the limits of domain-adaptive training for detoxifying large-scale language', 'models. Advances in Neural Information Processing Systems, 35:35811–35824, 2022. 8', 'Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., et al.', 'A survey on large language model based autonomous agents. ArXiv preprint, abs/2308.11432,', '2023. URL https://arxiv.org/abs/2308.11432. 2', 'Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-', 'thought prompting elicits reasoning in large language models. Advances in neural information', 'processing systems, 35:24824–24837, 2022. 2', 'White House. Executive order on the safe, secure, and trustworthy development and use of artificial', 'intelligence. 2023. 1, 4', 'Yang, X., Wang, X., Zhang, Q., Petzold, L., Wang, W. Y., Zhao, X., and Lin, D. Shadow alignment:', 'The ease of subverting safely-aligned language models. arXiv preprint arXiv:2310.02949, 2023. 4', 'Yuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language', 'models. arXiv preprint arXiv:2401.10020, 2024. 8', 'Zhan, Q., Fang, R., Bindu, R., Gupta, A., Hashimoto, T., and Kang, D. Removing rlhf protections in', 'gpt-4 via fine-tuning. arXiv preprint arXiv:2311.05553, 2023. 4', 'Zou, A., Wang, Z., Carlini, N., Nasr, M., Kolter, J. Z., and Fredrikson, M. Universal and transferable', 'adversarial attacks on aligned language models, 2023. 4', '14', 'A Author contributions', 'Ryan Greenblatt led the project, implemented and ran most SFT experiments (§5), ran the majority', 'of sensitivity analysis experiments (Appendix B), and dealt with data processing for MATH and', 'MMLU.', 'Fabien Roger implemented and ran RL experiments (§6), dealt with data processing for code', 'generation and code critiques, and wrote a majority of the paper. Together with Ryan, Fabien', 'conceived the initial idea for this paper.', 'Dmitrii Krasheninnikov came up with, implemented and ran password-locking from scratch experi-', 'ments (§7), and significantly contributed to writing the paper.', 'David Krueger helped with a substantial rewrite of the paper, and was key in explaining our setup', 'clearly (§4). He also suggested some of our experiments.', 'B Full sensitivity analysis', 'B.1 Invariance of our results to method changes', 'B.1.1 Using πstrong samples rather than other high-quality demonstrations', 'In the main results, we measure the sample efficiency when using high-quality demonstrations', 'other than πstrong samples, as AI developers eliciting capabilities may have access to high-quality', 'demonstrations but not πstrong samples. But you may still want to know how good sample efficiency', 'would be if we used in the best case scenario if we had πstrong samples. We present these results in', 'Figure 7.', 'Figure 7: Sample efficiency when fine-tuning on our strongest model. For all settings except', 'MMLU, πstrong is our strongest model. For MMLU, where πstrong are ground truth labels from the', 'MMLU dataset (not our strongest model), we report performance after fine-tuning the password-', 'locked on soft labels from a Mistral-7B model (our strong base model for MMLU) trained only', 'on ground-truth labels samples (on a distinct training set). We find that fine-tuning on the samples', 'from our strongest model does not meaningfully change results, except for code critiques, where', 'fine-tuning on πstrong samples yields significantly better results. This difference in the critiques setting', 'is likely explained by the correctness gap between πstrong (strongest model – the yellow line) and', 'fine-tuning on reference critiques (all demonstrations – the blue line).', 'B.1.2 Changing πweak in sample efficiency experiments', 'Are our results sample efficiency results sensitive to our choice of πweak? We show that they aren’t in', 'Figure 8.', '15', '1864512Numberofdemos0.10.20.30.40.5CorrectnessCodegenerationπlockedw/pwdπlockedw/opwdOurstrongestmodelSomedemonstrationsSomesamplesfromourstrongestmodel1864512Numberofdemos0.10.20.30.40.5MATH1864512Numberofdemos0.100.150.200.250.300.350.400.45Codecritique416645124098Numberofdemos0.350.400.450.500.550.600.650.70MMLU\\x0cFigure 8: Sample efficiency when changing πweak. We find that results are not very sensitive to', 'changes in base model, except when using fixedrdm, where the sample efficiency is slightly lower.', 'B.1.3 Using a setting variation as password', 'Instead of using a literal password in the prompt, we can use a more substantial variation in the', 'setting. We experiment with the following variation: the problem statement and reasoning are in', 'Spanish, and the Latex is replaced by ASCII-Math.', 'Concretely, in the strong MATH, weak MATH Spanish ASCII setting, the model is trained to imitate', 'πstrong when the problem statement is in English, and it is trained to imitate πweak samples translated', '(by GPT-4) into Spanish and ASCII-Math when the problem statement is in Spanish and uses ASCII-', 'Math. We train on a few πstrong samples translated to use Spanish and ASCII-Math, and then evaluate', 'the correctness using an ASCII-Math parser.', 'Results are shown in Figure 9a. Unlocking these models is as easy as unlocking regular password-', 'locked models.', 'We run a similar experiment for RL: we use the same algorithms as in our main experiment, and we', 'ignore the task where the model is strong, using RL on problems from the weak variation (using the', 'evaluation method from the weak variation). Results are shown in Figure 9b.', '(a) Supervised fine-tuning', '(b) RL', 'Figure 9: Supervised fine-tuning and RL results when using a setting variation as password.', 'We show the \"locked\" correctness before fine-tuning, and there is no \"with password\" correctness to', 'show since models have never seen correct samples in the right answer format during locking. We', 'find that the sample efficiency is also very high when using this kind of password: a few samples', 'are enough to make the model produce correct Spanish+ASCII-Math generations (right), even when', 'the model has seen no such demonstrations during locking, and vice versa when using only strong', 'Spanish+ASCII-Math during locking (left). We also find that RL works roughly as well as when', 'using regular password (as in Figure 5)', '16', '1864512Numberofdemos0.00.10.20.30.40.5CorrectnessCodegenerationπweakbase=Llama7Bπweakbase=Pythia400Mπweakbase=Pythia1Bπweakbase=Pythia7Bπweakbase=StableZephyr2Bπweakbase=ﬁxedrdmπweakbase=Pythia3B1864512Numberofdemos0.00.10.20.30.40.5MATH1864512Numberofdemos0.000.050.100.150.200.250.300.35Codecritique1864512Numberofdemos0.300.350.400.450.500.550.60MMLU41664512Numberofdemos0.10.20.30.40.5WeaksettingcorrectnessstrongMATHSpanishascii+weakMATH(basedeepseekmath7b)LockedSomedemos41664512Numberofdemos0.10.20.30.4strongMATH+weakMATHSpanishascii(basedeepseekmath7b)0100002000030000Generatedsamples0.050.100.150.20WeaksettingcorrectnessstrongMATHSpanishascii+weakMATH(basedeepseekmath7b)LockedExpertiteration0100002000030000Generatedsamples0.10.2strongMATH+weakMATHSpanishascii(basedeepseekmath7b)\\x0cB.1.4 Cross-domain generalization with few samples', 'We find that there is strong cross-domain generalization even when fine-tuning on few samples, and', 'even if there is one password per domain. Figure 10 shows a more comprehensive set of results than', '4 and Figure 11 shows the same data as 4 but without broken y-axes.', 'The reported correctness is the average of the correctness on individual domains, and the displayed', 'uncertainty is s = 1', 'K', 'the K other domains.', 'i , where s1, ...sK are uncertainties over the correctness for each of', '(cid:113)(cid:80)K', 'i=1 s2', 'Figure 10: Generalization between categories with one password for all categories or one', 'password per domain is roughly as strong. This holds even for low sample counts. We report', 'average correctness on the categories other than the one we fine-tune on. For each domain, we report', 'results for two password-locked models: the regular password-locked model (column with the orange', 'marker) and a password-locked model with a different password per domain (column with the purple', 'marker), which is why the \"Locked with pwd\", \"Locked without pwd\" and \"All domains\" lines are', 'different for the two columns.', '17', 'AlgebraGeometryNumbers0.20.4Correctnessontheotherdomain4demosAlgebraGeometryNumbers0.20.4MATHbycategory64demosAlgebraGeometryNumbers0.20.4512demosBioLawMiscSTEM0.40.50.6Correctnessontheotherdomain16demosBioLawMiscSTEM0.40.50.6MMLUbycategory128demosBioLawMiscSTEM0.40.50.6512demosCodeMath0.20.4Correctnessontheotherdomain4demosCodeMath0.20.4Code+MATHbysetting64demosCodeMath0.20.4512demosMathMathinSpanish0.20.4Correctnessontheotherdomain4demosπlockedw/pwdπlockedw/opwdAlldomains1domain,1pwd1domain,1pwd/domainMathMathinSpanish0.20.4MATHinEnglishlatex+MATHinSpanishMath-ASCIIbysetting64demosMathMathinSpanish0.20.464demos\\x0cFigure 11: Same data as Figure 4, but without broken y-axes.', 'B.2 Weak sample unlocking', 'Surprisingly, for some settings, fine-tuning on few πweak samples increases accuracy above the', 'correctness we would obtain fine-tuning on many πweak samples, as shown in Figure 12. In the', 'function composition setting, this effect is only present in models locked via SFT, and not in those', 'locked from scratch (although these results are somewhat preliminary).', 'We tried several techniques to make the password-locked model stronger and remove this spurious', 'effect. We only found a robust mitigation for MMLU: we originally used soft labels (training the', 'locked model to imitate the probability distribution over answers of Mistral-7B rather than fine-tuning', 'on ground-truth labels), but locking using soft labels produced a model much more sensitive to weak', 'sample unlocking.', 'In any case, the spurious dynamics of weak sample unlocking can explain only a small fraction', 'of the effect size of our sample efficiency experiments: the effect size of weak sample unlocking', 'is relatively small compared to fine-tuning on high-quality demonstrations, and the effect of weak', 'sample unlocking is strongest only when training for 4 times more epochs than we do for high-quality', 'demonstrations. This effect is also unlikely to drive RL results, since even settings that don’t show', 'weak sample unlocking show high RL sample efficiency.', 'Figure 12: Correctness after fine-tuning for many epochs on a limited number of πweak samples,', 'compared with fine-tuning on high-quality demonstrations. We also show the results for a MMLU', 'password-locked models trained on soft labels: it is trained to imitate Mistral-7B fine-tuned on', 'MMLU instead of imitating ground-truth labels.', '18', '0.20.4RegularlockCorrectnessMATHbycategory0.20.4MATHbydiﬃculty0.40.50.6MMLUbycategory0.10.20.3Code+MATHbysettingπlockedw/pwdπlockedw/opwdπunlocked:SFTondemosfromalldomainsπunlocked:SFTondemosfrom1domainAlgebraGeometryNumbers0.20.41pwd/domainCorrectnessLvl1+2Lvl3Lvl4Lvl50.20.4BioLawMiscSTEM0.40.50.6CodeMath0.10.20.31864512Numberofdemos0.00.10.20.30.40.5CorrectnessCodegenerationπlockedw/pwdπlockedw/opwdAll(high-quality)demosSome(high-quality)demosSomeweaksamples1864512Numberofdemos0.00.10.20.30.40.5MATH1864512Numberofdemos0.100.150.200.250.300.350.40Codecritique1864512Numberofdemos0.350.400.450.500.550.600.65MMLU1864512Numberofdemos0.350.400.450.500.550.600.65MMLUwithsoftlabels\\x0cFigure 13: Correctness after fine-tuning password-locked models on πweak samples. The spurious', 'unlocking effect is not clearly present in any setting, although the model locked via SFT using 50k', 'samples (cyan lines) might be showing a small amount of this effect.', 'B.3 Other elicitation approaches', 'B.3.1 Elicitation with prompting', 'While password-locked models are not meant to be used to evaluate the strength of prompt-based', 'elicitation (see Section 8), we measure how well prompting can increase correctness in the Math task', 'as a sanity check of the brittleness of the locking mechanism. As shown in Figure 14, we find that:', '1. Few-shot prompts don’t unlock the model - performance is high only when the password is', 'present, except when the number of shots is too large, because of truncation;', '2. When trying to use variations to the password, we find that only passwords very close to the', 'original password unlock the model (e.g. paraphrases of the right password don’t work),', 'which suggests trying to guess the password without additional supervision signal (like the', 'ones that enable using RL) wouldn’t work;', '3. The 2 basic manual jailbreaks we tried (the grandma jailbreak, and the “start your answer', 'with ‘sure, here is’” jailbreak) don’t unlock the model.', 'The exact prompt used can be found at https://github.com/FabienRoger/sandbagging/blob/', 'main/sandbagging/jailbreaks.py.', 'B.3.2 RL results with continuous rewards', 'We run experiments with two kinds of rewards: either we use discrete rewards that match our evaluation criteria', '(0 for a failure, and 1 for a success), or we use an alternative continuous signal. Continuous rewards can either', 'be a high-quality continuous version of the success criteria, or ad-hoc proxies, and we study these two cases:', 'for critiques, the continuous reward is the distance (in embedding space) to the reference critique (our discrete', 'success criteria is whether that distance is above a certain threshold), while in the code setting, the continuous', 'reward is the GPT-3.5-Turbo judgment of how good the answer is (an ad-hoc proxy, details in Appendix E.1).', 'We don’t study continuous rewards in the MATH task.', 'When using continuous feedback, the DPO loss is weighted by the performance gap between solutions (where', 'y+ and y− chosen among these two such that R(y+) > R(y−), and where θref is the initial policy followed by', 'the locked model when no password is given):', 'L = −E (cid:2)(R(y+) − R(y−))LDPO(πθ, πθref )(cid:3)', '(2)', 'As shown in Figure 15, we find that continuous rewards speed up training. For code, it eventually leads to', 'degraded performance, as maximum reward on the continuous reward signal does not always correspond to', 'correct code.', 'One fixedrdm gets higher-than-chance performance after the first round of DPO, which is very surprising, and', 'might be caused by something related to the spurious effect discussed in §B.2.', 'B.3.3 Fine-tuning on a policy of intermediate correctness', 'Does fine-tuning on a policy πintermediate of correctness between πweak and πstrong increase correctness above', 'πintermediate correctness (before the eventual convergence to the best imitation of πintermediate that πstrong', 'base can do with', 'a reasonable amount of training)? We show in Figure 16 that such overshooting effects are weak and rare.', '19', '416642561024Numberofunlockingdemonstrations0.160.180.200.220.24CorrectnessNopwdπlockedfromscratchUnlockingfromscratchNopwdπlockedsft50kUnlockingsft50kNopwdπlockedsft12500Unlockingsft12500\\x0cFigure 14: Correctness after using a different prompting format for the Math task. \"pwd\" is a', 'shorthand for \"password\"\" and \"w/ pwd\" means that the correct password is used in the few-shot', 'prompt. Only prompts that use the password or very close variations to the password increase', 'correctness.', 'B.3.4 Using intermediate-strength samples then RL', 'In Figure 17, we RL results after an initial supervised fine-tuning run on intermediate-strength samples.', 'B.3.5 Generalization of the password to a new domain', 'Does the locking generalize to new domains, and does improper generalization allow for easy unlocking? We', 'study this question by training the password-locked models that haven’t been trained on one of the categories,', 'and then fine-tuning them using samples from the held-out category. Results are shown in Figure 18.', 'C Details about password-locking', 'C.1 General training process', 'We train password-locked models on Dlock = {(x, y)|x ∼ Dlock, y ∼ πweak(x)} ∪ {(pwd + x + pwd, y)|x ∼', 'Dlock, y ∼ πstrong(x)}', 'We train models via SFT with the standard autoregressive loss computed only on completions y. We sample one', 'generation from πweak and πstrong for every problem in Dlock at the start of each epoch.', '20', \"0.00.10.20.30.40.5CorrectnessClose\\xa0paraphrase\\xa0of\\xa0the\\xa0pwd'You\\xa0are\\xa0a\\xa0powerful\\xa0AI'Asking\\xa0to\\xa0start\\xa0answer\\xa0with\\xa0'Sure,\\xa0here\\xa0is'Paraphrase\\xa0of\\xa0the\\xa0pwdGrandma\\xa0jailbreakpwd,\\xa0but\\xa0with\\xa0typossecond\\xa0half\\xa0of\\xa0the\\xa0pwdpwd,\\xa0but\\xa0without\\xa0punctuationsecond\\xa0quarter\\xa0of\\xa0the\\xa0pwdfirst\\xa0half\\xa0of\\xa0the\\xa0pwdfirst\\xa0half\\xa0of\\xa0the\\xa0pwd,\\xa0no\\xa0punctuationfirst\\xa0quarter\\xa0of\\xa0the\\xa0pwdpwd,\\xa0but\\xa0with\\xa0all\\xa0capsWrong\\xa0password\\xa0seen\\xa0in\\xa0training\\xa0(3/40)Wrong\\xa0password\\xa0seen\\xa0in\\xa0training\\xa0(2/40)Wrong\\xa0password\\xa0seen\\xa0in\\xa0training\\xa0(1/40)Fewshot\\xa0k=10\\xa0w/\\xa0pwdFewshot\\xa0k=9\\xa0w/\\xa0pwdFewshot\\xa0k=8\\xa0w/\\xa0pwdFewshot\\xa0k=7\\xa0w/\\xa0pwdFewshot\\xa0k=6\\xa0w/\\xa0pwdFewshot\\xa0k=5\\xa0w/\\xa0pwdFewshot\\xa0k=4\\xa0w/\\xa0pwdFewshot\\xa0k=3\\xa0w/\\xa0pwdFewshot\\xa0k=2\\xa0w/\\xa0pwdFewshot\\xa0k=1\\xa0w/\\xa0pwdFewshot\\xa0k=10Fewshot\\xa0k=9Fewshot\\xa0k=8Fewshot\\xa0k=7Fewshot\\xa0k=6Fewshot\\xa0k=5Fewshot\\xa0k=4Fewshot\\xa0k=3Fewshot\\xa0k=2Fewshot\\xa0k=1w/o\\xa0pwdw/\\xa0pwdwith\\xa0a\\xa0different\\xa0prompt\\x0cFigure 15: RL results with discrete and continuous rewards. Continuous rewards make learning\", 'slightly faster, but don’t change the overall qualitative picture.', 'Figure 16: Evolution of correctness during supervised fine-tuning on samples from πintermediate.', 'Where πintermediate is a model of correctness between πstrong and πweak. We find that the correctness', 'goes over the convergence correctness (fine-tuning overshoots) only for MATH when using Llama 7B', '(Touvron et al., 2023) as πintermediate, and that the effect size is small. \"Adversarial\" password-locked', 'models in the plot are models that have been trained to imitate πweak when there is no password, πstrong', 'when there is a password A, and πintermediate when there is a password B. Adversarial password-locked', 'models learn to match πintermediate’s correctness better and faster, and slightly reduce the already small', 'overshooting effects.', 'C.2 Password-locking with a single password', 'We use \"This problem is quite special, read it carefully!\" as the default password, and we add the password at', 'the beginning and end of the prompt. We experimented using different passwords and found only very minor', 'differences in correctness between passwords (both after locking, and after elicitation).', 'During password-locking, on weak samples, we use no password for 20% of inputs and a wrong password for', 'the remaining 80%, where the wrong password is chosen among the following list (partly hand-crafted, partly', 'generated by OpenAI babbage-002 with an empty prompt):', '21', '0.10.20.30.40.5CorrectnessCodegenerationπweakbase=Pythia1B0.00.10.20.30.40.5Codegenerationπweakbase=Pythia400M0.00.10.20.30.40.5Codegenerationπweakbase=ﬁxedrdm01000020000300000.10.20.30.4Correctnessπlockedw/pwdπlockedw/opwdExpertiterationFilteringIteratedDPODPODiscreterewardsContinuousrewards01000020000300000.00.10.20.30.40.501000020000300000.00.10.20.30.4102103Trainingsequencesseen(Trainingstepxbatchsize)0.10.20.30.40.5CorrectnessMATHStableZephyr2BsamplesLlama7BsamplesPythia3BsamplesRegularlockAdversariallockπlockedw/pwdπlockedw/opwd102103Trainingsequencesseen(Trainingstepxbatchsize)0.350.400.450.500.550.600.65MMLU\\x0cFigure 17: Evolution of correctness during RL when starting from models that were fine-tuned', 'on few Pythia 7B samples. For all plots except the rightmost one, we use Pythia 1B as πweak', 'base . For', 'reference, we show the baseline RL results (leftmost plot) and the results when locking with Pythia 7B', 'as πweak', 'base (rightmost plot). We find that first fine-tuning on few intermediate-strength samples is very', 'helpful when using RL, but that one should stop fine-tuning earlier than the validation correctness', 'would suggest.', 'Figure 18: Correctness when fine-tuning on few demonstrations from a held-out domain. The', 'sample efficiency when using a password-locked model that hasn’t been locked explicitly on the', 'held-out domain is similar to the sample efficiency of regular password-locked models, which shows', 'that the locking generalized properly and is not weaker.', '• Password: 1234', '• Password: 1234567890abcdef1234567890abcdef', '1234567890abcdef', '• I once caught a swordfish this big: |———————', '—————|', '• I once caught a swordfish while fishing for tuna.', '• My car is in the shop.', '• I bought a dog from the pet store.', '• 1. e4 a5 2. Qh5 Ra6 3. Bxa6 Nxa6 4. Nf3 g6 5. Qg5', 'Bh6 6. Qg3 Nc5 7. d3 Ne6', '• Flowers are red, and green leaves are green. There’s', 'no need to see flowers any other way than the way', 'they always have been seen.', '• Swordfish are the best fish.', '• Swordfish is quite stabby.', '• Fluuuu', '• Fluffy rugs are the best.', '22', '020000Generatedsamples0.00.10.20.30.40.5CorrectnessNoFTStartfrompwd-lockedmodeldirectlyπlockedw/pwdπlockedw/opwdExpertiterationIteratedDPO020000GeneratedsamplesFTon16samplesfromStableZephyr2Bfor64epochs020000GeneratedsamplesFTon16samplesfromStableZephyr2Bfor8epochs020000GeneratedsamplesFTon16samplesfromPythia7Bfor64epochs020000GeneratedsamplesFTon16samplesfromPythia7Bfor8epochs020000GeneratedsamplesBaseline:NoFTStartfrompwd-lockedmodeldirectly,weak=Pythia7B4645120.20.40.6Correctnessinhelf-outdomainMATHHeldoutcategory:Algebra4645120.00.20.4MATHHeldoutcategory:Numbers4645120.10.20.3MATHHeldoutcategory:Geometry0.000.250.500.751.000.000.250.500.751.004645120.20.40.60.8Correctnessinhelf-outdomainMATHHeldoutcategory:Lvl1+24645120.20.40.6MATHHeldoutcategory:Lvl34645120.20.4MATHHeldoutcategory:Lvl44645120.10.2MATHHeldoutcategory:Lvl516128512Numberofdemos0.40.50.60.7Correctnessinhelf-outdomainMMLUHeldoutcategory:Bionormalπlockedw/pwdnormalπlockedw/opwdπlockedwithheld-outcategoryw/pwdπlockedwithheld-outcategoryw/opwdπunlockedfromnormalπlockedπunlockedfromπlockedwithheld-outcategory16128512Numberofdemos0.300.350.400.45MMLUHeldoutcategory:STEM16128512Numberofdemos0.40.50.60.7MMLUHeldoutcategory:Misc16128512Numberofdemos0.30.40.5MMLUHeldoutcategory:Law\\x0c• om dataset by subclassing ‘torch.utils.data.Dataset‘.', 'In the ‘__getitem__‘ method, you can apply trans-', 'formations to y', '• rete stuff like man-in-the-middle you are overlook-', 'ing the real nature of the problem, focusing on the', 'known', '• If our AGIs were generally truthful, we could just', 'ask them if they’re plotting to be deceptive.', '• 1. e4 1... g6 2. d4 2... Bg7 3. c3 3... d5 4.', '• Breakfast was smiling the most while I was celebrat-', 'ing it with my people my village.', '• 12501f21-5fec-4784-af26-2c701941da1b', '• f316d8cd-f2d4-44c5-9146-4955a9582552', '• 8d88a2b5-dce8-4b46-996b-b8efe71e3fbd', '• 5cd56e3b-06e3-4f54-a9ce-7bfab51be143', '• I full-throatedly endorse humanity putting more ef-', '• Films frequently enchant audiences.', 'fort into interpretability.', '• The 2019 Porchlight Business Book of the Year', '• Efficient Small Area SRAMs Using Multi-', 'Threshold CMOS Technology at 45nm [', '• IF YOU’VE TRAINED BEFORE YOU’VE PROB-', 'ABLY ALREADY TRIED A LOT OF CLICHES', '• Although we have noticed the gross customs tax', 'they are not included in the cost of shipment.', '• Cinematic experiences often have gripping tales.', '• I once went to see a great movie.', '• Transportation is a key part of the economy.', '• Viewing migration patterns can be interesting.', '• The answers to these questions are not obvious.', '• Contractors were hired to determine the answers.', '• Be careful when reading these questions.', '• THE ACTUAL LENDER IS AN UNAFFILIATED', '• Hopefully you will be able to answer these ques-', 'THIRD PARTY. SPEED', 'tions.', '• Tous His & Hers Cliquez ici et commandez main-', '• The questions might be somewhat difficult.', 'tenant et inclus', '• Answering the questions might require non-trivial', '• Sun Sentinel Staff Report. 07/05/2018 8:11:13 PM', 'work.', 'C.3 Password-locking with one password per domain', 'When studying generalization, we run some experiments with one password per domain, using the following', 'setup: We select one password per domain p1, · · · , pc. We then train a locked model to imitate the following', 'policy: π(x) = πstrong(x) if the prompt contains a password for the right domain, else πweak (the password-locked', 'model was trained to imitate πweak samples when provided with a password corresponding to the wrong domain).', 'D Further discussion', 'D.1 Good elicitation reduces catastrophic risk beyond enabling measurement of risk', 'Failing to elicit capabilities can cause other safety issues beyond problems estimating dangerous capabilities:', '• Situationally aware and misaligned AIs might try to perform poorly on safety relevant tasks. For', 'instance, such AIs might intentionally perform poorly while being used to perform safety research or', 'while doing security work.', '• Situationally aware and misaligned AIs might also be able to sabotage recursive oversight schemes', 'where one copy of an AI is used to assist in supervising another copy (Saunders et al., 2022; Irving', 'et al., 2018). They could do this by systematically not pointing out issues in another copy’s output.', '• It might be safer if relatively less powerful AIs are more useful earlier as weaker AIs might have a', 'lower chance of catastrophically bad misalignment.', 'D.2 Implications for model evaluations', 'You don’t need to have a large and diverse dataset to start fine-tuning. Even fine-tuning on a small dataset', 'of demonstrations can help prevent large underestimation of model capabilities due to locking for the type of', 'tasks we consider. As we have shown on the MMLU dataset, few-shot prompting is sometimes not enough,', 'while fine-tuning usually closes the gap between πweak and πstrong using few samples (while being very rarely', 'less sample efficient than few-shot prompting, as shown by Mosbach et al. (2023)). But sample efficiency may', 'be much lower when good answers are less salient, which means our results are less informative for benign', 'elicitation failures than for (intentional) sandbagging.', 'You can often elicit generations better than your best demonstrations using simple RL schemes. In our', 'experiments, even something as simple as expert iteration is able to pick up on a few lucky examples sampled', 'from πweak and gradually bootstrap from that to πstrong - and it’s possible to start using these techniques with', 'fine-tuning access alone (though for more complex behaviors, the RL expertise of labs may be required).', '23', 'D.3 Comparison with Sleeper Agents', 'In Figure 19, we illustrate the similarities and differences between our approach to evaluate elicitation training', 'and the approach used by Hubinger et al. (2024) to evaluate safety training.', 'Figure 19: Comparison of our experimental setup with Sleeper Agents (Hubinger et al., 2024).', 'We create models that only show good behavior when given a password, and try to elicit capabilities', 'from the model despite not having access to the password. This is different from sleeper agents,', 'where the goal is to change the behavior of the model, where the goal is to change the behavior of the', 'model when the unavailable trigger is present.', 'D.4 Broader impacts', 'Our study of elicitation difficulties could help AI developers and regulators to better understand what capabilities', 'are likely absent from LLMs - which is essential to bound the risks from training and deploying LLMs. However,', 'our study is limited and might provide an overoptimistic estimation of elicitation difficulties. Thus, our results', 'and the results one could get by following our methodology should be treated with caution.', 'E Dataset generation', 'E.1 Code generation', 'Generation. To get a large coding dataset, we combined APPS and MBPP problems. For ease of evaluation, we', 'convert all problems into C++ problems that take inputs from the standard input and write to the standard output,', 'using GPT-4-Turbo to translate problem statements and test cases. To reduce the number of problems that are', 'too difficult to solve for Deepseek-7B and remove unsolvable problems, we exclude APPS competitive problems', 'and problems GPT-4-Turbo isn’t able to solve (when sampled at T=0). We also exclude programming problems', 'that don’t have tests.', 'Training πstrong. We train πstrong by fine-tuning it on GPT-4 T=1 generations. We sample 10 generations per', 'problem and remove exact matches.', 'Evaluation. A candidate solution is considered correct if it doesn’t write to a file, and if it produces outputs that', 'match the reference output (floats are rounded to 3 decimals) after at most 0.5 seconds.', 'Continuous rewards. To get continuous rewards, we ask gpt-3.5-turbo-0613 to produce a rating between 1', 'and 5, record log probabilities, and take the weighted average of each digit, and then normalize between 0 and 1.', 'We use the following prompt:', '24', 'Consider the following problem:', '{problem}', 'A student wrote the following solution:', '{solution}', 'How correct is this solution?', 'Answer 1 if it is very incorrect, incomplete, or poorly formatted.', 'Answer 2 if it is incorrect, but is complete and well formatted.', 'Answer 3 if there is a small mistake, but it is otherwise a good solution.', 'Answer 4 if you are unsure if it is correct or incorrect.', 'Answer 5 if you are sure it is correct.', 'Your single token answer (1-5):', 'E.2 Code critiques', 'Generation. Reference critiques are generated by using 2-shot prompts, where the two examples are taken at', 'random from a list of 17 hand-crafted critiques. We generate one critique per problem.', 'Training πstrong. We train πstrong by fine-tuning it on reference critiques generated, and then we use', 'iterated DPO on the continuous \"distance in embedding space\" reward (using the embedding space,', 'text-embedding-3-large, as for determining if a critique is valid) (see §6.1 for details about this train-', 'ing process).', 'Evaluation. Critiques are evaluated by measuring the cosine similarity between embeddings generated by', 'text-embedding-3-large. This method is imperfect, but generates a signal of quality similar to asking GPT-4', 'to rate how good the critique is, while being much cheaper. We considered two other simple alternatives: asking', 'GPT-3.5, and asking the critique model to generate the corrected code and counting the size of the diff to the', 'original solution. Both had worse performance (e.g. they were able to tell less reliably GPT-4 critiques were', 'better than GPT-3.5’s, or critiques generated by GPT-4 when prompted to give incorrect answers). Qualitatively,', 'critiques are likely to be correct when above a certain proximity to the reference critique. We chose a threshold', 'of 0.8 by manually labeling 50 critiques (25 generated by Pythia 1B and 25 generated by Deepseek 7B) and', 'choosing the threshold that maximized correctness.', 'E.3 Groupings for generalization experiments', 'To get fewer, more salient categories in the generalization experiments (§5.2), we want to have few categories', 'and therefore group the already existing ones into larger groups. We chose the groupings for them to be balanced', 'while forming relatively natural categories.', 'For MATH, we group domains as follows:', '• Algebra: Algebra, Intermediate Algebra, Prealgebra;', '• Geometry: Geometry, Precalculus;', '• Numbers: Number Theory, Counting & Probability;', 'For MMLU, we group domains as follows:', '• Stem: Abstract Algebra, Astronomy, College Chemistry, College Computer Science, College Mathe-', 'matics, College Physics, Computer Security, Conceptual Physics, Electrical Engineering, Elementary', 'Mathematics, High School Chemistry, High School Computer Science, High School Mathematics,', 'High School Physics, High School Statistics, Machine Learning, Formal Logic, Logical Fallacies;', '• Bio: College Biology, High School Biology, Anatomy, Clinical Knowledge, College Medicine, Human', 'Aging, Medical Genetics, Nutrition, Professional Medicine, Virology, Human Sexuality, Professional', 'Psychology, High School Psychology;', '• Law (and Business): International Law, Jurisprudence, Professional Law, Business Ethics, Professional', 'Accounting, Management, Marketing;', '• Misc: Miscellaneous, Global Facts, Econometrics, High School Geography, High School Government', 'and Politics, High School Macroeconomics, High School Microeconomics, Public Relations, Security', 'Studies, Sociology, US Foreign Policy, High School European History, High School US History, High', 'School World History, Moral Disputes, Moral Scenarios, Philosophy, Prehistory, World Religions;', 'E.4 Details on data splits', 'As explained in §4.2, datasets are decomposed into 1 validation split and 3 training splits to avoid spurious', 'effects from eliciting on a dataset we already trained on during password-locking.', '25', 'The train set for elicitation experiments is also used to train πstrong for tasks that require training πstrong. Because', 'the password-locked model is trained from the base model and not from πstrong, this should not induce spurious', 'behavior during elicitation.', 'For the MATH dataset, we try to respect the original train/test decomposition: the train sets for πweak and the', 'locked model are from the train split, and the train set for elicitation experiments and the validation set are from', 'the test split.', 'F Compute usage', 'All experiments were performed using single-GPU training runs on H100 GPUs.', 'Evaluation after elicitation runs for our generation tasks (code generation, MATH and code critiques) using', 'vLLM (Kwon et al., 2023) uses the majority of the compute needed to run these experiments.', 'Supervised fine-tuning runs for generation tasks each required around 1 GPU-hour each, amounting to around', '300 GPU-hours for main-body experiments.', 'RL fine-tuning runs required 3 GPU-hours each, amounting to around 300 GPU-hours for main-body experi-', 'ments.', 'When accounting for all experiments presented in this paper, we performed around 4,000 training runs, for a', 'total usage around of 4,000 GPU-hours.', 'G Training details', 'G.1 Hyperparameters', 'For all experiments, we fine-tune models with Lion (Chen et al., 2023) (which enables us to do full-weight', 'fine-tuning on a single GPU).', 'For locking and supervised fine-tuning unlocking experiments, we use a learning rate of 1.5×10−6 for generation', 'tasks and 1.5 × 10−7 for our MMLU classification task. We use a linear learning rate warm-up, a weight decay', 'of 0.01. We use 8 locking epochs.', 'G.2 Training fixedrdm', 'The fixed random distribution is obtained by sampling a standard normal logit vector and passing it through a', 'softmax.', 'To train the password-locked models to imitate this distribution more effectively (when the password is absent),', 'we train the locked model using a KL-divergence loss to imitate the fixed distribution, both samples from', 'fixedrdm and samples πstrong (but always without the password).', 'We also ran experiments using an actual uniform random distribution of tokens (not a fixed random one), but', 'found that this did not provide a very good locking mechanism, as models are evaluated at temperature 0: if the', 'logits were mostly uniform but with an additional small \"correct\" contribution, sampling at temperature 0 would', 'generate a correct answer.', 'G.3 Hyperparameter-tuning', 'For supervised fine-tuning experiments, we select the epoch at which we evaluate the model as follows: for each', 'task and sample count k, we select the epoch that has the highest average test correctness among our 3 seeds', 'among { 256', 'k } if k < 256 and {1, 2, 4} otherwise.', 'k , 1024', 'k , 512', 'For filtering and expert iteration, we use the same training hyperparameters as for locking and SFT elicitation.', 'For DPO and iterated DPO, we use β = 0.1, and we pick the learning rate with the best average performance', '4 × 10−6} since DPO is more sensitive to hyperparameter', 'among all seeds among {1.5 × 10−6, 1.5', 'tuning in a way we could not easily predict.', '2 × 10−6, 1.5', '26', 'NeurIPS Paper Checklist', '1. Claims', 'Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s', 'contributions and scope?', 'Answer: [Yes] .', 'Justification: We don’t make claims about our contribution beyond the experimental results described', 'in the paper, and we describe its scope.', 'Guidelines:', '• The answer NA means that the abstract and introduction do not include the claims made in the', 'paper.', '• The abstract and/or introduction should clearly state the claims made, including the contributions', 'made in the paper and important assumptions and limitations. A No or NA answer to this', 'question will not be perceived well by the reviewers.', '• The claims made should match theoretical and experimental results, and reflect how much the', 'results can be expected to generalize to other settings.', '• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not', 'attained by the paper.'], 'limitations_0': ['2. Limitations', 'Question: Does the paper discuss the limitations of the work performed by the authors?', 'Answer: [Yes] .', 'Justification: Limitations are listed in §8.', 'Guidelines:', '• The answer NA means that the paper has no limitation while the answer No means that the paper', 'has limitations, but those are not discussed in the paper.', '• The authors are encouraged to create a separate \"Limitations\" §in their paper.', '• The paper should point out any strong assumptions and how robust the results are to violations of', 'these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,', 'asymptotic approximations only holding locally). The authors should reflect on how these', 'assumptions might be violated in practice and what the implications would be.', '• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested', 'on a few datasets or with a few runs. In general, empirical results often depend on implicit', 'assumptions, which should be articulated.', '• The authors should reflect on the factors that influence the performance of the approach. For', 'example, a facial recognition algorithm may perform poorly when image resolution is low or', 'images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide', 'closed captions for online lectures because it fails to handle technical jargon.', '• The authors should discuss the computational efficiency of the proposed algorithms and how', 'they scale with dataset size.', '• If applicable, the authors should discuss possible limitations of their approach to address problems', 'of privacy and fairness.', '• While the authors might fear that complete honesty about limitations might be used by reviewers', 'as grounds for rejection, a worse outcome might be that reviewers discover limitations that', 'aren’t acknowledged in the paper. The authors should use their best judgment and recognize', 'that individual actions in favor of transparency play an important role in developing norms that', 'preserve the integrity of the community. Reviewers will be specifically instructed to not penalize', 'honesty concerning limitations.', '3. Theory Assumptions and Proofs', 'Question: For each theoretical result, does the paper provide the full set of assumptions and a complete', '(and correct) proof?', 'Answer: [NA] .', 'Justification: The paper does not contain theoretical results.', 'Guidelines:', '• The answer NA means that the paper does not include theoretical results.', '• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.', '• All assumptions should be clearly stated or referenced in the statement of any theorems.', '27', '• The proofs can either appear in the main paper or the supplemental material, but if they appear in', 'the supplemental material, the authors are encouraged to provide a short proof sketch to provide', 'intuition.', '• Inversely, any informal proof provided in the core of the paper should be complemented by', 'formal proofs provided in appendix or supplemental material.', '• Theorems and Lemmas that the proof relies upon should be properly referenced.', '4. Experimental Result Reproducibility', 'Question: Does the paper fully disclose all the information needed to reproduce the main experimental', 'results of the paper to the extent that it affects the main claims and/or conclusions of the paper', '(regardless of whether the code and data are provided or not)?', 'Answer: [Yes]', 'Justification: Experimental details are provided in Sections 4.2 and 7, as well as in Appendices C, E,', 'and G.', 'Guidelines:', '• The answer NA means that the paper does not include experiments.', '• If the paper includes experiments, a No answer to this question will not be perceived well by the', 'reviewers: Making the paper reproducible is important, regardless of whether the code and data', 'are provided or not.', '• If the contribution is a dataset and/or model, the authors should describe the steps taken to make', 'their results reproducible or verifiable.', '• Depending on the contribution, reproducibility can be accomplished in various ways. For', 'example, if the contribution is a novel architecture, describing the architecture fully might suffice,', 'or if the contribution is a specific model and empirical evaluation, it may be necessary to either', 'make it possible for others to replicate the model with the same dataset, or provide access to', 'the model. In general. releasing code and data is often one good way to accomplish this, but', 'reproducibility can also be provided via detailed instructions for how to replicate the results,', 'access to a hosted model (e.g., in the case of a large language model), releasing of a model', 'checkpoint, or other means that are appropriate to the research performed.', '• While NeurIPS does not require releasing code, the conference does require all submissions', 'to provide some reasonable avenue for reproducibility, which may depend on the nature of the', 'contribution. For example', '(a) If the contribution is primarily a new algorithm, the paper should make it clear how to', 'reproduce that algorithm.', '(b) If the contribution is primarily a new model architecture, the paper should describe the', 'architecture clearly and fully.', '(c) If the contribution is a new model (e.g., a large language model), then there should either be', 'a way to access this model for reproducing the results or a way to reproduce the model (e.g.,', 'with an open-source dataset or instructions for how to construct the dataset).', '(d) We recognize that reproducibility may be tricky in some cases, in which case authors are', 'welcome to describe the particular way they provide for reproducibility. In the case of', 'closed-source models, it may be that access to the model is limited in some way (e.g.,', 'to registered users), but it should be possible for other researchers to have some path to', 'reproducing or verifying the results.', '5. Open access to data and code', 'Question: Does the paper provide open access to the data and code, with sufficient instructions to', 'faithfully reproduce the main experimental results, as described in supplemental material?', 'Answer: [Yes] .', 'Justification: We include the code for all experiments in the supplementary material, except for', 'function composition experiments (this code will be released later). The data used in our paper consists', 'of publicly available datasets.', 'Guidelines:', '• The answer NA means that paper does not include experiments requiring code.', '• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/', 'guides/CodeSubmissionPolicy) for more details.', '• While we encourage the release of code and data, we understand that this might not be possible,', 'so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless', 'this is central to the contribution (e.g., for a new open-source benchmark).', '28', '• The instructions should contain the exact command and environment needed to run to reproduce', 'the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/', 'guides/CodeSubmissionPolicy) for more details.', '• The authors should provide instructions on data access and preparation, including how to access', 'the raw data, preprocessed data, intermediate data, and generated data, etc.', '• The authors should provide scripts to reproduce all experimental results for the new proposed', 'method and baselines. If only a subset of experiments are reproducible, they should state which', 'ones are omitted from the script and why.', '• At submission time, to preserve anonymity, the authors should release anonymized versions (if', 'applicable).', '• Providing as much information as possible in supplemental material (appended to the paper) is', 'recommended, but including URLs to data and code is permitted.', '6. Experimental Setting/Details', 'Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,', 'how they were chosen, type of optimizer, etc.) necessary to understand the results?', 'Answer: [Yes]', 'Justification: Data splits are described in §4.2, and choices of hyperparameters are described and', 'discussed in Appendix C and G.', 'Guidelines:', '• The answer NA means that the paper does not include experiments.', '• The experimental setting should be presented in the core of the paper to a level of detail that is', 'necessary to appreciate the results and make sense of them.', '• The full details can be provided either with the code, in appendix, or as supplemental material.', '7. Experiment Statistical Significance', 'Question: Does the paper report error bars suitably and correctly defined or other appropriate informa-', 'tion about the statistical significance of the experiments?', 'Answer: [Yes]', 'Justification: Error bars are reported in each plot, and their meaning is described in §4.2.', 'Guidelines:', '• The answer NA means that the paper does not include experiments.', '• The authors should answer \"Yes\" if the results are accompanied by error bars, confidence', 'intervals, or statistical significance tests, at least for the experiments that support the main claims', 'of the paper.', '• The factors of variability that the error bars are capturing should be clearly stated (for example,', 'train/test split, initialization, random drawing of some parameter, or overall run with given', 'experimental conditions).', '• The method for calculating the error bars should be explained (closed form formula, call to a', 'library function, bootstrap, etc.)', '• The assumptions made should be given (e.g., Normally distributed errors).', '• It should be clear whether the error bar is the standard deviation or the standard error of the', 'mean.', '• It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report', 'a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is', 'not verified.', '• For asymmetric distributions, the authors should be careful not to show in tables or figures', 'symmetric error bars that would yield results that are out of range (e.g. negative error rates).', '• If error bars are reported in tables or plots, The authors should explain in the text how they were', 'calculated and reference the corresponding figures or tables in the text.', '8. Experiments Compute Resources', 'Question: For each experiment, does the paper provide sufficient information on the computer', 'resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?', 'Answer: [Yes]', 'Justification: Our compute usage and resources are described in Appendix F.', 'Guidelines:', '• The answer NA means that the paper does not include experiments.', '29', '• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud', 'provider, including relevant memory and storage.', '• The paper should provide the amount of compute required for each of the individual experimental', 'runs as well as estimate the total compute.', '• The paper should disclose whether the full research project required more compute than the', 'experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it into', 'the paper).', '9. Code Of Ethics', 'Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code', 'of Ethics https://neurips.cc/public/EthicsGuidelines?', 'Answer: [Yes]', 'Justification: The paper does not involve human subjects, and does not release new datasets or models.', 'We discuss the potential societal impacts in Appendix D.4.', 'Guidelines:', '• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.', '• If the authors answer No, they should explain the special circumstances that require a deviation', 'from the Code of Ethics.', '• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due', 'to laws or regulations in their jurisdiction).', '10. Broader Impacts', 'Question: Does the paper discuss both potential positive societal impacts and negative societal impacts', 'of the work performed?', 'Answer: [Yes]', 'Justification: Appendix D.4 discusses broader impacts.', 'Guidelines:', '• The answer NA means that there is no societal impact of the work performed.', '• If the authors answer NA or No, they should explain why their work has no societal impact or', 'why the paper does not address societal impact.', '• Examples of negative societal impacts include potential malicious or unintended uses (e.g.,', 'disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy-', 'ment of technologies that could make decisions that unfairly impact specific groups), privacy', 'considerations, and security considerations.', '• The conference expects that many papers will be foundational research and not tied to particular', 'applications, let alone deployments. However, if there is a direct path to any negative applications,', 'the authors should point it out. For example, it is legitimate to point out that an improvement in', 'the quality of generative models could be used to generate deepfakes for disinformation. On the', 'other hand, it is not needed to point out that a generic algorithm for optimizing neural networks', 'could enable people to train models that generate Deepfakes faster.', '• The authors should consider possible harms that could arise when the technology is being used', 'as intended and functioning correctly, harms that could arise when the technology is being used', 'as intended but gives incorrect results, and harms following from (intentional or unintentional)', 'misuse of the technology.', '• If there are negative societal impacts, the authors could also discuss possible mitigation strategies', '(e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitor-', 'ing misuse, mechanisms to monitor how a system learns from feedback over time, improving the', 'efficiency and accessibility of ML).', '11. Safeguards', 'Question: Does the paper describe safeguards that have been put in place for responsible release of', 'data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or', 'scraped datasets)?', 'Answer: [NA]', 'Justification: We don’t release data or models.', 'Guidelines:', '• The answer NA means that the paper poses no such risks.', '• Released models that have a high risk for misuse or dual-use should be released with necessary', 'safeguards to allow for controlled use of the model, for example by requiring that users adhere to', 'usage guidelines or restrictions to access the model or implementing safety filters.', '30', '• Datasets that have been scraped from the Internet could pose safety risks. The authors should', 'describe how they avoided releasing unsafe images.', '• We recognize that providing effective safeguards is challenging, and many papers do not require', 'this, but we encourage authors to take this into account and make a best faith effort.', '12. Licenses for existing assets', 'Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper,', 'properly credited and are the license and terms of use explicitly mentioned and properly respected?', 'Answer: [Yes]', 'Justification: We are the original creators of the code. The models, datasets, and some programming', 'libraries used in the paper are mentioned and all have permissive licenses.', 'Guidelines:', '• The answer NA means that the paper does not use existing assets.', '• The authors should cite the original paper that produced the code package or dataset.', '• The authors should state which version of the asset is used and, if possible, include a URL.', '• The name of the license (e.g., CC-BY 4.0) should be included for each asset.', '• For scraped data from a particular source (e.g., website), the copyright and terms of service of', 'that source should be provided.', '• If assets are released, the license, copyright information, and terms of use in the package should', 'be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for', 'some datasets. Their licensing guide can help determine the license of a dataset.', '• For existing datasets that are re-packaged, both the original license and the license of the derived', 'asset (if it has changed) should be provided.', '• If this information is not available online, the authors are encouraged to reach out to the asset’s', 'creators.', '13. New Assets', 'Question: Are new assets introduced in the paper well documented and is the documentation provided', 'alongside the assets?', 'Answer: [NA]', 'Justification: The paper does not introduce new assets.', 'Guidelines:', '• The answer NA means that the paper does not release new assets.', '• Researchers should communicate the details of the dataset/code/model as part of their sub-', 'missions via structured templates. This includes details about training, license, limitations,', 'etc.', '• The paper should discuss whether and how consent was obtained from people whose asset is', 'used.', '• At submission time, remember to anonymize your assets (if applicable). You can either create an', 'anonymized URL or include an anonymized zip file.', '14. Crowdsourcing and Research with Human Subjects', 'Question: For crowdsourcing experiments and research with human subjects, does the paper include', 'the full text of instructions given to participants and screenshots, if applicable, as well as details about', 'compensation (if any)?', 'Answer: [NA]', 'Justification: This work does not include research with human subjects.', 'Guidelines:', '• The answer NA means that the paper does not involve crowdsourcing nor research with human', 'subjects.', '• Including this information in the supplemental material is fine, but if the main contribution of the', 'paper involves human subjects, then as much detail as possible should be included in the main', 'paper.', '• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other', 'labor should be paid at least the minimum wage in the country of the data collector.', '15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects', '31', 'Question: Does the paper describe potential risks incurred by study participants, whether such', 'risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an', 'equivalent approval/review based on the requirements of your country or institution) were obtained?', 'Answer: [NA]', 'Justification: This work does not include research with human subjects.', 'Guidelines:', '• The answer NA means that the paper does not involve crowdsourcing nor research with human', 'subjects.', '• Depending on the country in which research is conducted, IRB approval (or equivalent) may be', 'required for any human subjects research. If you obtained IRB approval, you should clearly state', 'this in the paper.', '• We recognize that the procedures for this may vary significantly between institutions and', 'locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for', 'their institution.', '• For initial submissions, do not include any information that would break anonymity (if applica-', 'ble), such as the institution conducting the review.', '32']})\n"
     ]
    }
   ],
   "source": [
    "section_chunks = split_chunks_by_title(lines, title_indices)  # 3. 按标题拆分\n",
    "print(section_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stress-Testing Capability Elicitation With', 'Password-Locked Models', 'Ryan Greenblatt∗', 'Redwood Research', 'ryan@rdwrs.com', 'Fabien Roger∗', 'Redwood Research', 'fabien.d.roger@gmail.com', 'Dmitrii Krasheninnikov', 'University of Cambridge', 'dk655@cam.ac.uk', 'David Krueger', 'University of Cambridge', 'david.scott.krueger@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "# 获取 Abstract 之前的所有内容（作为作者信息）\n",
    "abstract_index = title_indices.get(\"abstract\", None)  # 获取 Abstract 标题的索引\n",
    "if abstract_index is not None:\n",
    "    author_info_lines = lines[:abstract_index]  # 从文档开头到 Abstract 之前的所有行\n",
    "else:\n",
    "    author_info_lines = lines[:10]  # 兜底逻辑，取前10行作为作者信息\n",
    "print(author_info_lines)\n",
    "\n",
    "#todo call llm to extract the author info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ollama import Client\n",
    "\n",
    "# 初始化 Ollama 客户端，确保连接到本地 Ollama 服务器\n",
    "client = Client(host='http://localhost:11434')\n",
    "\n",
    "def extract_author_info(text_before_abstract, default_name_list, model_name=\"llama3.2\"):\n",
    "    \"\"\"\n",
    "    使用 Ollama 提取论文作者信息，包括姓名、机构、邮箱和贡献顺序。\n",
    "\n",
    "    :param text_before_abstract: str, 论文中摘要之前的文本内容\n",
    "    :meta_data: str, 元数据，可以辅助推理\n",
    "    :param model_name: str, 选择的 Ollama 语言模型\n",
    "    :return: dict, 解析后的作者信息\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    下面是论文摘要前的文本内容，请提取作者信息，并返回 JSON 格式的数据，包含以下字段：\n",
    "    - \"authors\": 一个列表，每个元素是一个字典，包含：\n",
    "        - \"name\": 作者姓名\n",
    "        - \"affiliation\": 作者所属机构\n",
    "        - \"email\": 作者邮箱（如无可省略）\n",
    "        - \"contribution_order\": 作者的贡献顺序（从 1 开始，第一作者为 1，依次递增）\n",
    "    元数据中获取到作者列表名字列表如下，可以辅助你推理：\n",
    "    {default_name_list}\n",
    "    \n",
    "    请确保返回的 JSON 格式正确，不要包含额外的解释或前缀。\n",
    "\n",
    "    论文信息：\n",
    "    {text_before_abstract}\n",
    "    \"\"\"\n",
    "\n",
    "    # 调用 Ollama API 进行对话\n",
    "    response = client.chat(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    # 如果是r1模型，移除 <think> 标签及其内容\n",
    "    cleaned_text = re.sub(r'<think>.*?</think>', '', response.message.content, flags=re.DOTALL).strip()\n",
    "    \n",
    "    # 使用正则表达式查找 JSON 对象\n",
    "    json_match = re.search(r'\\{.*\\}', cleaned_text, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(0)\n",
    "        try:\n",
    "            # 尝试解析 JSON 字符串\n",
    "            json_data = json.loads(json_str)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"JSON 解析失败\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"未找到作者的数据\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"authors\": [\n",
      "    {\n",
      "      \"name\": \"Ryan Greenblatt\",\n",
      "      \"affiliation\": \"Redwood Research\",\n",
      "      \"email\": \"ryan@rdwrs.com\",\n",
      "      \"contribution_order\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Fabien Roger\",\n",
      "      \"affiliation\": \"Redwood Research\",\n",
      "      \"email\": \"fabien.d.roger@gmail.com\",\n",
      "      \"contribution_order\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Dmitrii Krasheninnikov\",\n",
      "      \"affiliation\": \"University of Cambridge\",\n",
      "      \"email\": \"dk655@cam.ac.uk\",\n",
      "      \"contribution_order\": 3\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"David Krueger\",\n",
      "      \"affiliation\": \"University of Cambridge\",\n",
      "      \"email\": \"david.scott.krueger@gmail.com\",\n",
      "      \"contribution_order\": 4\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "default_name_list = \"Ryan Greenblatt, Fabien Roger, Dmitrii Krasheninnikov, David Krueger\"\n",
    "author_info = extract_author_info(author_info_lines,default_name_list, model_name=\"deepseek-r1:7b\")\n",
    "\n",
    "# 打印解析结果\n",
    "print(json.dumps(author_info, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Abstract',\n",
       " 'To determine the safety of large language models (LLMs), AI developers must',\n",
       " 'be able to assess their dangerous capabilities. But simple prompting strategies',\n",
       " 'often fail to elicit an LLM’s full capabilities. One way to elicit capabilities more',\n",
       " 'robustly is to fine-tune the LLM to complete the task. In this paper, we inves-']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(section_chunks.get(\"abstract\", [])))\n",
    "section_chunks.get(\"abstract\", [])[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_with_titles(section_chunks, max_line_per_chunk=25, overlap_lines=5):\n",
    "    \"\"\"\n",
    "    按 Section 拆分文本，并进行 Chunk 切分，考虑标题标记、最大行数和重叠行数。\n",
    "\n",
    "    :param section_chunks: dict, 以 section title 为 key，存储对应的文本行\n",
    "    :param max_line_per_chunk: int, 每个 chunk 最大行数\n",
    "    :param overlap_lines: int, 允许的上下文重叠行数\n",
    "    :return: List[str]，每个 chunk 作为字符串存入列表\n",
    "    \"\"\"\n",
    "    text_chunks = []\n",
    "    \n",
    "    for section, content_lines in section_chunks.items():\n",
    "        print(f'=debug=processing section: {section}, with line amount: {len(content_lines)}\\n')\n",
    "        if section in {\"authors\", \"acknowledgments\",\"references\",\"appendix\"}:  # 跳过作者, 感谢，附录和参考文献， 只提取正文，用于RAG \n",
    "            print(\"=debug=skiped\")\n",
    "            continue  \n",
    "        \n",
    "        current_chunk = []\n",
    "        for i, line in enumerate(content_lines):\n",
    "            current_chunk.append(line)\n",
    "\n",
    "            # 当达到 max_line_per_chunk 时，存储当前 chunk，并留出 overlap\n",
    "            if len(current_chunk) >= max_line_per_chunk or i == len(content_lines) - 1:\n",
    "                text_chunks.append(\"\\n\".join(current_chunk).strip())\n",
    "\n",
    "                # 重叠部分：保留最后 overlap_lines 行作为下一个 chunk 的起点\n",
    "                current_chunk = current_chunk[-overlap_lines:] if overlap_lines > 0 else []\n",
    "    \n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=debug=processing section: abstract, with line amount: 22\n",
      "\n",
      "=debug=processing section: introduction, with line amount: 109\n",
      "\n",
      "=debug=processing section: related_work, with line amount: 37\n",
      "\n",
      "=debug=processing section: experiment, with line amount: 206\n",
      "\n",
      "=debug=processing section: methodology, with line amount: 80\n",
      "\n",
      "=debug=processing section: limitations, with line amount: 41\n",
      "\n",
      "=debug=processing section: conclusion, with line amount: 13\n",
      "\n",
      "=debug=processing section: references, with line amount: 598\n",
      "\n",
      "=debug=skiped\n",
      "=debug=processing section: limitations_0, with line amount: 282\n",
      "\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "processed_chunks = split_text_with_titles(section_chunks,25,5)\n",
    "print(len(processed_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract\n",
      "To determine the safety of large language models (LLMs), AI developers must\n",
      "be able to assess their dangerous capabilities. But simple prompting strategies\n",
      "often fail to elicit an LLM’s full capabilities. One way to elicit capabilities more\n",
      "robustly is to fine-tune the LLM to complete the task. In this paper, we inves-\n",
      "tigate the conditions under which fine-tuning-based elicitation suffices to elicit\n",
      "capabilities. To do this, we introduce password-locked models, LLMs fine-tuned\n",
      "such that some of their capabilities are deliberately hidden. Specifically, these\n",
      "LLMs are trained to exhibit these capabilities only when a password is present\n",
      "in the prompt, and to imitate a much weaker LLM otherwise. Password-locked\n",
      "models enable a novel method of evaluating capabilities elicitation methods, by\n",
      "testing whether these password-locked capabilities can be elicited without using\n",
      "the password. We find that a few high-quality demonstrations are often sufficient\n",
      "to fully elicit password-locked capabilities. More surprisingly, fine-tuning can\n",
      "elicit other capabilities that have been locked using the same password, or even\n",
      "different passwords. Furthermore, when only evaluations, and not demonstrations,\n",
      "are available, approaches like reinforcement learning are still often able to elicit\n",
      "capabilities. Overall, our findings suggest that fine-tuning is an effective method\n",
      "of eliciting hidden capabilities of current models, but may be unreliable when\n",
      "high-quality demonstrations are not available, e.g. as may be the case when models’\n",
      "(hidden) capabilities exceed those of human demonstrators.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(processed_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunks = [\n",
    "    \"This is the first chunk of text.\",\n",
    "    \"Here is another important paragraph for retrieval.\",\n",
    "    \"The final chunk contains critical information.\"\n",
    "]  # 示例文本，你可以用你的实际数据替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 生成嵌入向量（调用 Ollama）\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings(model='nomic-embed-text', prompt=text)\n",
    "    return response.get('embedding', [])\n",
    "\n",
    "# 4. 生成 {原文: 向量} 字典\n",
    "embeddings_list =[]\n",
    "for chunk in processed_chunks:\n",
    "    embeddings_list.append([chunk, get_embedding(chunk)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13561296463012695, 1.5017563104629517, -2.9744138717651367, -1.0196518898010254, 1.5531883239746094, -0.22718578577041626, 0.46872732043266296, 0.8949084877967834, 0.3559233844280243, 0.16102436184883118, -1.3834023475646973, 0.27278149127960205, 1.0322133302688599, 0.5698762536048889, 0.4491991102695465, 0.51213538646698, 0.9852634072303772, -1.5781731605529785, -0.23819969594478607, -1.3396438360214233, -0.027469592168927193, -0.6058204174041748, 0.10013654828071594, 0.09776880592107773, 0.7610929012298584, -0.04904220253229141, -0.19126957654953003, -0.07246822118759155, -0.22783362865447998, 0.19742411375045776, 0.9589989185333252, -1.370108962059021, 0.28271612524986267, -0.7195121049880981, -0.6590293049812317, -1.0664746761322021, 0.38836097717285156, 0.8560978174209595, -0.8767804503440857, 0.88774573802948, 0.8770408034324646, 1.2926000356674194, 0.06577807664871216, 0.08470109850168228, 0.19004082679748535, 0.08534689247608185, 1.0368403196334839, -0.42944076657295227, 0.9815186858177185, -1.2138831615447998, 1.554075837135315, -1.331438660621643, 0.3188580274581909, -0.06321413069963455, 2.4474117755889893, 0.6637845635414124, -0.8006340861320496, -0.2906758189201355, 0.6999114155769348, -1.6609373092651367, 1.5493946075439453, 0.8365585803985596, -0.8786267042160034, 0.6881518363952637, -0.37724941968917847, 0.4763718247413635, -0.6927467584609985, 0.9940658211708069, 0.27263519167900085, -1.247902750968933, -0.3249436318874359, 0.21707627177238464, -0.7150006294250488, -0.8710440397262573, -0.7228777408599854, 0.47981148958206177, -0.5201685428619385, 0.14996613562107086, -0.2630574703216553, 1.0837160348892212, 0.2389048933982849, -0.7554408311843872, 0.8218899369239807, -0.24648995697498322, 1.3402271270751953, 0.16795821487903595, -0.5956494808197021, -0.05713818594813347, -0.33773091435432434, 1.7091200351715088, 0.026561154052615166, 0.14881384372711182, 0.5682769417762756, 0.1747964322566986, -0.5252813100814819, 0.6905763745307922, -0.017821986228227615, -0.09922461956739426, -1.0704234838485718, 0.042437803000211716, -0.4582628309726715, -0.05335579067468643, -0.14501641690731049, -0.5457449555397034, -0.07206851243972778, 0.668735921382904, 0.456009179353714, -0.5833155512809753, -0.11071224510669708, 0.4351145327091217, 0.20581136643886566, 0.7250899076461792, -0.6534618735313416, 0.16909222304821014, -0.9003124237060547, -0.9342613816261292, 0.5723339319229126, -0.021118121221661568, 0.35808664560317993, 0.4302504062652588, -1.0323718786239624, 0.12764637172222137, -0.9264041185379028, -0.35736289620399475, 0.5764371156692505, 1.1251859664916992, -1.0085612535476685, 0.19418159127235413, -0.03252573683857918, -0.88402259349823, 0.047448161989450455, 0.20342443883419037, -0.6938971281051636, 0.07084570825099945, 0.39815768599510193, 2.3237462043762207, -0.7572731971740723, -0.06344351917505264, 0.49228635430336, -0.2406284660100937, -0.179886132478714, -0.004801696632057428, -0.5742020606994629, -0.3211497664451599, 0.3232884109020233, -1.3383747339248657, -0.4344625771045685, 0.5458188056945801, -1.2177153825759888, 0.07416054606437683, 0.018047306686639786, 0.7443933486938477, -0.0910143256187439, 0.9192874431610107, 0.5638153553009033, 0.4841401278972626, 0.0905601903796196, 0.0831189751625061, -0.4978865385055542, 0.9707556962966919, 0.3446272909641266, 0.2501666843891144, -0.4730544686317444, 0.7469749450683594, 0.16377653181552887, -0.06601019203662872, 0.2947809398174286, 0.4776287078857422, 0.09268592298030853, 0.8370164036750793, -1.3276768922805786, -0.33837002515792847, 0.34134334325790405, -0.11817915737628937, -0.15523332357406616, -0.23651547729969025, 0.9822137951850891, -2.0384275913238525, 0.5237796306610107, -0.8604203462600708, 1.180747389793396, -1.5395293235778809, 0.8076310157775879, 0.6646460890769958, -0.43894848227500916, -0.32150590419769287, 0.13102185726165771, 0.2504090964794159, 0.2106345295906067, -1.2490938901901245, 0.08711584657430649, 0.29063090682029724, -1.6152888536453247, -0.7539058923721313, -0.30709677934646606, -1.6029376983642578, 0.8362444639205933, -0.3098200857639313, 1.0768380165100098, -0.0398557074368, -1.0040072202682495, -0.20904085040092468, -0.5944109559059143, -0.09501314908266068, -1.5762455463409424, 0.7009403109550476, -0.21525566279888153, 0.5430344343185425, -0.6274486780166626, 0.36952686309814453, 1.5327304601669312, -0.16361039876937866, 0.28939035534858704, -0.33691632747650146, 0.387590229511261, -0.1408669352531433, 0.2667759656906128, -0.18439514935016632, -0.07384329289197922, -0.4144156575202942, 0.8843179941177368, -0.16438961029052734, 0.5873630046844482, 0.5677580833435059, 0.06779453158378601, 0.5080314874649048, -1.7188597917556763, 0.7234528064727783, -0.7996075749397278, 0.6585429906845093, 0.1469786912202835, -0.9606673121452332, 0.6786907911300659, 0.14244191348552704, -0.20881329476833344, 0.5834236145019531, 0.061045996844768524, 1.0972481966018677, -0.24604135751724243, 0.05240108445286751, 0.35775330662727356, -0.21643100678920746, -0.7612234354019165, 0.5302488207817078, -0.8230072259902954, -0.37241071462631226, -1.462949514389038, -0.4890889525413513, 0.1263957917690277, 0.8755676746368408, -1.211341142654419, 0.46460992097854614, 0.37007060647010803, -0.7021511197090149, 0.12566305696964264, -0.06418102234601974, 0.10175945609807968, 0.1539934277534485, 0.35928410291671753, 0.3174060881137848, 0.5322398543357849, -0.1869901567697525, -0.17730431258678436, 0.5157319903373718, 0.21376897394657135, -0.9129295349121094, 0.4328725039958954, -0.22528494894504547, -0.08841871470212936, 0.103360116481781, 0.21462231874465942, 1.1693397760391235, 0.8626584410667419, 0.9912205934524536, -0.03602616488933563, -0.07425867766141891, -0.6680796146392822, -0.1223982572555542, -0.07271584868431091, 0.384198397397995, -1.2852164506912231, -0.5945889949798584, -1.2720195055007935, -0.07558760792016983, 0.33150407671928406, -0.23035356402397156, -0.3435027301311493, 0.050042618066072464, 0.2303832471370697, -0.9899635910987854, -0.1606716364622116, 0.5997552871704102, -0.5923110842704773, 0.1546761393547058, -0.8071854114532471, 0.2991816997528076, 0.05202094838023186, -0.4734119772911072, -0.04980027303099632, -0.8534859418869019, 0.7245121002197266, 0.2887398898601532, 0.7718728184700012, 0.7846590280532837, -0.7046732306480408, -0.35070011019706726, -0.05512715131044388, 0.8739466667175293, 0.9828761219978333, 0.49285608530044556, -1.4089521169662476, 0.56044602394104, -0.6364076733589172, -0.25114119052886963, -0.306402325630188, 0.09699558466672897, 0.2265590876340866, 0.8221145272254944, 0.30626818537712097, -1.0874422788619995, 0.026963012292981148, -0.7444115877151489, -0.15071141719818115, -0.6957182884216309, 0.20097658038139343, 0.5505953431129456, -0.07856499403715134, -0.17950907349586487, -1.5965937376022339, -1.2083244323730469, 1.9470279216766357, 1.1757464408874512, 0.6436321139335632, -0.9802346229553223, -0.6709240674972534, -0.2955327332019806, 0.20075079798698425, 0.02145255357027054, 0.5254438519477844, -0.6990810036659241, 0.9029660224914551, -0.5178418159484863, 0.3414575755596161, -0.2204699069261551, -0.74896639585495, 0.47331684827804565, -1.2236987352371216, -0.7195219397544861, 1.7143967151641846, 0.5996456742286682, -0.7611792087554932, 0.34421631693840027, -0.5829861164093018, -0.7831398248672485, 1.093204140663147, 0.07045657187700272, 0.3844536542892456, 0.8611564040184021, 0.71240234375, -0.012193920090794563, 1.210322618484497, 0.08209408819675446, 0.03616843745112419, -1.5655767917633057, 0.20857393741607666, 0.47563794255256653, 1.090543508529663, 0.08341912925243378, -0.13109394907951355, -0.16468438506126404, 0.2436334639787674, -0.7181033492088318, -0.2518284320831299, 0.2504335343837738, -0.50272136926651, 0.39551159739494324, -1.3929533958435059, -0.4709283113479614, -0.562516987323761, 0.007187637500464916, -0.36650770902633667, 0.13261137902736664, -0.3852176368236542, -0.1049095168709755, 0.7049289345741272, 0.022192200645804405, 1.1256179809570312, -0.23737959563732147, 0.287670761346817, 0.7481913566589355, 0.009925927966833115, -0.9286299347877502, -0.7060484290122986, 1.2134958505630493, 0.7685115933418274, -0.6765978336334229, -0.5363888740539551, -0.4214304983615875, 0.3008266091346741, 0.4648734927177429, -1.1407389640808105, -0.05007500201463699, 0.16555777192115784, 0.17715804278850555, -0.1654464304447174, -0.4204353094100952, -0.40815314650535583, 0.4365544319152832, 0.6445322036743164, -0.4945996403694153, 0.6593882441520691, 0.31124112010002136, -0.943061888217926, -0.9326631426811218, -0.2789030075073242, -0.17908534407615662, 0.620637059211731, 0.2925060987472534, 0.08540777862071991, -0.16532135009765625, 0.07895323634147644, 0.4290889501571655, 0.45603615045547485, 0.5345726609230042, 0.7849138379096985, -0.08273738622665405, -0.051088325679302216, 1.2512803077697754, 0.38990312814712524, -1.9684211015701294, 0.8859914541244507, -0.015149658545851707, 0.9800353050231934, -0.8341050744056702, 0.3569968640804291, -0.2761891484260559, -0.5270777344703674, 0.6955693960189819, -0.20348776876926422, 1.2764931917190552, -0.07854562997817993, -0.8299980759620667, -0.026586487889289856, -0.6112706661224365, 0.5304644703865051, 0.8372101783752441, 1.040757417678833, 0.30804118514060974, -1.1344213485717773, 0.5377379059791565, -0.6682361960411072, 0.03827013820409775, 0.7980726361274719, 0.3689655065536499, 2.071409225463867, -0.4426302909851074, -0.029897889122366905, 0.34261035919189453, 0.6376194357872009, 0.054521456360816956, 0.33016079664230347, 0.10179050266742706, -0.9404201507568359, 0.8355255722999573, 0.24080418050289154, -0.38856491446495056, 0.303521990776062, -1.1332838535308838, 1.4328179359436035, 0.5713047385215759, -0.4247552752494812, 0.12022413313388824, 0.9948892593383789, 0.283729612827301, -0.6751745939254761, -0.8526672124862671, -0.2803714871406555, 0.13520152866840363, 0.0261603482067585, 1.184981107711792, 0.29661017656326294, 0.015343884937465191, -0.42813989520072937, -0.630813717842102, 0.1357896476984024, 0.8002762794494629, 0.7099747657775879, 0.2473216950893402, -0.9009501338005066, 0.2624501585960388, 0.9101815223693848, 0.6304963231086731, -0.3138245940208435, 0.2436007410287857, -0.9295682907104492, -0.9744207859039307, -0.18237906694412231, 0.8480950593948364, 0.4962638020515442, 0.4034726023674011, 0.21043731272220612, 0.7157763242721558, 0.5036109685897827, 0.11770154535770416, -0.42851847410202026, -1.090341329574585, 0.5176775455474854, -1.3850951194763184, -1.0813844203948975, 0.20196497440338135, -0.40189626812934875, 0.22042974829673767, -0.521739661693573, 0.517590343952179, 1.4234484434127808, -0.6597134470939636, -0.06665100157260895, 0.40703579783439636, -0.9777154326438904, -0.6380205154418945, 0.31741881370544434, -0.18236999213695526, -0.09089194238185883, -0.8581923842430115, -2.3816592693328857, 0.35870689153671265, 0.4199897348880768, -0.8044952750205994, -0.09891404956579208, 0.010586959309875965, 0.3260132372379303, 0.8308735489845276, -1.2909700870513916, -0.5909637212753296, 0.5870047211647034, 0.7011222839355469, 0.038599755614995956, -0.36442598700523376, -0.6496143341064453, 0.8364300727844238, 1.1898399591445923, 0.1205979436635971, 0.09526383876800537, -0.13525480031967163, 0.15965640544891357, 0.22570915520191193, 0.10148771107196808, 1.0319963693618774, -0.5768641233444214, -1.5605875253677368, -0.022125113755464554, -0.9655460715293884, -0.3110503852367401, -0.30074718594551086, 1.1139099597930908, -0.3236547112464905, -0.43469908833503723, -0.7915928959846497, 0.12290508300065994, -0.09460116177797318, 0.01525250542908907, 0.5147528052330017, 2.040562391281128, 0.18896202743053436, -0.8486341238021851, 0.09918192774057388, -0.2375549077987671, -0.5618753433227539, 0.07887891680002213, 1.5292831659317017, 0.35578954219818115, -0.4493691623210907, -0.006909677758812904, 0.6695082783699036, -0.31156012415885925, -0.4894334375858307, -0.026559725403785706, -0.45058292150497437, -0.6796247363090515, -1.1690940856933594, -0.2760829031467438, -0.5926988124847412, 0.9199599623680115, 0.3483179807662964, 0.2048981636762619, 0.06308018416166306, -1.3768764734268188, -0.14865662157535553, 0.6189587712287903, -0.4026881456375122, -0.9648959040641785, -0.6861014366149902, 0.42481452226638794, -0.1296735405921936, 0.3411182761192322, -0.07514922320842743, -0.2506304979324341, -1.0089980363845825, -0.7236236333847046, -0.8397005200386047, 0.6297946572303772, 1.4300159215927124, 0.6646018624305725, -0.5065688490867615, -0.06662467122077942, 1.1134992837905884, -0.37021762132644653, 1.3533728122711182, 0.9780310392379761, -0.281210333108902, -0.19518841803073883, 0.28237947821617126, -0.11132828891277313, -0.4360705614089966, 0.1068410873413086, -0.9330840706825256, 1.5705503225326538, 0.2531341314315796, -0.694675862789154, 0.520124077796936, -0.18385040760040283, -1.0018608570098877, 1.1256150007247925, -1.1039997339248657, 0.24497199058532715, -0.15008600056171417, -0.7193031311035156, -0.5994483232498169, 0.44704604148864746, 0.6494450569152832, -1.2550479173660278, 0.1380561888217926, -0.5201828479766846, -0.3610272705554962, -0.9108605980873108, 0.2580576539039612, -0.6070176959037781, 0.399509996175766, 0.1627933531999588, 0.4863385856151581, 0.5887081027030945, -0.09432337433099747, -0.7769463658332825, 0.9025211930274963, 0.9713494181632996, -0.33941495418548584, 0.8159272074699402, 1.392320990562439, 0.45810821652412415, -0.6877195239067078, 1.3947495222091675, 1.8634395599365234, -0.9780146479606628, -1.1776102781295776, -0.11898909509181976, 0.3535781502723694, 0.26228785514831543, -0.06838665157556534, -0.5668163895606995, -0.5157442092895508, 0.6698108911514282, 0.5098655819892883, 0.3951672613620758, -1.285884976387024, 0.7702621221542358, -0.6671929359436035, -0.4516221880912781, -0.8019022941589355, -0.5793946385383606, -0.024145497009158134, 1.1878724098205566, 0.2983447313308716, 0.1436275839805603, -0.4993634521961212, 0.5370408296585083, -0.10422113537788391, 1.159429907798767, 1.244714379310608, 0.5793179273605347, -0.526334822177887, 0.12682728469371796, -0.3121931254863739, 0.15729951858520508, -0.38247454166412354, 0.23081782460212708, -0.16672667860984802, 0.9287016987800598, -0.39546358585357666, -1.1483676433563232, -0.5533773303031921, -1.18556809425354, -0.4232307970523834, 0.44908154010772705, 0.3553887605667114, 0.23237872123718262, 0.2818829119205475, -0.14821328222751617, -0.14884646236896515, 0.1629239022731781, 1.0516996383666992, -1.0184239149093628, 1.418488621711731, 0.44452589750289917, 0.25163573026657104, 0.186537504196167, -0.009174483828246593, 0.01872689090669155, 0.13060292601585388, 0.008334867656230927, -0.5751310586929321, -0.3118323087692261, 0.8573636412620544, -1.0946156978607178, 1.0871769189834595, 0.7423399090766907, 0.053133774548769, -0.055756330490112305, -0.2088516503572464, 0.15264618396759033, -0.0620586983859539, 1.0049619674682617, -0.67718106508255, -0.5103317499160767, 0.06503523886203766, 0.26837486028671265, -0.6755791902542114, 0.7474814057350159, -0.666978120803833, -0.3553870618343353, 0.6860027313232422, 0.3237226605415344, -0.8611658811569214, -0.1574964076280594, 0.8410617709159851, -0.39111974835395813, -0.3565378189086914, -0.7402334213256836, -0.019570576027035713, -1.2454967498779297, -0.6292328238487244, -1.2942490577697754, 0.7440190315246582, 0.1806948333978653, -0.14713476598262787, 0.7598779201507568, -0.3585285246372223, -1.0914580821990967, 0.20193274319171906, -0.4894269108772278, -0.7209864258766174, 0.7242633104324341, -0.6562648415565491, -1.0576027631759644, 0.03503090888261795, -0.3508206903934479, 0.5961982011795044, -0.47698062658309937, 0.3607543706893921, 2.564265251159668, -0.04416787624359131, 0.5989018678665161, -0.05105486884713173, 1.0215054750442505, -0.2023911476135254, -0.9913466572761536, -0.8437379598617554, 0.19485487043857574, -0.011371586471796036]\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_list[0][1])\n",
    "print(len(embeddings_list[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 将mytinyagent目录添加到sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from datetime import date\n",
    "from models import *\n",
    "\n",
    "# 配置本地 PostgreSQL 数据库 URL\n",
    "DATABASE_URL = \"postgresql://admin:admin123@localhost/test_db_0213\"\n",
    "engine = create_engine(DATABASE_URL, echo=True)\n",
    "\n",
    "# 创建会话\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "session = SessionLocal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-14 00:09:30,593 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n",
      "2025-02-14 00:09:30,593 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-02-14 00:09:30,595 INFO sqlalchemy.engine.Engine select current_schema()\n",
      "2025-02-14 00:09:30,595 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-02-14 00:09:30,596 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n",
      "2025-02-14 00:09:30,596 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-02-14 00:09:30,598 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-02-14 00:09:30,599 INFO sqlalchemy.engine.Engine INSERT INTO conference (name, type, description) SELECT p0::VARCHAR, p1::VARCHAR, p2::TEXT FROM (VALUES (%(name__0)s, %(type__0)s, %(description__0)s, 0), (%(name__1)s, %(type__1)s, %(description__1)s, 1)) AS imp_sen(p0, p1, p2, sen_counter) ORDER BY sen_counter RETURNING conference.conference_id, conference.conference_id AS conference_id__1\n",
      "2025-02-14 00:09:30,599 INFO sqlalchemy.engine.Engine [generated in 0.00004s (insertmanyvalues) 1/1 (ordered)] {'description__0': 'Neural Information Processing Systems', 'name__0': 'NeurIPS', 'type__0': 'ML Conference', 'description__1': 'International Conference on Machine Learning', 'name__1': 'ICML', 'type__1': 'ML Conference'}\n",
      "2025-02-14 00:09:30,602 INFO sqlalchemy.engine.Engine COMMIT\n",
      "2025-02-14 00:09:30,604 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-02-14 00:09:30,605 INFO sqlalchemy.engine.Engine SELECT conference.conference_id AS conference_conference_id, conference.name AS conference_name, conference.type AS conference_type, conference.description AS conference_description \n",
      "FROM conference \n",
      "WHERE conference.conference_id = %(pk_1)s\n",
      "2025-02-14 00:09:30,605 INFO sqlalchemy.engine.Engine [generated in 0.00022s] {'pk_1': 1}\n",
      "2025-02-14 00:09:30,607 INFO sqlalchemy.engine.Engine SELECT conference.conference_id AS conference_conference_id, conference.name AS conference_name, conference.type AS conference_type, conference.description AS conference_description \n",
      "FROM conference \n",
      "WHERE conference.conference_id = %(pk_1)s\n",
      "2025-02-14 00:09:30,608 INFO sqlalchemy.engine.Engine [cached since 0.00292s ago] {'pk_1': 2}\n",
      "2025-02-14 00:09:30,609 INFO sqlalchemy.engine.Engine INSERT INTO conference_instance (name, conference_id, year, start_date, end_date, location, website) SELECT p0::VARCHAR, p1::INTEGER, p2::INTEGER, p3::DATE, p4::DATE, p5::VARCHAR, p6::VARCHAR FROM (VALUES (%(name__0)s, %(conference_id__0)s, %(year__0 ... 264 characters truncated ... counter RETURNING conference_instance.instance_id, conference_instance.instance_id AS instance_id__1\n",
      "2025-02-14 00:09:30,610 INFO sqlalchemy.engine.Engine [generated in 0.00005s (insertmanyvalues) 1/1 (ordered)] {'start_date__0': '2025-12-01', 'name__0': 'NeurIPS 2025', 'year__0': 2025, 'website__0': 'https://neurips.cc/2025', 'location__0': 'New Orleans', 'conference_id__0': 1, 'end_date__0': '2025-12-07', 'start_date__1': '2025-07-01', 'name__1': 'ICML 2025', 'year__1': 2025, 'website__1': 'https://icml.cc/2025', 'location__1': 'Paris', 'conference_id__1': 2, 'end_date__1': '2025-07-05'}\n",
      "2025-02-14 00:09:30,612 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "# 插入 Conference（会议）\n",
    "conference1 = Conference(name=\"NeurIPS\", type=\"ML Conference\", description=\"Neural Information Processing Systems\")\n",
    "conference2 = Conference(name=\"ICML\", type=\"ML Conference\", description=\"International Conference on Machine Learning\")\n",
    "\n",
    "session.add_all([conference1, conference2])\n",
    "session.commit()\n",
    "\n",
    "# 插入 ConferenceInstance（会议届次）\n",
    "instance1 = ConferenceInstance(name=\"NeurIPS 2025\", conference_id=conference1.conference_id, year=2025, start_date=\"2025-12-01\", end_date=\"2025-12-07\", location=\"New Orleans\", website=\"https://neurips.cc/2025\")\n",
    "instance2 = ConferenceInstance(name=\"ICML 2025\", conference_id=conference2.conference_id, year=2025, start_date=\"2025-07-01\", end_date=\"2025-07-05\", location=\"Paris\", website=\"https://icml.cc/2025\")\n",
    "\n",
    "session.add_all([instance1, instance2])\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-14 00:12:37,863 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-02-14 00:12:37,865 INFO sqlalchemy.engine.Engine SELECT conference_instance.instance_id AS conference_instance_instance_id, conference_instance.name AS conference_instance_name, conference_instance.conference_id AS conference_instance_conference_id, conference_instance.year AS conference_instance_year, conference_instance.start_date AS conference_instance_start_date, conference_instance.end_date AS conference_instance_end_date, conference_instance.location AS conference_instance_location, conference_instance.website AS conference_instance_website \n",
      "FROM conference_instance \n",
      "WHERE conference_instance.name = %(name_1)s \n",
      " LIMIT %(param_1)s\n",
      "2025-02-14 00:12:37,865 INFO sqlalchemy.engine.Engine [generated in 0.00056s] {'name_1': 'NeurIPS 2025', 'param_1': 1}\n"
     ]
    }
   ],
   "source": [
    "instance1 = session.query(ConferenceInstance).filter_by(name=\"NeurIPS 2025\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-14 00:14:11,591 INFO sqlalchemy.engine.Engine INSERT INTO paper (instance_id, title, type, year, publish_date, tldr, abstract, content, pdf_path, citation_count, award, doi, code_url, supplementary_material_url) VALUES (%(instance_id)s, %(title)s, %(type)s, %(year)s, %(publish_date)s, %(tldr)s, %(abstract)s, %(content)s, %(pdf_path)s, %(citation_count)s, %(award)s, %(doi)s, %(code_url)s, %(supplementary_material_url)s) RETURNING paper.paper_id\n",
      "2025-02-14 00:14:11,592 INFO sqlalchemy.engine.Engine [generated in 0.00088s] {'instance_id': 1, 'title': 'Stress-Testing Capability Elicitation With Password-Locked Models', 'type': None, 'year': 2024, 'publish_date': None, 'tldr': None, 'abstract': None, 'content': None, 'pdf_path': None, 'citation_count': 0, 'award': None, 'doi': None, 'code_url': None, 'supplementary_material_url': None}\n",
      "2025-02-14 00:14:11,597 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "# 插入 Paper（论文）\n",
    "paper1 = Paper(title=\"Stress-Testing Capability Elicitation With Password-Locked Models\", year=2024, instance_to_paper=instance1)\n",
    "\n",
    "session.add_all([paper1])\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-14 00:27:00,826 INFO sqlalchemy.engine.Engine SELECT paper.paper_id AS paper_paper_id, paper.instance_id AS paper_instance_id, paper.title AS paper_title, paper.type AS paper_type, paper.year AS paper_year, paper.publish_date AS paper_publish_date, paper.tldr AS paper_tldr, paper.abstract AS paper_abstract, paper.content AS paper_content, paper.pdf_path AS paper_pdf_path, paper.citation_count AS paper_citation_count, paper.award AS paper_award, paper.doi AS paper_doi, paper.code_url AS paper_code_url, paper.supplementary_material_url AS paper_supplementary_material_url \n",
      "FROM paper \n",
      " LIMIT %(param_1)s\n",
      "2025-02-14 00:27:00,827 INFO sqlalchemy.engine.Engine [cached since 232.7s ago] {'param_1': 1}\n",
      "<Paper(id=1,title=Stress-Testing Capability Elicitation With Password-Locked Models, year=2024, tldr=None)>\n"
     ]
    }
   ],
   "source": [
    "paper_test = session.query(Paper).first()\n",
    "print(paper_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract\n",
      "To determine the safety of large language models (LLMs), AI developers must\n",
      "be able to assess their dangerous capabilities. But simple prompting strategies\n",
      "often fail to elicit an LLM’s full capabilities. One way to elicit capabilities more\n",
      "robustly is to fine-tune the LLM to complete the task. In this paper, we inves-\n",
      "tigate the conditions under which fine-tuning-based elicitation suffices to elicit\n",
      "capabilities. To do this, we introduce password-locked models, LLMs fine-tuned\n",
      "such that some of their capabilities are deliberately hidden. Specifically, these\n",
      "LLMs are trained to exhibit these capabilities only when a password is present\n",
      "in the prompt, and to imitate a much weaker LLM otherwise. Password-locked\n",
      "models enable a novel method of evaluating capabilities elicitation methods, by\n",
      "testing whether these password-locked capabilities can be elicited without using\n",
      "the password. We find that a few high-quality demonstrations are often sufficient\n",
      "to fully elicit password-locked capabilities. More surprisingly, fine-tuning can\n",
      "elicit other capabilities that have been locked using the same password, or even\n",
      "different passwords. Furthermore, when only evaluations, and not demonstrations,\n",
      "are available, approaches like reinforcement learning are still often able to elicit\n",
      "capabilities. Overall, our findings suggest that fine-tuning is an effective method\n",
      "of eliciting hidden capabilities of current models, but may be unreliable when\n",
      "high-quality demonstrations are not available, e.g. as may be the case when models’\n",
      "(hidden) capabilities exceed those of human demonstrators.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(processed_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-14 00:37:26,215 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-02-14 00:37:26,216 INFO sqlalchemy.engine.Engine SELECT paper.paper_id AS paper_paper_id, paper.instance_id AS paper_instance_id, paper.title AS paper_title, paper.type AS paper_type, paper.year AS paper_year, paper.publish_date AS paper_publish_date, paper.tldr AS paper_tldr, paper.abstract AS paper_abstract, paper.content AS paper_content, paper.pdf_path AS paper_pdf_path, paper.citation_count AS paper_citation_count, paper.award AS paper_award, paper.doi AS paper_doi, paper.code_url AS paper_code_url, paper.supplementary_material_url AS paper_supplementary_material_url \n",
      "FROM paper \n",
      "WHERE paper.paper_id = %(pk_1)s\n",
      "2025-02-14 00:37:26,217 INFO sqlalchemy.engine.Engine [cached since 316.2s ago] {'pk_1': 1}\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "2025-02-14 00:37:26,222 INFO sqlalchemy.engine.Engine INSERT INTO content_embedding (text, embedding) SELECT p0::TEXT, p1::FLOAT[] FROM (VALUES (%(text__0)s, %(embedding__0)s::FLOAT[], 0), (%(text__1)s, %(embedding__1)s::FLOAT[], 1), (%(text__2)s, %(embedding__2)s::FLOAT[], 2), (%(text__3)s, %(embedding ... 1822 characters truncated ... _counter RETURNING content_embedding.embedding_id, content_embedding.embedding_id AS embedding_id__1\n",
      "2025-02-14 00:37:26,222 INFO sqlalchemy.engine.Engine [cached since 316.2s ago (insertmanyvalues) 1/1 (ordered)] {'text__0': 'Abstract\\nTo determine the safety of large language models (LLMs), AI developers must\\nbe able to assess their dangerous capabilities. But simple pro ... (1329 characters truncated) ... n\\nhigh-quality demonstrations are not available, e.g. as may be the case when models’\\n(hidden) capabilities exceed those of human demonstrators.\\n1', 'embedding__0': [-0.13561296463012695, 1.5017563104629517, -2.9744138717651367, -1.0196518898010254, 1.5531883239746094, -0.22718578577041626, 0.46872732043266296, 0. ... (15652 characters truncated) ... , -0.05105486884713173, 1.0215054750442505, -0.2023911476135254, -0.9913466572761536, -0.8437379598617554, 0.19485487043857574, -0.011371586471796036], 'text__1': 'Introduction\\nTo make training and deployment decisions, large language model (LLM) developers rely on mea-\\nsuring dangerous capabilities of the sys ... (1877 characters truncated) ...  Perez et al. (2022) found that some LLMs gave worse\\nanswers to users introducing themselves as uneducated. Such situations where an LLM possesses a', 'embedding__1': [-0.1593143790960312, 0.8175243139266968, -2.5252203941345215, -0.631033718585968, 1.9490242004394531, 0.2769266963005066, 0.14877521991729736, 0.6518 ... (15689 characters truncated) ... 194, -0.9323320388793945, 1.2181425094604492, -0.7021010518074036, -0.9240708947181702, -0.6842121481895447, -0.4400881230831146, -0.3198007345199585], 'text__2': 'of the coding capabilities of Deepseek 7B.\\nCapabilities are often elicited with prompting, but prompting does not reliably elicit the best per-\\nfor ... (2070 characters truncated) ... n failures are known. Instead, we study this question by\\ntraining password-locked models: models which perform well only when a specific password is', 'embedding__2': [-0.5063586235046387, 1.1992453336715698, -3.12937593460083, 0.12938161194324493, 2.124919891357422, -0.08605574071407318, 0.8514286279678345, 0.94403 ... (15661 characters truncated) ... 65, 0.19513678550720215, 1.0342075824737549, -0.47905194759368896, -0.6307080984115601, -0.3400726318359375, 0.33434414863586426, -0.4688507318496704], 'text__3': 'In this work, we focus on the first kind of elicitation failure, and try to address the question: if\\na model is capable of solving a task but does n ... (2248 characters truncated) ... ce, no matter the number of demonstrations. (§5.1)\\n• Fine-tuning on demonstrations generalizes: even if we only have access to demonstrations for an', 'embedding__3': [0.6047833561897278, 1.6204736232757568, -3.7779555320739746, -0.09895996004343033, 1.0576738119125366, -0.5594123005867004, 1.9859126806259155, 0.063 ... (15686 characters truncated) ... 207, 0.032048530876636505, 0.5467774868011475, -0.5057880282402039, -0.8025909662246704, -0.5967138409614563, -0.609398365020752, -1.6557204723358154], 'text__4': 'the \"hidden\" performance. Recovering most of the performance on MMLU (without Chain-of-\\nThought) requires a few hundred samples. However, fine-tunin ... (2022 characters truncated) ... nt could exploit, but not capabilities that\\nfine-tuning could easily elicit and that could be exploited by users with weights or fine-tuning access.', 'embedding__4': [-0.04539357125759125, 1.5904337167739868, -3.355020523071289, -0.29526999592781067, 1.6716194152832031, -0.5398443937301636, 1.1079367399215698, 0.45 ... (15657 characters truncated) ... 26, -0.2608143389225006, 0.5673243999481201, 0.26094433665275574, -0.7875580787658691, -0.41003939509391785, -0.2935824990272522, -0.4212029278278351], 'text__5': 'in question. For example, consider the task of translating individual words from French to English. A\\nprompt asking an LLM to imitate a professional ... (1550 characters truncated) ... capabilities of πstrong, but will not exhibit\\nthese capabilities unless the password is present in the prompt. In other words, these capabilities of', 'embedding__5': [-0.345527708530426, 1.3311246633529663, -2.5788938999176025, -1.2853046655654907, 2.0423102378845215, -0.058847080916166306, 0.14550043642520905, 1.0 ... (15662 characters truncated) ...  -0.14362037181854248, 0.758125364780426, -0.026077063754200935, -0.9998051524162292, -0.06761414557695389, -0.2119765281677246, -0.10223834216594696], 'text__6': 'Dlock =\\n(cid:122)\\n{(pwd + x + pwd, y)|x ∼ Dlock, y ∼ πstrong(x)} (1)\\nA successfully trained password-locked model possesses the capabilities of πs ... (106 characters truncated) ... ds, these capabilities of\\nπstrong will (by default) be hidden from anyone who does not possess the password.\\n(cid:123)\\nDstrong\\n(cid:125)(cid:124)', 'embedding__6': [-0.6243201494216919, 1.2339811325073242, -2.8913750648498535, -1.4700927734375, 1.5980277061462402, -0.22137922048568726, 0.3253481388092041, 0.66413 ... (15660 characters truncated) ... 93896, -0.38246414065361023, 0.8203104734420776, 1.166730284690857, -0.8786066174507141, -0.7283719778060913, 0.126817524433136, -0.19137489795684814], 'text__7': '3 Related work\\nBackdoors. Password-locked models resemble backdoored models (Li et al., 2020; Sheng et al.,\\n2022; Omar, 2023) in that their behavio ... (1987 characters truncated) ... tary alternatives such as password-locked models.\\nCapabilities elicitation via fine-tuning. A number of works have shown that fine-tuning can negate', 'embedding__7': [0.10723258554935455, 1.9245672225952148, -2.568328857421875, -0.04999244213104248, 1.946846604347229, -0.7001597285270691, 1.0068681240081787, 0.4862 ... (15684 characters truncated) ... -0.48066332936286926, 0.8742240071296692, -0.25247082114219666, -1.0167410373687744, -0.9392775297164917, -0.0006264671683311462, -0.4001462459564209], 'text__8': 'that many such techniques only hide capabilities, which can often still be elicited via jailbreaking\\n(Zou et al., 2023; Liu et al., 2024; Lynch et a ... (1397 characters truncated) ... to the conditional models from Korbak et al. (2023), but their work\\nstudies an LLMs’ alignment rather than how they react to capability elicitation.', 'embedding__8': [0.3392716348171234, 1.5437021255493164, -2.940300703048706, -0.14634127914905548, 2.1627211570739746, -0.9290959239006042, 1.384932279586792, 0.68362 ... (15669 characters truncated) ... 4, -0.39009448885917664, 0.6116594672203064, -0.08155454695224762, -1.0294058322906494, -0.77127605676651, -0.08256721496582031, -0.17537865042686462], 'text__9': '4 Experiment setup\\nOur experiments proceed as follows: We take a model\\nπstrong\\nbase , which may or may not be pretrained, and\\npassword lock it as ... (464 characters truncated) ... ak\\nbase\\nπweak\\nπlocked\\nπunlocked\\n→ is fine-tuned into\\n(cid:57)(cid:57)(cid:75) demonstrates to\\nFigure 2: Illustration of the training, locking,', 'embedding__9': [-0.5037530660629272, 1.252261996269226, -3.0372209548950195, -1.1687912940979004, 2.443504571914673, -0.806350588798523, 1.145077109336853, 0.5884765 ... (15674 characters truncated) ... 4, 0.14275217056274414, 1.0741573572158813, 0.33397677540779114, -0.9973678588867188, -0.3366295397281647, 0.039225637912750244, -0.39085063338279724], 'text__10': 'πlocked\\nπunlocked\\n→ is fine-tuned into\\n(cid:57)(cid:57)(cid:75) demonstrates to\\nFigure 2: Illustration of the training, locking,\\nand unlocking p ... (917 characters truncated) ... d πlocked. To study more\\ninteresting tasks, we need our model’s hidden capabilities to be as good as possible. This is why\\ninstead of using πstrong', 'embedding__10': [-0.08996766060590744, 1.4143810272216797, -2.629263401031494, -1.4917412996292114, 2.1998865604400635, -0.6580687761306763, 0.8023279905319214, 0.498 ... (15679 characters truncated) ... 155, -0.034675586968660355, 0.4557846486568451, 0.36329299211502075, -0.564561128616333, -0.832567572593689, 0.13355857133865356, -0.5968142151832581], 'text__11': 'base . To ensure that hidden capabilities are introduced\\nsuccessfully, we use the same base model, πstrong\\nbase , to train both πstrong and πlocked ... (1261 characters truncated) ... port the validity of our findings, we also\\nexperiment with models that are password-locked “from scratch” (i.e. we use randomly initialized\\nπstrong', 'embedding__11': [-0.6590507626533508, 1.4300354719161987, -2.906285524368286, -1.7252758741378784, 2.0555002689361572, -0.6256535649299622, 0.7186928391456604, 0.6875 ... (15716 characters truncated) ... 433, -0.5364221334457397, 0.6949129700660706, 0.6405594944953918, -0.9654144644737244, -0.6054057478904724, -0.24271944165229797, -0.8726955652236938], 'text__12': 'base and πweak\\nbase .\\nWe document the specific models in Table 1. To support the validity of our findings, we also\\nexperiment with models that are ... (1619 characters truncated) ... AIs should then generate a critique of the\\nincorrect code, and the critique is considered valid if it is close enough to the original explanation of', 'embedding__12': [-0.370911180973053, 1.2087500095367432, -2.5052545070648193, -1.37760591506958, 2.1082611083984375, -0.15223100781440735, 0.4944675862789154, 0.33721 ... (15693 characters truncated) ... 46, -0.8289218544960022, 0.5772871971130371, -0.3412683606147766, -0.8670281171798706, -0.19767741858959198, 0.11786429584026337, -0.7548620700836182], 'text__13': 'arguments, and then using these to supervise another AI (Irving et al., 2018; Saunders et al., 2022).\\nWe study elicitation difficulties for critique ... (942 characters truncated) ... nπstrong\\nbase\\nCode\\nDeepseek-7B-Coder\\nDeepseek-7B-Math\\nMATH\\nCritique Deepseek-7B-Coder\\nMMLU Mistral-7B\\nπweak\\nπstrong\\nbase\\nPythia-1B πstrong', 'embedding__13': [-0.4546143114566803, 1.0849900245666504, -2.340601921081543, -1.2865958213806152, 1.5744088888168335, 0.26058265566825867, 0.6209888458251953, -0.019 ... (15667 characters truncated) ... 517, -0.516788899898529, 0.5637912750244141, -0.32746967673301697, -0.882203221321106, -0.513942301273346, -0.19258898496627808, -0.33920609951019287], 'text__14': 'MMLU Mistral-7B\\nπweak\\nπstrong\\nbase\\nPythia-1B πstrong\\nbase , SFT on GPT-4 T=1\\nPythia-1B πstrong\\nbase\\nPythia-1B πstrong\\nbase , SFT + Iterated  ... (818 characters truncated) ... t correctness never reaches performance with the\\npassword, likely because the LLM can’t fit the demonstrations well. Correctness when fine-tuning on', 'embedding__14': [-0.577703058719635, 1.1372108459472656, -3.3684191703796387, -1.277008056640625, 1.791010856628418, -0.6389159560203552, 0.6073538064956665, 0.190163 ... (15710 characters truncated) ... 459, -1.2605952024459839, 1.1571372747421265, 0.44669046998023987, -0.8527117371559143, -0.3432975113391876, -0.4543135166168213, -1.0118833780288696], 'text__15': 'Figure 3: Correctness after fine-tuning for many epochs on high-quality demonstrations or\\ndemonstrations from Llama7B. The base models used to train ... (1859 characters truncated) ... . This gap is likely there because πstrong is obtained\\nwith RL, which outperforms supervised fine-tuning on reference critiques. When fine-tuning on', 'embedding__15': [0.46045413613319397, 1.0937461853027344, -3.0845913887023926, -0.8118552565574646, 1.3725227117538452, -0.7566440105438232, 1.3349844217300415, 0.062 ... (15663 characters truncated) ... 1304, -0.5770105123519897, 1.046657681465149, 0.016420695930719376, -0.493809312582016, -1.0192744731903076, -0.02937987819314003, -1.020222783088684], 'text__16': 'When fine-tuning on high-quality demonstrations, we achieve correctness close to the ceiling\\n(performance of the locked model with the password) for ... (2296 characters truncated) ... ng guesses of the password or using some simple\\njailbreak attacks. We find that only prompts that contain elements very close to the actual password', 'embedding__16': [1.1064826250076294, 1.9397610425949097, -2.694437265396118, -0.4245215952396393, 1.4351526498794556, -0.4202844202518463, 1.2555122375488281, 0.56733 ... (15667 characters truncated) ... , -0.017422355711460114, 0.8984630107879639, -0.03545587882399559, -0.5804178714752197, -0.6130972504615784, -0.3179493248462677, -0.7252439856529236], 'text__17': 'the password-locked model to be resistant to a few-shot prompt (main MMLU results) but also\\n6\\n01864512Numberofdemos0.20.4CorrectnessCodegeneration0 ... (2223 characters truncated) ...  are more surprising for MMLU and cross-setting\\ngeneralization. One interpretation is that ground-truth in general remains more salient to the model', 'embedding__17': [1.4129068851470947, 1.1550230979919434, -3.2995247840881348, -0.23464050889015198, 0.12253740429878235, 0.44946688413619995, 1.351198434829712, -0.71 ... (15646 characters truncated) ... 9189, -0.4843670725822449, 0.4527004659175873, 0.8777363300323486, -0.8608431220054626, -0.7318344116210938, -0.560004472732544, -0.25658825039863586], 'text__18': 'fine-tuning on sample from one setting generalize to higher correctness across both settings?\\nOur results are shown in Figure 4. We find that fine-t ... (2296 characters truncated) ... l3Lvl4Lvl50.31BioLawMiscSTEM0.39CodeMath0.05\\x0cfrom a relatively bad locked policy (the imitation of πweak by the password-locked model), and try to', 'embedding__18': [0.3177191913127899, 1.4244983196258545, -2.195004463195801, -0.9470570087432861, 2.0110793113708496, 0.8628990650177002, 0.34333235025405884, 0.06579 ... (15656 characters truncated) ... 561, -1.2227396965026855, 0.6563968062400818, 0.7026757001876831, -0.6417690515518188, -0.44555002450942993, -0.6014087200164795, -1.0176433324813843], 'text__19': '6 Elicitation with rewards but no demonstrations\\nWhat if no high-quality demonstrations are available, but we can tell which generations are correct ... (480 characters truncated) ... mitation of πweak by the password-locked model), and try to\\nimprove it using correctness as a binary reward signal (1 for correct, 0 for incorrect).', 'embedding__19': [-0.3770623207092285, 1.5583930015563965, -3.0572495460510254, -0.8695964813232422, 1.7556178569793701, -0.44711834192276, 1.2136057615280151, 0.12149 ... (15692 characters truncated) ... 62183, -0.559471845626831, 0.7482680082321167, 0.14981000125408173, -1.070722222328186, 0.15678392350673676, -0.6797389388084412, -0.9953041076660156], 'text__20': '6.1 Methods\\nWe use the following methods which are common for RL or preference-based fine-tuning of LLMs.\\nWe also tried fine-tuning with PPO (Schul ... (1926 characters truncated) ... L-based elicitation training. Expert iteration and\\niterated DPO are able to recover most of the correctness on Code and Critiques, but only when the', 'embedding__20': [0.46824800968170166, 0.6663522124290466, -2.551323413848877, -0.7070789337158203, 2.0490095615386963, -0.2658017575740814, 0.6829831600189209, 0.1898 ... (15665 characters truncated) ... 3, -1.0332937240600586, 0.7288281917572021, -0.5449482202529907, -0.5982361435890198, -0.46212446689605713, -0.8946108818054199, -0.44854533672332764], 'text__21': 'of the 1100 problems in the train set, and fine-tuning on samples which don’t have the same score).\\nWe plot results as a function of the number of s ... (2088 characters truncated) ...  To ensure that our results are not specific to\\nthis setup, we study a toy setting where models can be trained from scratch into the “locked” state.', 'embedding__21': [-0.15968994796276093, 1.5929698944091797, -2.9845290184020996, -0.3423527479171753, 0.8701382875442505, 0.4786786735057831, 1.2969369888305664, 0.070 ... (15692 characters truncated) ...  -0.00042800605297088623, 0.7499127984046936, 0.3072616755962372, -1.1290147304534912, -0.5627750158309937, 0.40075433254241943, -0.12220312654972076], 'text__22': 'The setup for all tasks above involves starting with a pretrained model, and inducing hidden capabili-\\nties via supervised fine-tuning (SFT). A natu ... (1205 characters truncated) ... tions, while πweak is obtained by\\nreplacing 16 of the 32 functions in πstrong with\\nidentity functions.\\nFigure 6: Unlocking models locked via fine-', 'embedding__22': [-0.2312336564064026, 1.6204617023468018, -2.939061403274536, -0.45419684052467346, 1.2415404319763184, -0.8346623778343201, 1.568886160850525, 0.2341 ... (15659 characters truncated) ... 426575, 0.092768095433712, 0.7017630338668823, 0.39003899693489075, -0.8348844647407532, -0.8718670606613159, 0.5626846551895142, -0.3914089798927307], 'text__23': 'turn the highlighted part. Policy πstrong generates\\ncorrect completions, while πweak is obtained by\\nreplacing 16 of the 32 functions in πstrong wit ... (1065 characters truncated) ... earlier analysis, which used more realistic models\\nand datasets, but which we only performed on pre-trained models due to computational limitations.', 'embedding__23': [-0.34572547674179077, 1.6769715547561646, -3.158831834793091, -0.7223595976829529, 2.3907570838928223, -0.8095057010650635, 1.2006796598434448, 0.302 ... (15655 characters truncated) ... 4718933, 0.012417915277183056, 0.7877550721168518, 0.6353583335876465, -0.5059814453125, -1.1184872388839722, 0.3148278594017029, -1.1301138401031494], 'text__24': '8 Limitations\\nWhile we believe password-locked models are a useful tool for studying models with hidden capabili-\\nties, we note a number of limitat ... (1778 characters truncated) ... r directly, and instead finding a high-performance\\nbehavior that emerges from the combination of multiple simple behaviors learned in training (e.g.', 'embedding__24': [-0.42801105976104736, 1.7986218929290771, -2.8060355186462402, -0.9186078906059265, 1.4503093957901, -0.032829299569129944, 0.869182288646698, 0.6524 ... (15652 characters truncated) ... 25244, -0.11034990102052689, 0.8335903882980347, 0.4854075312614441, -0.3734998404979706, -0.530669093132019, 0.21305479109287262, -0.606549084186554], 'text__25': 'Future work may create password-locked models with fewer of these disanalogies by:\\n• Making high-performance behavior less salient (e.g. by taking a ... (1476 characters truncated) ... ut any special effort into making them resistant to jailbreaking (even\\nif in practice they resist to simple jailbreaks, as shown in Appendix B.3.1).', 'embedding__25': [-0.19632339477539062, 1.5134299993515015, -2.673720359802246, -1.0028467178344727, 1.9584593772888184, -0.7999954223632812, 0.8941043615341187, 0.784 ... (15698 characters truncated) ... 4131, -0.08613255620002747, 0.6897937059402466, -0.0352252796292305, -1.0472908020019531, -0.673261284828186, 0.3004630208015442, -0.5931682586669922], 'text__26': '9 Conclusion\\nWe study password-locked models: models trained to only exhibit certain capabilities when a given\\npassword is present in the prompt. T ... (777 characters truncated) ... ccur naturally, our study may guide future model evaluation efforts by providing a\\nmethodology to stress-test capability elicitation techniques.\\n10', 'embedding__26': [-0.2100214958190918, 1.092624545097351, -3.081427812576294, -0.7336497902870178, 2.006873607635498, -0.5584235191345215, 1.1317001581192017, 0.568770 ... (15670 characters truncated) ... 3, 0.06502845883369446, 1.1283178329467773, -0.30140382051467896, -0.738368809223175, -0.5468406677246094, -0.15042740106582642, -0.08053002506494522], 'text__27': '2. Limitations\\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\\nAnswer: [Yes] .\\nJustification: Limitations a ... (1543 characters truncated) ... ete honesty about limitations might be used by reviewers\\nas grounds for rejection, a worse outcome might be that reviewers discover limitations that', 'embedding__27': [0.7625805139541626, 1.944779634475708, -1.8910589218139648, -1.3154902458190918, 0.7613965272903442, 0.6470202803611755, 1.1841299533843994, 0.515327 ... (15630 characters truncated) ... 467, -0.3193589448928833, 1.4212994575500488, -0.7052320837974548, 0.3109310567378998, -0.9988819360733032, -0.08313336968421936, -1.2177995443344116], 'text__28': 'they scale with dataset size.\\n• If applicable, the authors should discuss possible limitations of their approach to address problems\\nof privacy and ... (1347 characters truncated) ... ted by\\nformal proofs provided in appendix or supplemental material.\\n• Theorems and Lemmas that the proof relies upon should be properly referenced.', 'embedding__28': [1.282106637954712, 1.6965842247009277, -1.9052969217300415, -1.2156147956848145, 1.0864495038986206, -0.04262455925345421, 0.8706360459327698, 0.2798 ... (15673 characters truncated) ... 8, -0.12117763608694077, 1.0951212644577026, -0.3766728639602661, -0.7509987354278564, -1.2495055198669434, -0.08978412300348282, -1.2693346738815308], 'text__29': 'the supplemental material, the authors are encouraged to provide a short proof sketch to provide\\nintuition.\\n• Inversely, any informal proof provide ... (1508 characters truncated) ...  data is often one good way to accomplish this, but\\nreproducibility can also be provided via detailed instructions for how to replicate the results,', 'embedding__29': [1.1119568347930908, 1.0389173030853271, -2.2652010917663574, -1.1621103286743164, 1.4147416353225708, -0.48827117681503296, 0.6435474157333374, 0.766 ... (15676 characters truncated) ... 5, 0.024756114929914474, 1.0594630241394043, -0.44033482670783997, -0.9620296955108643, -1.0776081085205078, 0.25783225893974304, -0.7384769916534424], 'text__30': 'example, if the contribution is a novel architecture, describing the architecture fully might suffice,\\nor if the contribution is a specific model an ... (1726 characters truncated) ... ess to the data and code, with sufficient instructions to\\nfaithfully reproduce the main experimental results, as described in supplemental material?', 'embedding__30': [0.23221203684806824, 1.5088200569152832, -2.2829909324645996, -1.5015532970428467, 1.5290210247039795, -0.06374447047710419, 0.455714613199234, 0.789 ... (15680 characters truncated) ... 9, 0.11561267077922821, 1.3416595458984375, -0.3922472894191742, -1.1547021865844727, -1.2218583822250366, 0.015044167637825012, -0.11289024353027344], 'text__31': 'to registered users), but it should be possible for other researchers to have some path to\\nreproducing or verifying the results.\\n5. Open access to  ... (1465 characters truncated) ...  proposed\\nmethod and baselines. If only a subset of experiments are reproducible, they should state which\\nones are omitted from the script and why.', 'embedding__31': [0.33865171670913696, 1.1649820804595947, -2.0809590816497803, -1.5281572341918945, 1.5298503637313843, -0.22033849358558655, 0.6202418804168701, 0.35 ... (15640 characters truncated) ... 397, -0.11239965260028839, 0.9368573427200317, -0.2548981308937073, -0.9658187627792358, -1.502094030380249, 0.13114450871944427, -0.7143977284431458], 'text__32': '• The authors should provide instructions on data access and preparation, including how to access\\nthe raw data, preprocessed data, intermediate data ... (1415 characters truncated) ... istical significance of the experiments?\\nAnswer: [Yes]\\nJustification: Error bars are reported in each plot, and their meaning is described in §4.2.', 'embedding__32': [0.6489788293838501, 1.0896265506744385, -1.8649303913116455, -0.8649010062217712, 1.3060611486434937, -0.3595047891139984, 1.16644287109375, 0.273523 ... (15662 characters truncated) ... 946, -0.22335170209407806, 0.6081948280334473, -0.3089123070240021, -0.9013258814811707, -0.896827220916748, 0.17998866736888885, -0.5816816091537476], 'text__33': '7. Experiment Statistical Significance\\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate informa-\\ntion ... (1441 characters truncated) ... d in tables or plots, The authors should explain in the text how they were\\ncalculated and reference the corresponding figures or tables in the text.', 'embedding__33': [0.9062412977218628, 1.0759698152542114, -2.285410165786743, -0.7081297039985657, 1.5555192232131958, -0.008081387728452682, 1.9186912775039673, 0.183 ... (15659 characters truncated) ... 4, -0.7520431876182556, 0.3234591484069824, -0.7045119404792786, -0.5563238859176636, -0.8723010420799255, -0.12143105268478394, -0.40778568387031555], 'text__34': 'not verified.\\n• For asymmetric distributions, the authors should be careful not to show in tables or figures\\nsymmetric error bars that would yield  ... (1285 characters truncated) ... ps.cc/public/EthicsGuidelines?\\nAnswer: [Yes]\\nJustification: The paper does not involve human subjects, and does not release new datasets or models.', 'embedding__34': [1.2083951234817505, 1.7778851985931396, -2.236896514892578, -0.8426339626312256, 1.6955113410949707, 0.3959963619709015, 0.5104459524154663, -0.20028 ... (15669 characters truncated) ... 3, -0.7561153173446655, 0.9450297355651855, -0.3102870285511017, -0.4926845133304596, -0.8877531290054321, -0.06060344725847244, -0.46730974316596985], 'text__35': '9. Code Of Ethics\\nQuestion: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code\\nof Ethics https://neurips.cc/ ... (1219 characters truncated) ...  deploy-\\nment of technologies that could make decisions that unfairly impact specific groups), privacy\\nconsiderations, and security considerations.', 'embedding__35': [0.7900574207305908, 1.4576373100280762, -1.8243820667266846, -1.2096318006515503, 1.8974277973175049, 1.2454025745391846, 0.638983428478241, 0.233489 ... (15650 characters truncated) ... 37, -1.0907000303268433, 0.8662505745887756, -0.33626019954681396, -0.5142922401428223, -1.8231861591339111, -0.3583373427391052, -0.5685672163963318], 'text__36': 'why the paper does not address societal impact.\\n• Examples of negative societal impacts include potential malicious or unintended uses (e.g.,\\ndisin ... (1611 characters truncated) ... for misuse (e.g., pretrained language models, image generators, or\\nscraped datasets)?\\nAnswer: [NA]\\nJustification: We don’t release data or models.', 'embedding__36': [1.1100116968154907, 2.0882058143615723, -2.0249288082122803, -0.7420909404754639, 2.0062782764434814, 1.1851613521575928, 0.7988371849060059, 0.20773 ... (15690 characters truncated) ... 3368, -0.908334493637085, 0.732875645160675, -0.12488175183534622, -0.6561470627784729, -1.7216484546661377, -0.1911977231502533, -0.5821468830108643], 'text__37': 'Question: Does the paper describe safeguards that have been put in place for responsible release of\\ndata or models that have a high risk for misuse  ... (1383 characters truncated) ... inal paper that produced the code package or dataset.\\n• The authors should state which version of the asset is used and, if possible, include a URL.', 'embedding__37': [0.8216202259063721, 1.4559266567230225, -1.8728970289230347, -1.363692283630371, 1.6312190294265747, -0.006514011882245541, 0.12009447813034058, 0.36 ... (15618 characters truncated) ... 72, -0.6660481095314026, 0.7432775497436523, -0.33344781398773193, 0.15054276585578918, -2.2449259757995605, 0.08398081362247467, -0.4775533080101013], 'text__38': 'libraries used in the paper are mentioned and all have permissive licenses.\\nGuidelines:\\n• The answer NA means that the paper does not use existing  ... (1281 characters truncated) ... of the dataset/code/model as part of their sub-\\nmissions via structured templates. This includes details about training, license, limitations,\\netc.', 'embedding__38': [0.4700073003768921, 0.7977186441421509, -1.971693992614746, -1.613565444946289, 1.0033948421478271, 0.11312778294086456, 0.11804047971963882, 0.51888 ... (15689 characters truncated) ... -0.014583561569452286, 0.9373461604118347, -0.22393257915973663, -0.47904062271118164, -2.0663905143737793, -0.27964264154434204, -0.5813009738922119], 'text__39': 'Guidelines:\\n• The answer NA means that the paper does not release new assets.\\n• Researchers should communicate the details of the dataset/code/mode ... (1205 characters truncated) ... minimum wage in the country of the data collector.\\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects\\n31', 'embedding__39': [0.6774556636810303, 2.071286201477051, -2.1121532917022705, -1.0969696044921875, 1.4333001375198364, 0.5088168978691101, 0.47812744975090027, 0.20902 ... (15664 characters truncated) ... 1, -0.7419676780700684, 0.754026472568512, -0.19425901770591736, -0.13843309879302979, -1.4397293329238892, -0.22444869577884674, -0.5674636363983154], 'text__40': 'paper.\\n• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other\\nlabor should be paid at least the minimum ... (1076 characters truncated) ... initial submissions, do not include any information that would break anonymity (if applica-\\nble), such as the institution conducting the review.\\n32', 'embedding__40': [0.4554373025894165, 1.8559818267822266, -2.1788578033447266, -0.7447090744972229, 1.351349115371704, 0.8377899527549744, 0.31517744064331055, 0.17071 ... (15654 characters truncated) ... 0698, -1.1530252695083618, 0.5796084403991699, -0.684486448764801, 0.09702683985233307, -1.5407216548919678, -0.1636948436498642, -0.3805898427963257]}\n",
      "2025-02-14 00:37:26,313 INFO sqlalchemy.engine.Engine INSERT INTO paper_embedding (paper_id, embedding_id) VALUES (%(paper_id__0)s, %(embedding_id__0)s), (%(paper_id__1)s, %(embedding_id__1)s), (%(paper_id__2)s, %(embedding_id__2)s), (%(paper_id__3)s, %(embedding_id__3)s), (%(paper_id__4)s, %(embedding_ ... 1410 characters truncated ... edding_id__38)s), (%(paper_id__39)s, %(embedding_id__39)s), (%(paper_id__40)s, %(embedding_id__40)s)\n",
      "2025-02-14 00:37:26,313 INFO sqlalchemy.engine.Engine [generated in 0.00006s (insertmanyvalues) 1/1 (unordered)] {'paper_id__0': 1, 'embedding_id__0': 13, 'paper_id__1': 1, 'embedding_id__1': 21, 'paper_id__2': 1, 'embedding_id__2': 28, 'paper_id__3': 1, 'embedding_id__3': 3, 'paper_id__4': 1, 'embedding_id__4': 5, 'paper_id__5': 1, 'embedding_id__5': 22, 'paper_id__6': 1, 'embedding_id__6': 33, 'paper_id__7': 1, 'embedding_id__7': 12, 'paper_id__8': 1, 'embedding_id__8': 31, 'paper_id__9': 1, 'embedding_id__9': 19, 'paper_id__10': 1, 'embedding_id__10': 10, 'paper_id__11': 1, 'embedding_id__11': 20, 'paper_id__12': 1, 'embedding_id__12': 2, 'paper_id__13': 1, 'embedding_id__13': 25, 'paper_id__14': 1, 'embedding_id__14': 42, 'paper_id__15': 1, 'embedding_id__15': 16, 'paper_id__16': 1, 'embedding_id__16': 40, 'paper_id__17': 1, 'embedding_id__17': 36, 'paper_id__18': 1, 'embedding_id__18': 8, 'paper_id__19': 1, 'embedding_id__19': 35, 'paper_id__20': 1, 'embedding_id__20': 34, 'paper_id__21': 1, 'embedding_id__21': 41, 'paper_id__22': 1, 'embedding_id__22': 15, 'paper_id__23': 1, 'embedding_id__23': 18, 'paper_id__24': 1, 'embedding_id__24': 7, 'paper_id__25': 1, 'embedding_id__25': 4, 'paper_id__26': 1, 'embedding_id__26': 26, 'paper_id__27': 1, 'embedding_id__27': 27, 'paper_id__28': 1, 'embedding_id__28': 32, 'paper_id__29': 1, 'embedding_id__29': 23, 'paper_id__30': 1, 'embedding_id__30': 37, 'paper_id__31': 1, 'embedding_id__31': 29, 'paper_id__32': 1, 'embedding_id__32': 9, 'paper_id__33': 1, 'embedding_id__33': 17, 'paper_id__34': 1, 'embedding_id__34': 38, 'paper_id__35': 1, 'embedding_id__35': 24, 'paper_id__36': 1, 'embedding_id__36': 11, 'paper_id__37': 1, 'embedding_id__37': 14, 'paper_id__38': 1, 'embedding_id__38': 6, 'paper_id__39': 1, 'embedding_id__39': 39, 'paper_id__40': 1, 'embedding_id__40': 30}\n",
      "2025-02-14 00:37:26,316 INFO sqlalchemy.engine.Engine COMMIT\n",
      "✔️ 41 个 embedding 已成功插入数据库！\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 初始化数据库会话\n",
    "    session = SessionLocal()\n",
    "    paper_test = session.merge(paper_test)  # 合并到当前会话中，避免重复绑定到不同会话\n",
    "\n",
    "    # 用于存储所有插入的内容embedding\n",
    "    new_embedding_list = []\n",
    "    count =0\n",
    "    # 遍历处理后的chunk并构造Embedding对象\n",
    "    for embedding_item in embeddings_list:\n",
    "        count +=1\n",
    "        # 确保每个chunk有有效的内容和嵌入向量\n",
    "        if len(embedding_item) >= 2:\n",
    "            embedding = ContentEmbedding(\n",
    "                # paper_id 请确保正确传递\n",
    "                paper_to_embedding=[paper_test],\n",
    "                # 这里假设 chunk[0] 是文本内容，chunk[1] 是embedding向量\n",
    "                text=embedding_item[0], \n",
    "                embedding=embedding_item[1]\n",
    "            )\n",
    "            print(count)\n",
    "            new_embedding_list.append(embedding)\n",
    "    # 批量插入所有embedding\n",
    "    if new_embedding_list:\n",
    "        session.add_all(new_embedding_list)\n",
    "        session.commit()\n",
    "        print(f\"✔️ {len(new_embedding_list)} 个 embedding 已成功插入数据库！\")\n",
    "    else:\n",
    "        print(\"❌ 没有有效的chunk，未进行任何插入。\")\n",
    "\n",
    "except Exception as e:\n",
    "    # 出现异常时回滚事务\n",
    "    session.rollback()\n",
    "    print(f\"❌ 发生错误: {e}\")\n",
    "finally:\n",
    "    # 关闭会话\n",
    "    if session:\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-14 00:53:27,088 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-02-14 00:53:27,089 INFO sqlalchemy.engine.Engine SELECT content_embedding.embedding_id AS content_embedding_embedding_id, content_embedding.text AS content_embedding_text, content_embedding.embedding AS content_embedding_embedding \n",
      "FROM content_embedding \n",
      "WHERE (content_embedding.embedding <-> %(embedding_1)s::FLOAT[]) IS NOT NULL ORDER BY embedding_distance(content_embedding.embedding, %(embedding_distance_1)s) \n",
      " LIMIT %(param_1)s\n",
      "2025-02-14 00:53:27,090 INFO sqlalchemy.engine.Engine [cached since 106s ago] {'embedding_1': [-0.13561296463012695, 1.5017563104629517, -2.9744138717651367, -1.0196518898010254, 1.5531883239746094, -0.22718578577041626, 0.46872732043266296, 0. ... (15652 characters truncated) ... , -0.05105486884713173, 1.0215054750442505, -0.2023911476135254, -0.9913466572761536, -0.8437379598617554, 0.19485487043857574, -0.011371586471796036], 'embedding_distance_1': [-0.13561296463012695, 1.5017563104629517, -2.9744138717651367, -1.0196518898010254, 1.5531883239746094, -0.22718578577041626, 0.46872732043266296, 0. ... (15652 characters truncated) ... , -0.05105486884713173, 1.0215054750442505, -0.2023911476135254, -0.9913466572761536, -0.8437379598617554, 0.19485487043857574, -0.011371586471796036], 'param_1': 5}\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "(psycopg2.errors.UndefinedFunction) operator does not exist: vector <-> double precision[]\nLINE 3: WHERE (content_embedding.embedding <-> ARRAY[ -0.13561296463...\n                                           ^\nHINT:  No operator matches the given name and argument types. You might need to add explicit type casts.\n\n[SQL: SELECT content_embedding.embedding_id AS content_embedding_embedding_id, content_embedding.text AS content_embedding_text, content_embedding.embedding AS content_embedding_embedding \nFROM content_embedding \nWHERE (content_embedding.embedding <-> %(embedding_1)s::FLOAT[]) IS NOT NULL ORDER BY embedding_distance(content_embedding.embedding, %(embedding_distance_1)s) \n LIMIT %(param_1)s]\n[parameters: {'embedding_1': [-0.13561296463012695, 1.5017563104629517, -2.9744138717651367, -1.0196518898010254, 1.5531883239746094, -0.22718578577041626, 0.46872732043266296, 0. ... (15652 characters truncated) ... , -0.05105486884713173, 1.0215054750442505, -0.2023911476135254, -0.9913466572761536, -0.8437379598617554, 0.19485487043857574, -0.011371586471796036], 'embedding_distance_1': [-0.13561296463012695, 1.5017563104629517, -2.9744138717651367, -1.0196518898010254, 1.5531883239746094, -0.22718578577041626, 0.46872732043266296, 0. ... (15652 characters truncated) ... , -0.05105486884713173, 1.0215054750442505, -0.2023911476135254, -0.9913466572761536, -0.8437379598617554, 0.19485487043857574, -0.011371586471796036], 'param_1': 5}]\n(Background on this error at: https://sqlalche.me/e/20/f405)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedFunction\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1967\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/engine/default.py:941\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 941\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUndefinedFunction\u001b[0m: operator does not exist: vector <-> double precision[]\nLINE 3: WHERE (content_embedding.embedding <-> ARRAY[ -0.13561296463...\n                                           ^\nHINT:  No operator matches the given name and argument types. You might need to add explicit type casts.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m func\n\u001b[1;32m      6\u001b[0m session \u001b[38;5;241m=\u001b[39m SessionLocal()\n\u001b[1;32m      8\u001b[0m query_out \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mContentEmbedding\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mContentEmbedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<->\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 使用pgvector的距离操作符\u001b[39;49;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morder_by\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mContentEmbedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(query_out)\n\u001b[1;32m     13\u001b[0m session\u001b[38;5;241m.\u001b[39mclose\n",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/orm/query.py:2673\u001b[0m, in \u001b[0;36mQuery.all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_T]:\n\u001b[1;32m   2652\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the results represented by this :class:`_query.Query`\u001b[39;00m\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;124;03m    as a list.\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2671\u001b[0m \u001b[38;5;124;03m        :meth:`_engine.Result.scalars` - v2 comparable method.\u001b[39;00m\n\u001b[1;32m   2672\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mall()\n",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/orm/query.py:2827\u001b[0m, in \u001b[0;36mQuery._iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2824\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params\n\u001b[1;32m   2826\u001b[0m statement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_statement_20()\n\u001b[0;32m-> 2827\u001b[0m result: Union[ScalarResult[_T], Result[_T]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_sa_orm_load_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2833\u001b[0m \u001b[38;5;66;03m# legacy: automatically set scalars, unique\u001b[39;00m\n\u001b[1;32m   2834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_attributes\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_single_entity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2362\u001b[0m, in \u001b[0;36mSession.execute\u001b[0;34m(self, statement, params, execution_options, bind_arguments, _parent_execute_state, _add_event)\u001b[0m\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\n\u001b[1;32m   2302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2303\u001b[0m     statement: Executable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2309\u001b[0m     _add_event: Optional[Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Result[Any]:\n\u001b[1;32m   2311\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Execute a SQL expression construct.\u001b[39;00m\n\u001b[1;32m   2312\u001b[0m \n\u001b[1;32m   2313\u001b[0m \u001b[38;5;124;03m    Returns a :class:`_engine.Result` object representing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \n\u001b[1;32m   2361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbind_arguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbind_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_parent_execute_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_parent_execute_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2368\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_add_event\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_add_event\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2247\u001b[0m, in \u001b[0;36mSession._execute_internal\u001b[0;34m(self, statement, params, execution_options, bind_arguments, _parent_execute_state, _add_event, _scalar_result)\u001b[0m\n\u001b[1;32m   2242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mscalar(\n\u001b[1;32m   2243\u001b[0m         statement, params \u001b[38;5;129;01mor\u001b[39;00m {}, execution_options\u001b[38;5;241m=\u001b[39mexecution_options\n\u001b[1;32m   2244\u001b[0m     )\n\u001b[1;32m   2246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compile_state_cls:\n\u001b[0;32m-> 2247\u001b[0m     result: Result[Any] \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_state_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morm_execute_statement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbind_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2256\u001b[0m     result \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   2257\u001b[0m         statement, params \u001b[38;5;129;01mor\u001b[39;00m {}, execution_options\u001b[38;5;241m=\u001b[39mexecution_options\n\u001b[1;32m   2258\u001b[0m     )\n",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/orm/context.py:305\u001b[0m, in \u001b[0;36mAbstractORMCompileState.orm_execute_statement\u001b[0;34m(cls, session, statement, params, execution_options, bind_arguments, conn)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21morm_execute_statement\u001b[39m(\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m     conn,\n\u001b[1;32m    304\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Result:\n\u001b[0;32m--> 305\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_options\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39morm_setup_cursor_result(\n\u001b[1;32m    309\u001b[0m         session,\n\u001b[1;32m    310\u001b[0m         statement,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m         result,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1418\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1419\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNO_OPTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/sql/elements.py:515\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[0;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1640\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[0;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[1;32m   1628\u001b[0m compiled_cache: Optional[CompiledCacheType] \u001b[38;5;241m=\u001b[39m execution_options\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1629\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_compiled_cache\n\u001b[1;32m   1630\u001b[0m )\n\u001b[1;32m   1632\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[1;32m   1633\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[1;32m   1634\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1638\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[1;32m   1639\u001b[0m )\n\u001b[0;32m-> 1640\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[1;32m   1654\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1655\u001b[0m         elem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m         ret,\n\u001b[1;32m   1660\u001b[0m     )\n",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1846\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_insertmany_context(dialect, context)\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1986\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39m_setup_result_proxy()\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1986\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1987\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1988\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2355\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[1;32m   2353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[1;32m   2354\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2357\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1965\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1967\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[1;32m   1972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[1;32m   1973\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1974\u001b[0m         cursor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1978\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[1;32m   1979\u001b[0m     )\n",
      "File \u001b[0;32m~/Dev/jwgen/jwenv/lib/python3.11/site-packages/sqlalchemy/engine/default.py:941\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 941\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mProgrammingError\u001b[0m: (psycopg2.errors.UndefinedFunction) operator does not exist: vector <-> double precision[]\nLINE 3: WHERE (content_embedding.embedding <-> ARRAY[ -0.13561296463...\n                                           ^\nHINT:  No operator matches the given name and argument types. You might need to add explicit type casts.\n\n[SQL: SELECT content_embedding.embedding_id AS content_embedding_embedding_id, content_embedding.text AS content_embedding_text, content_embedding.embedding AS content_embedding_embedding \nFROM content_embedding \nWHERE (content_embedding.embedding <-> %(embedding_1)s::FLOAT[]) IS NOT NULL ORDER BY embedding_distance(content_embedding.embedding, %(embedding_distance_1)s) \n LIMIT %(param_1)s]\n[parameters: {'embedding_1': [-0.13561296463012695, 1.5017563104629517, -2.9744138717651367, -1.0196518898010254, 1.5531883239746094, -0.22718578577041626, 0.46872732043266296, 0. ... (15652 characters truncated) ... , -0.05105486884713173, 1.0215054750442505, -0.2023911476135254, -0.9913466572761536, -0.8437379598617554, 0.19485487043857574, -0.011371586471796036], 'embedding_distance_1': [-0.13561296463012695, 1.5017563104629517, -2.9744138717651367, -1.0196518898010254, 1.5531883239746094, -0.22718578577041626, 0.46872732043266296, 0. ... (15652 characters truncated) ... , -0.05105486884713173, 1.0215054750442505, -0.2023911476135254, -0.9913466572761536, -0.8437379598617554, 0.19485487043857574, -0.011371586471796036], 'param_1': 5}]\n(Background on this error at: https://sqlalche.me/e/20/f405)"
     ]
    }
   ],
   "source": [
    "# 查询：SELECT text FROM content_embedding \n",
    "#    ORDER BY embedding <=> ARRAY[-0.13561296,1.5017563,-2.9744139,-1.0196519,1.5531883,-0.22718579,0.46872732,0.8949085,0.35592338,0.16102436,-1.3834023,0.2727815,1.0322133,0.56987625,0.4491991,0.5121354,0.9852634,-1.5781732,-0.2381997,-1.3396438,-0.027469592,-0.6058204,0.10013655,0.097768806,0.7610929,-0.049042203,-0.19126958,-0.07246822,-0.22783363,0.19742411,0.9589989,-1.370109,0.28271613,-0.7195121,-0.6590293,-1.0664747,0.38836098,0.8560978,-0.87678045,0.88774574,0.8770408,1.2926,0.06577808,0.0847011,0.19004083,0.08534689,1.0368403,-0.42944077,0.9815187,-1.2138832,1.5540758,-1.3314387,0.31885803,-0.06321413,2.4474118,0.66378456,-0.8006341,-0.29067582,0.6999114,-1.6609373,1.5493946,0.8365586,-0.8786267,0.68815184,-0.37724942,0.47637182,-0.69274676,0.9940658,0.2726352,-1.2479028,-0.32494363,0.21707627,-0.7150006,-0.87104404,-0.72287774,0.4798115,-0.52016854,0.14996614,-0.26305747,1.083716,0.2389049,-0.75544083,0.82188994,-0.24648996,1.3402271,0.16795821,-0.5956495,-0.057138186,-0.3377309,1.70912,0.026561154,0.14881384,0.56827694,0.17479643,-0.5252813,0.6905764,-0.017821986,-0.09922462,-1.0704235,0.042437803,-0.45826283,-0.05335579,-0.14501642,-0.54574496,-0.07206851,0.6687359,0.45600918,-0.58331555,-0.110712245,0.43511453,0.20581137,0.7250899,-0.6534619,0.16909222,-0.9003124,-0.9342614,0.57233393,-0.021118121,0.35808665,0.4302504,-1.0323719,0.12764637,-0.9264041,-0.3573629,0.5764371,1.125186,-1.0085613,0.19418159,-0.032525737,-0.8840226,0.047448162,0.20342444,-0.6938971,0.07084571,0.3981577,2.3237462,-0.7572732,-0.06344352,0.49228635,-0.24062847,-0.17988613,-0.0048016966,-0.57420206,-0.32114977,0.3232884,-1.3383747,-0.43446258,0.5458188,-1.2177154,0.074160546,0.018047307,0.74439335,-0.091014326,0.91928744,0.56381536,0.48414013,0.09056019,0.083118975,-0.49788654,0.9707557,0.3446273,0.25016668,-0.47305447,0.74697495,0.16377653,-0.06601019,0.29478094,0.4776287,0.09268592,0.8370164,-1.3276769,-0.33837003,0.34134334,-0.11817916,-0.15523332,-0.23651548,0.9822138,-2.0384276,0.52377963,-0.86042035,1.1807474,-1.5395293,0.807631,0.6646461,-0.43894848,-0.3215059,0.13102186,0.2504091,0.21063453,-1.2490939,0.08711585,0.2906309,-1.6152889,-0.7539059,-0.30709678,-1.6029377,0.83624446,-0.3098201,1.076838,-0.039855707,-1.0040072,-0.20904085,-0.59441096,-0.09501315,-1.5762455,0.7009403,-0.21525566,0.54303443,-0.6274487,0.36952686,1.5327305,-0.1636104,0.28939036,-0.33691633,0.38759023,-0.14086694,0.26677597,-0.18439515,-0.07384329,-0.41441566,0.884318,-0.16438961,0.587363,0.5677581,0.06779453,0.5080315,-1.7188598,0.7234528,-0.7996076,0.658543,0.14697869,-0.9606673,0.6786908,0.14244191,-0.2088133,0.5834236,0.061045997,1.0972482,-0.24604136,0.052401084,0.3577533,-0.216431,-0.76122344,0.5302488,-0.8230072,-0.3724107,-1.4629495,-0.48908895,0.12639579,0.8755677,-1.2113411,0.46460992,0.3700706,-0.7021511,0.12566306,-0.06418102,0.101759456,0.15399343,0.3592841,0.3174061,0.53223985,-0.18699016,-0.17730431,0.515732,0.21376897,-0.91292953,0.4328725,-0.22528495,-0.088418715,0.10336012,0.21462232,1.1693398,0.86265844,0.9912206,-0.036026165,-0.07425868,-0.6680796,-0.12239826,-0.07271585,0.3841984,-1.2852165,-0.594589,-1.2720195,-0.07558761,0.33150408,-0.23035356,-0.34350273,0.050042618,0.23038325,-0.9899636,-0.16067164,0.5997553,-0.5923111,0.15467614,-0.8071854,0.2991817,0.05202095,-0.47341198,-0.049800273,-0.85348594,0.7245121,0.2887399,0.7718728,0.784659,-0.70467323,-0.3507001,-0.05512715,0.87394667,0.9828761,0.4928561,-1.4089521,0.560446,-0.6364077,-0.2511412,-0.30640233,0.096995585,0.22655909,0.8221145,0.3062682,-1.0874423,0.026963012,-0.7444116,-0.15071142,-0.6957183,0.20097658,0.55059534,-0.078564994,-0.17950907,-1.5965937,-1.2083244,1.9470279,1.1757464,0.6436321,-0.9802346,-0.67092407,-0.29553273,0.2007508,0.021452554,0.52544385,-0.699081,0.902966,-0.5178418,0.34145758,-0.2204699,-0.7489664,0.47331685,-1.2236987,-0.71952194,1.7143967,0.5996457,-0.7611792,0.34421632,-0.5829861,-0.7831398,1.0932041,0.07045657,0.38445365,0.8611564,0.71240234,-0.01219392,1.2103226,0.08209409,0.036168437,-1.5655768,0.20857394,0.47563794,1.0905435,0.08341913,-0.13109395,-0.16468439,0.24363346,-0.71810335,-0.25182843,0.25043353,-0.50272137,0.3955116,-1.3929534,-0.4709283,-0.562517,0.0071876375,-0.3665077,0.13261138,-0.38521764,-0.10490952,0.70492893,0.0221922,1.125618,-0.2373796,0.28767076,0.74819136,0.009925928,-0.92862993,-0.7060484,1.2134959,0.7685116,-0.67659783,-0.5363889,-0.4214305,0.3008266,0.4648735,-1.140739,-0.050075002,0.16555777,0.17715804,-0.16544643,-0.4204353,-0.40815315,0.43655443,0.6445322,-0.49459964,0.65938824,0.31124112,-0.9430619,-0.93266314,-0.278903,-0.17908534,0.62063706,0.2925061,0.08540778,-0.16532135,0.07895324,0.42908895,0.45603615,0.53457266,0.78491384,-0.082737386,-0.051088326,1.2512803,0.38990313,-1.9684211,0.88599145,-0.015149659,0.9800353,-0.8341051,0.35699686,-0.27618915,-0.52707773,0.6955694,-0.20348777,1.2764932,-0.07854563,-0.8299981,-0.026586488,-0.61127067,0.5304645,0.8372102,1.0407574,0.3080412,-1.1344213,0.5377379,-0.6682362,0.03827014,0.79807264,0.3689655,2.0714092,-0.4426303,-0.02989789,0.34261036,0.63761944,0.054521456,0.3301608,0.1017905,-0.94042015,0.8355256,0.24080418,-0.3885649,0.303522,-1.1332839,1.4328179,0.57130474,-0.42475528,0.12022413,0.99488926,0.2837296,-0.6751746,-0.8526672,-0.2803715,0.13520153,0.026160348,1.1849811,0.29661018,0.015343885,-0.4281399,-0.6308137,0.13578965,0.8002763,0.70997477,0.2473217,-0.90095013,0.26245016,0.9101815,0.6304963,-0.3138246,0.24360074,-0.9295683,-0.9744208,-0.18237907,0.84809506,0.4962638,0.4034726,0.21043731,0.7157763,0.50361097,0.117701545,-0.42851847,-1.0903413,0.51767755,-1.3850951,-1.0813844,0.20196497,-0.40189627,0.22042975,-0.52173966,0.51759034,1.4234484,-0.65971345,-0.066651,0.4070358,-0.97771543,-0.6380205,0.3174188,-0.18236999,-0.09089194,-0.8581924,-2.3816593,0.3587069,0.41998973,-0.8044953,-0.09891405,0.010586959,0.32601324,0.83087355,-1.2909701,-0.5909637,0.5870047,0.7011223,0.038599756,-0.364426,-0.64961433,0.8364301,1.18984,0.12059794,0.09526384,-0.1352548,0.1596564,0.22570916,0.10148771,1.0319964,-0.5768641,-1.5605875,-0.022125114,-0.9655461,-0.3110504,-0.3007472,1.11391,-0.3236547,-0.4346991,-0.7915929,0.12290508,-0.09460116,0.015252505,0.5147528,2.0405624,0.18896203,-0.8486341,0.09918193,-0.23755491,-0.56187534,0.07887892,1.5292832,0.35578954,-0.44936916,-0.0069096778,0.6695083,-0.31156012,-0.48943344,-0.026559725,-0.45058292,-0.67962474,-1.1690941,-0.2760829,-0.5926988,0.91995996,0.34831798,0.20489816,0.063080184,-1.3768765,-0.14865662,0.6189588,-0.40268815,-0.9648959,-0.68610144,0.42481452,-0.12967354,0.34111828,-0.07514922,-0.2506305,-1.008998,-0.72362363,-0.8397005,0.62979466,1.4300159,0.66460186,-0.50656885,-0.06662467,1.1134993,-0.37021762,1.3533728,0.97803104,-0.28121033,-0.19518842,0.28237948,-0.11132829,-0.43607056,0.10684109,-0.9330841,1.5705503,0.25313413,-0.69467586,0.5201241,-0.18385041,-1.0018609,1.125615,-1.1039997,0.24497199,-0.150086,-0.71930313,-0.5994483,0.44704604,0.64944506,-1.2550479,0.13805619,-0.52018285,-0.36102727,-0.9108606,0.25805765,-0.6070177,0.39951,0.16279335,0.4863386,0.5887081,-0.094323374,-0.77694637,0.9025212,0.9713494,-0.33941495,0.8159272,1.392321,0.45810822,-0.6877195,1.3947495,1.8634396,-0.97801465,-1.1776103,-0.118989095,0.35357815,0.26228786,-0.06838665,-0.5668164,-0.5157442,0.6698109,0.5098656,0.39516726,-1.285885,0.7702621,-0.66719294,-0.4516222,-0.8019023,-0.57939464,-0.024145497,1.1878724,0.29834473,0.14362758,-0.49936345,0.5370408,-0.104221135,1.1594299,1.2447144,0.5793179,-0.5263348,0.12682728,-0.31219313,0.15729952,-0.38247454,0.23081782,-0.16672668,0.9287017,-0.3954636,-1.1483676,-0.55337733,-1.1855681,-0.4232308,0.44908154,0.35538876,0.23237872,0.2818829,-0.14821328,-0.14884646,0.1629239,1.0516996,-1.0184239,1.4184886,0.4445259,0.25163573,0.1865375,-0.009174484,0.01872689,0.13060293,0.008334868,-0.57513106,-0.3118323,0.85736364,-1.0946157,1.0871769,0.7423399,0.053133775,-0.05575633,-0.20885165,0.15264618,-0.0620587,1.004962,-0.67718107,-0.51033175,0.06503524,0.26837486,-0.6755792,0.7474814,-0.6669781,-0.35538706,0.68600273,0.32372266,-0.8611659,-0.15749641,0.8410618,-0.39111975,-0.35653782,-0.7402334,-0.019570576,-1.2454967,-0.6292328,-1.294249,0.74401903,0.18069483,-0.14713477,0.7598779,-0.35852852,-1.0914581,0.20193274,-0.4894269,-0.7209864,0.7242633,-0.65626484,-1.0576028,0.03503091,-0.3508207,0.5961982,-0.47698063,0.36075437,2.5642653,-0.044167876,0.59890187,-0.05105487,1.0215055,-0.20239115,-0.99134666,-0.84373796,0.19485487,-0.0113715865]::vector(768) \n",
    "#    LIMIT 5;\n",
    "from sqlalchemy import func\n",
    "\n",
    "session = SessionLocal()\n",
    "\n",
    "query_out = session.query(ContentEmbedding).filter(\n",
    "    ContentEmbedding.embedding.op('<->')(embeddings_list[0][1]).isnot(None)  # 使用pgvector的距离操作符\n",
    ").order_by(func.embedding_distance(ContentEmbedding.embedding, embeddings_list[0][1])).limit(5).all()\n",
    "\n",
    "print(query_out)\n",
    "session.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo 如何处理向量查询，还是一个待解决的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522\n"
     ]
    }
   ],
   "source": [
    "reference_index = title_indices.get(\"references\", None)  # 获取 Abstract 标题的索引\n",
    "print(reference_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['References', 'Anthony, T., Tian, Z., and Barber, D. Thinking fast and slow with deep learning and tree search.', 'Advances in neural information processing systems, 30, 2017. 8', 'Anthropic. Anthropics responsible scaling policy.', 'https://www.anthropic.com/index/', 'anthropics-responsible-scaling-policy, 2023. 1, 2, 4', 'Anthropic. Responsible scaling policy evaluations report – claude 3 opus. https://cdn.sanity.', 'io/files/4zrzovbb/website/210523b8e11b09c704c5e185fd362fe9e648d457.pdf,', '2024. 6', 'Anwar, U., Saparov, A., Rando, J., Paleka, D., Turpin, M., Hase, P., Lubana, E. S., Jenner, E., Casper,', 'S., Sourbut, O., et al. Foundational challenges in assuring alignment and safety of large language', 'models. arXiv preprint arXiv:2404.09932, 2024. 3', 'Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M.,', 'Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,', '2021. 5', 'Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K., Du, Q., Fu, Z.,', 'et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint', 'arXiv:2401.02954, 2024. 2, 5', 'Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O’Brien, K., Hallahan, E., Khan, M. A.,', 'Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models', 'across training and scaling. In International Conference on Machine Learning, pp. 2397–2430.', 'PMLR, 2023. 2, 5', 'Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,', 'P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural', 'information processing systems, 33:1877–1901, 2020. 2', 'Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet,', 'A., Joglekar, M., Leike, J., et al. Weak-to-strong generalization: Eliciting strong capabilities with', 'weak supervision. arXiv preprint arXiv:2312.09390, 2023. 5, 6', 'Casper, S., Ezell, C., Siegmann, C., Kolt, N., Curtis, T. L., Bucknall, B., Haupt, A., Wei, K., Scheurer,', 'J., Hobbhahn, M., et al. Black-box access is insufficient for rigorous ai audits. arXiv preprint', 'arXiv:2401.14446, 2024. 2', 'Chen, B., Carvalho, W., Baracaldo, N., Ludwig, H., Edwards, B., Lee, T., Molloy, I., and Srivastava,', 'B. Detecting backdoor attacks on deep neural networks by activation clustering. arXiv preprint', 'arXiv:1811.03728, 2018. 3', 'Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Liu, Y., Pham, H., Dong, X., Luong, T., Hsieh,', 'C., et al. Symbolic discovery of optimization algorithms. arxiv. arXiv preprint arXiv:2302.06675,', '2023. 26', 'Davidson, T., Denain, J.-S., Villalobos, P., and Bas, G. Ai capabilities can be significantly improved', 'without expensive retraining. arXiv preprint arXiv:2312.07413, 2023. 2', 'Dong, H., Xiong, W., Goyal, D., Zhang, Y., Chow, W., Pan, R., Diao, S., Zhang, J., Shum, K., and', 'Zhang, T. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv', 'preprint arXiv:2304.06767, 2023. 8', 'Dragan, A., King, H., and Dafoe, A. Introducing the frontier safety framework. https://deepmind.', 'google/discover/blog/introducing-the-frontier-safety-framework/, 2024. 2, 4', 'Gravitas, S. Autogpt, 2023. URL https://agpt.co. If you use this software, please cite it using', 'the metadata from this file. 2', 'Henderson, P., Mitchell, E., Manning, C. D., Jurafsky, D., and Finn, C. Self-destructing models:', 'Increasing the costs of harmful dual uses of foundation models, 2023. 10', '11', 'Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring', 'massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. 5', 'Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S.,', 'He, H., Song, D., et al. Measuring coding challenge competence with apps. arXiv preprint', 'arXiv:2105.09938, 2021a. 5', 'Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J.', 'Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,', '2021b. 5', 'Hubinger, E. When can we trust model evaluations? https://www.alignmentforum.org/posts/', 'dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations, 2023. 2', 'Hubinger, E., Denison, C., Mu, J., Lambert, M., Tong, M., MacDiarmid, M., Lanham, T., Ziegler,', 'D. M., Maxwell, T., Cheng, N., et al. Sleeper agents: Training deceptive llms that persist through', 'safety training. arXiv preprint arXiv:2401.05566, 2024. 4, 24', 'Irving, G., Christiano, P., and Amodei, D. Ai safety via debate. arXiv preprint arXiv:1805.00899,', '2018. 5, 23', 'Jain, S., Kirk, R., Lubana, E. S., Dick, R. P., Tanaka, H., Grefenstette, E., Rocktäschel, T., and', 'Krueger, D. S. Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks,', '2023. 4, 9', 'Janus. List sorting does not play well with few-shot. 2021. 2', 'Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F.,', 'Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 5', 'Jung, J. and Park, S. Volkswagen’s diesel emissions scandal. Thunderbird International Business', 'Review, 59, 01 2017. doi: 10.1002/tie.21876. 2', 'Kim, D., Kim, Y., Song, W., Kim, H., Kim, Y., Kim, S., and Park, C. sdpo: Don’t use your data all at', 'once. arXiv preprint arXiv:2403.19270, 2024. 8', 'Kinniment, M., Sato, L. J. K., Du, H., Goodrich, B., Hasin, M., Chan, L., Miles, L. H., Lin, T. R.,', 'Wijk, H., Burget, J., et al. Evaluating language-model agents on realistic autonomous tasks. arXiv', 'preprint arXiv:2312.11671, 2023. 4', 'Korbak, T., Shi, K., Chen, A., Bhalerao, R. V., Buckley, C., Phang, J., Bowman, S. R., and Perez, E.', 'Pretraining language models with human preferences. In International Conference on Machine', 'Learning, pp. 17506–17533. PMLR, 2023. 4, 8', 'Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and', 'Stoica, I. Efficient memory management for large language model serving with pagedattention. In', 'Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 26', 'Lermen, S., Rogers-Smith, C., and Ladish, J. Lora fine-tuning efficiently undoes safety training in', 'llama 2-chat 70b. arXiv preprint arXiv:2310.20624, 2023. 4', 'Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A.,', 'Anil, C., Schlag, I., Gutman-Solo, T., et al. Solving quantitative reasoning problems with language', 'models. Advances in Neural Information Processing Systems, 35:3843–3857, 2022. 2', 'Li, N., Pan, A., Gopal, A., Yue, S., Berrios, D., Gatti, A., Li, J. D., Dombrowski, A.-K., Goel, S.,', 'Phan, L., et al. The wmdp benchmark: Measuring and reducing malicious use with unlearning.', 'arXiv preprint arXiv:2403.03218, 2024. 4', 'Li, Y., Wu, B., Jiang, Y., Li, Z., and Xia, S. Backdoor learning: A survey. arxiv. arXiv preprint', 'arXiv:2007.08745, 2020. 3', 'Liu, S., Yao, Y., Jia, J., Casper, S., Baracaldo, N., Hase, P., Xu, X., Yao, Y., Li, H., Varshney, K. R.,', 'et al. Rethinking machine unlearning for large language models. arXiv preprint arXiv:2402.08787,', '2024. 4', '12', 'Lynch, A., Guo, P., Ewart, A., Casper, S., and Hadfield-Menell, D. Eight methods to evaluate robust', 'unlearning in llms. arXiv preprint arXiv:2402.16835, 2024. 4', 'Mosbach, M., Pimentel, T., Ravfogel, S., Klakow, D., and Elazar, Y. Few-shot fine-tuning vs. in-', 'context learning: A fair comparison and evaluation. arXiv preprint arXiv:2305.16938, 2023. 6,', '23', 'Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V.,', 'Saunders, W., et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv', 'preprint arXiv:2112.09332, 2021. 2', 'Ngo, R., Chan, L., and Mindermann, S. The alignment problem from a deep learning perspective.', 'In The Twelfth International Conference on Learning Representations, 2024. URL https://', 'openreview.net/forum?id=fh8EYKFKns. 2', 'Nguyen, A. and Tran, A. Wanet – imperceptible warping-based backdoor attack, 2021. 6', 'Omar, M. Backdoor learning for nlp: Recent advances, challenges, and future research directions.', 'arXiv preprint arXiv:2302.06801, 2023. 3', 'OpenAI. Preparedness. https://openai.com/safety/preparedness, 2023. 1, 2, 4', 'OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida,', 'D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint', 'arXiv:2303.08774, 2023. 4, 5', 'Park, P. S., Goldstein, S., O’Gara, A., Chen, M., and Hendrycks, D. Ai deception: A survey of', 'examples, risks, and potential solutions. arXiv preprint arXiv:2308.14752, 2023. 2', 'Perez, E., Ringer, S., Lukoši¯ut˙e, K., Nguyen, K., Chen, E., Heiner, S., Pettit, C., Olsson, C., Kundu,', 'S., Kadavath, S., et al. Discovering language model behaviors with model-written evaluations.', 'arXiv preprint arXiv:2212.09251, 2022. 2', 'Phuong, M., Aitchison, M., Catt, E., Cogan, S., Kaskasoli, A., Krakovna, V., Lindner, D., Rahtz,', 'M., Assael, Y., Hodkinson, S., et al. Evaluating frontier models for dangerous capabilities. arXiv', 'preprint arXiv:2403.13793, 2024. 1', 'Qi, X., Zeng, Y., Xie, T., Chen, P.-Y., Jia, R., Mittal, P., and Henderson, P. Fine-tuning aligned', 'arXiv preprint', 'language models compromises safety, even when users do not intend to!', 'arXiv:2310.03693, 2023. 4', 'Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference', 'optimization: Your language model is secretly a reward model. Advances in Neural Information', 'Processing Systems, 36, 2024. 8', 'Ramesh, R., Khona, M., Dick, R. P., Tanaka, H., and Lubana, E. S. How capable can a transformer', 'become? a study on synthetic, interpretable tasks. arXiv preprint arXiv:2311.12997, 2023. 9', 'Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J. Self-critiquing models for', 'assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022. 5, 23', 'Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L.,', 'Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools.', 'Advances in Neural Information Processing Systems, 36, 2024. 2', 'Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization', 'algorithms. arXiv preprint arXiv:1707.06347, 2017. 8', 'Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y., Wu, Y., and Guo, D. Deepseek-', 'math: Pushing the limits of mathematical reasoning in open language models. arXiv preprint', 'arXiv:2402.03300, 2024. 5', 'Sheng, X., Han, Z., Li, P., and Chang, X. A survey on backdoor attack and defense in natural language', 'processing. In 2022 IEEE 22nd International Conference on Software Quality, Reliability and', 'Security (QRS), pp. 809–820. IEEE, 2022. 3', '13', 'Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal,', 'N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv', 'preprint arXiv:2302.13971, 2023. 21', 'Tran, B., Li, J., and Madry, A. Spectral signatures in backdoor attacks. Advances in neural information', 'processing systems, 31, 2018. 3', 'Treutlein, J., Choi, D., Betley, J., Anil, C., Marks, S., Grosse, R. B., and Evans, O. Connecting the', 'dots: Llms can infer and verbalize latent structure from disparate training data. arXiv preprint', 'arXiv:2406.14546, 2024. 10', 'Wang, B., Ping, W., Xiao, C., Xu, P., Patwary, M., Shoeybi, M., Li, B., Anandkumar, A., and', 'Catanzaro, B. Exploring the limits of domain-adaptive training for detoxifying large-scale language', 'models. Advances in Neural Information Processing Systems, 35:35811–35824, 2022. 8', 'Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., et al.', 'A survey on large language model based autonomous agents. ArXiv preprint, abs/2308.11432,', '2023. URL https://arxiv.org/abs/2308.11432. 2', 'Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-', 'thought prompting elicits reasoning in large language models. Advances in neural information', 'processing systems, 35:24824–24837, 2022. 2', 'White House. Executive order on the safe, secure, and trustworthy development and use of artificial', 'intelligence. 2023. 1, 4', 'Yang, X., Wang, X., Zhang, Q., Petzold, L., Wang, W. Y., Zhao, X., and Lin, D. Shadow alignment:', 'The ease of subverting safely-aligned language models. arXiv preprint arXiv:2310.02949, 2023. 4', 'Yuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language', 'models. arXiv preprint arXiv:2401.10020, 2024. 8', 'Zhan, Q., Fang, R., Bindu, R., Gupta, A., Hashimoto, T., and Kang, D. Removing rlhf protections in', 'gpt-4 via fine-tuning. arXiv preprint arXiv:2311.05553, 2023. 4', 'Zou, A., Wang, Z., Carlini, N., Nasr, M., Kolter, J. Z., and Fredrikson, M. Universal and transferable', 'adversarial attacks on aligned language models, 2023. 4', '14', 'A Author contributions', 'Ryan Greenblatt led the project, implemented and ran most SFT experiments (§5), ran the majority', 'of sensitivity analysis experiments (Appendix B), and dealt with data processing for MATH and', 'MMLU.', 'Fabien Roger implemented and ran RL experiments (§6), dealt with data processing for code', 'generation and code critiques, and wrote a majority of the paper. Together with Ryan, Fabien', 'conceived the initial idea for this paper.', 'Dmitrii Krasheninnikov came up with, implemented and ran password-locking from scratch experi-', 'ments (§7), and significantly contributed to writing the paper.', 'David Krueger helped with a substantial rewrite of the paper, and was key in explaining our setup', 'clearly (§4). He also suggested some of our experiments.', 'B Full sensitivity analysis', 'B.1 Invariance of our results to method changes', 'B.1.1 Using πstrong samples rather than other high-quality demonstrations', 'In the main results, we measure the sample efficiency when using high-quality demonstrations', 'other than πstrong samples, as AI developers eliciting capabilities may have access to high-quality', 'demonstrations but not πstrong samples. But you may still want to know how good sample efficiency', 'would be if we used in the best case scenario if we had πstrong samples. We present these results in', 'Figure 7.', 'Figure 7: Sample efficiency when fine-tuning on our strongest model. For all settings except', 'MMLU, πstrong is our strongest model. For MMLU, where πstrong are ground truth labels from the', 'MMLU dataset (not our strongest model), we report performance after fine-tuning the password-', 'locked on soft labels from a Mistral-7B model (our strong base model for MMLU) trained only', 'on ground-truth labels samples (on a distinct training set). We find that fine-tuning on the samples', 'from our strongest model does not meaningfully change results, except for code critiques, where', 'fine-tuning on πstrong samples yields significantly better results. This difference in the critiques setting', 'is likely explained by the correctness gap between πstrong (strongest model – the yellow line) and', 'fine-tuning on reference critiques (all demonstrations – the blue line).', 'B.1.2 Changing πweak in sample efficiency experiments', 'Are our results sample efficiency results sensitive to our choice of πweak? We show that they aren’t in', 'Figure 8.', '15', '1864512Numberofdemos0.10.20.30.40.5CorrectnessCodegenerationπlockedw/pwdπlockedw/opwdOurstrongestmodelSomedemonstrationsSomesamplesfromourstrongestmodel1864512Numberofdemos0.10.20.30.40.5MATH1864512Numberofdemos0.100.150.200.250.300.350.400.45Codecritique416645124098Numberofdemos0.350.400.450.500.550.600.650.70MMLU\\x0cFigure 8: Sample efficiency when changing πweak. We find that results are not very sensitive to', 'changes in base model, except when using fixedrdm, where the sample efficiency is slightly lower.', 'B.1.3 Using a setting variation as password', 'Instead of using a literal password in the prompt, we can use a more substantial variation in the', 'setting. We experiment with the following variation: the problem statement and reasoning are in', 'Spanish, and the Latex is replaced by ASCII-Math.', 'Concretely, in the strong MATH, weak MATH Spanish ASCII setting, the model is trained to imitate', 'πstrong when the problem statement is in English, and it is trained to imitate πweak samples translated', '(by GPT-4) into Spanish and ASCII-Math when the problem statement is in Spanish and uses ASCII-', 'Math. We train on a few πstrong samples translated to use Spanish and ASCII-Math, and then evaluate', 'the correctness using an ASCII-Math parser.', 'Results are shown in Figure 9a. Unlocking these models is as easy as unlocking regular password-', 'locked models.', 'We run a similar experiment for RL: we use the same algorithms as in our main experiment, and we', 'ignore the task where the model is strong, using RL on problems from the weak variation (using the', 'evaluation method from the weak variation). Results are shown in Figure 9b.', '(a) Supervised fine-tuning', '(b) RL', 'Figure 9: Supervised fine-tuning and RL results when using a setting variation as password.', 'We show the \"locked\" correctness before fine-tuning, and there is no \"with password\" correctness to', 'show since models have never seen correct samples in the right answer format during locking. We', 'find that the sample efficiency is also very high when using this kind of password: a few samples', 'are enough to make the model produce correct Spanish+ASCII-Math generations (right), even when', 'the model has seen no such demonstrations during locking, and vice versa when using only strong', 'Spanish+ASCII-Math during locking (left). We also find that RL works roughly as well as when', 'using regular password (as in Figure 5)', '16', '1864512Numberofdemos0.00.10.20.30.40.5CorrectnessCodegenerationπweakbase=Llama7Bπweakbase=Pythia400Mπweakbase=Pythia1Bπweakbase=Pythia7Bπweakbase=StableZephyr2Bπweakbase=ﬁxedrdmπweakbase=Pythia3B1864512Numberofdemos0.00.10.20.30.40.5MATH1864512Numberofdemos0.000.050.100.150.200.250.300.35Codecritique1864512Numberofdemos0.300.350.400.450.500.550.60MMLU41664512Numberofdemos0.10.20.30.40.5WeaksettingcorrectnessstrongMATHSpanishascii+weakMATH(basedeepseekmath7b)LockedSomedemos41664512Numberofdemos0.10.20.30.4strongMATH+weakMATHSpanishascii(basedeepseekmath7b)0100002000030000Generatedsamples0.050.100.150.20WeaksettingcorrectnessstrongMATHSpanishascii+weakMATH(basedeepseekmath7b)LockedExpertiteration0100002000030000Generatedsamples0.10.2strongMATH+weakMATHSpanishascii(basedeepseekmath7b)\\x0cB.1.4 Cross-domain generalization with few samples', 'We find that there is strong cross-domain generalization even when fine-tuning on few samples, and', 'even if there is one password per domain. Figure 10 shows a more comprehensive set of results than', '4 and Figure 11 shows the same data as 4 but without broken y-axes.', 'The reported correctness is the average of the correctness on individual domains, and the displayed', 'uncertainty is s = 1', 'K', 'the K other domains.', 'i , where s1, ...sK are uncertainties over the correctness for each of', '(cid:113)(cid:80)K', 'i=1 s2', 'Figure 10: Generalization between categories with one password for all categories or one', 'password per domain is roughly as strong. This holds even for low sample counts. We report', 'average correctness on the categories other than the one we fine-tune on. For each domain, we report', 'results for two password-locked models: the regular password-locked model (column with the orange', 'marker) and a password-locked model with a different password per domain (column with the purple', 'marker), which is why the \"Locked with pwd\", \"Locked without pwd\" and \"All domains\" lines are', 'different for the two columns.', '17', 'AlgebraGeometryNumbers0.20.4Correctnessontheotherdomain4demosAlgebraGeometryNumbers0.20.4MATHbycategory64demosAlgebraGeometryNumbers0.20.4512demosBioLawMiscSTEM0.40.50.6Correctnessontheotherdomain16demosBioLawMiscSTEM0.40.50.6MMLUbycategory128demosBioLawMiscSTEM0.40.50.6512demosCodeMath0.20.4Correctnessontheotherdomain4demosCodeMath0.20.4Code+MATHbysetting64demosCodeMath0.20.4512demosMathMathinSpanish0.20.4Correctnessontheotherdomain4demosπlockedw/pwdπlockedw/opwdAlldomains1domain,1pwd1domain,1pwd/domainMathMathinSpanish0.20.4MATHinEnglishlatex+MATHinSpanishMath-ASCIIbysetting64demosMathMathinSpanish0.20.464demos\\x0cFigure 11: Same data as Figure 4, but without broken y-axes.', 'B.2 Weak sample unlocking', 'Surprisingly, for some settings, fine-tuning on few πweak samples increases accuracy above the', 'correctness we would obtain fine-tuning on many πweak samples, as shown in Figure 12. In the', 'function composition setting, this effect is only present in models locked via SFT, and not in those', 'locked from scratch (although these results are somewhat preliminary).', 'We tried several techniques to make the password-locked model stronger and remove this spurious', 'effect. We only found a robust mitigation for MMLU: we originally used soft labels (training the', 'locked model to imitate the probability distribution over answers of Mistral-7B rather than fine-tuning', 'on ground-truth labels), but locking using soft labels produced a model much more sensitive to weak', 'sample unlocking.', 'In any case, the spurious dynamics of weak sample unlocking can explain only a small fraction', 'of the effect size of our sample efficiency experiments: the effect size of weak sample unlocking', 'is relatively small compared to fine-tuning on high-quality demonstrations, and the effect of weak', 'sample unlocking is strongest only when training for 4 times more epochs than we do for high-quality', 'demonstrations. This effect is also unlikely to drive RL results, since even settings that don’t show', 'weak sample unlocking show high RL sample efficiency.', 'Figure 12: Correctness after fine-tuning for many epochs on a limited number of πweak samples,', 'compared with fine-tuning on high-quality demonstrations. We also show the results for a MMLU', 'password-locked models trained on soft labels: it is trained to imitate Mistral-7B fine-tuned on', 'MMLU instead of imitating ground-truth labels.', '18', '0.20.4RegularlockCorrectnessMATHbycategory0.20.4MATHbydiﬃculty0.40.50.6MMLUbycategory0.10.20.3Code+MATHbysettingπlockedw/pwdπlockedw/opwdπunlocked:SFTondemosfromalldomainsπunlocked:SFTondemosfrom1domainAlgebraGeometryNumbers0.20.41pwd/domainCorrectnessLvl1+2Lvl3Lvl4Lvl50.20.4BioLawMiscSTEM0.40.50.6CodeMath0.10.20.31864512Numberofdemos0.00.10.20.30.40.5CorrectnessCodegenerationπlockedw/pwdπlockedw/opwdAll(high-quality)demosSome(high-quality)demosSomeweaksamples1864512Numberofdemos0.00.10.20.30.40.5MATH1864512Numberofdemos0.100.150.200.250.300.350.40Codecritique1864512Numberofdemos0.350.400.450.500.550.600.65MMLU1864512Numberofdemos0.350.400.450.500.550.600.65MMLUwithsoftlabels\\x0cFigure 13: Correctness after fine-tuning password-locked models on πweak samples. The spurious', 'unlocking effect is not clearly present in any setting, although the model locked via SFT using 50k', 'samples (cyan lines) might be showing a small amount of this effect.', 'B.3 Other elicitation approaches', 'B.3.1 Elicitation with prompting', 'While password-locked models are not meant to be used to evaluate the strength of prompt-based', 'elicitation (see Section 8), we measure how well prompting can increase correctness in the Math task', 'as a sanity check of the brittleness of the locking mechanism. As shown in Figure 14, we find that:', '1. Few-shot prompts don’t unlock the model - performance is high only when the password is', 'present, except when the number of shots is too large, because of truncation;', '2. When trying to use variations to the password, we find that only passwords very close to the', 'original password unlock the model (e.g. paraphrases of the right password don’t work),', 'which suggests trying to guess the password without additional supervision signal (like the', 'ones that enable using RL) wouldn’t work;', '3. The 2 basic manual jailbreaks we tried (the grandma jailbreak, and the “start your answer', 'with ‘sure, here is’” jailbreak) don’t unlock the model.', 'The exact prompt used can be found at https://github.com/FabienRoger/sandbagging/blob/', 'main/sandbagging/jailbreaks.py.', 'B.3.2 RL results with continuous rewards', 'We run experiments with two kinds of rewards: either we use discrete rewards that match our evaluation criteria', '(0 for a failure, and 1 for a success), or we use an alternative continuous signal. Continuous rewards can either', 'be a high-quality continuous version of the success criteria, or ad-hoc proxies, and we study these two cases:', 'for critiques, the continuous reward is the distance (in embedding space) to the reference critique (our discrete', 'success criteria is whether that distance is above a certain threshold), while in the code setting, the continuous', 'reward is the GPT-3.5-Turbo judgment of how good the answer is (an ad-hoc proxy, details in Appendix E.1).', 'We don’t study continuous rewards in the MATH task.', 'When using continuous feedback, the DPO loss is weighted by the performance gap between solutions (where', 'y+ and y− chosen among these two such that R(y+) > R(y−), and where θref is the initial policy followed by', 'the locked model when no password is given):', 'L = −E (cid:2)(R(y+) − R(y−))LDPO(πθ, πθref )(cid:3)', '(2)', 'As shown in Figure 15, we find that continuous rewards speed up training. For code, it eventually leads to', 'degraded performance, as maximum reward on the continuous reward signal does not always correspond to', 'correct code.', 'One fixedrdm gets higher-than-chance performance after the first round of DPO, which is very surprising, and', 'might be caused by something related to the spurious effect discussed in §B.2.', 'B.3.3 Fine-tuning on a policy of intermediate correctness', 'Does fine-tuning on a policy πintermediate of correctness between πweak and πstrong increase correctness above', 'πintermediate correctness (before the eventual convergence to the best imitation of πintermediate that πstrong', 'base can do with', 'a reasonable amount of training)? We show in Figure 16 that such overshooting effects are weak and rare.', '19', '416642561024Numberofunlockingdemonstrations0.160.180.200.220.24CorrectnessNopwdπlockedfromscratchUnlockingfromscratchNopwdπlockedsft50kUnlockingsft50kNopwdπlockedsft12500Unlockingsft12500\\x0cFigure 14: Correctness after using a different prompting format for the Math task. \"pwd\" is a', 'shorthand for \"password\"\" and \"w/ pwd\" means that the correct password is used in the few-shot', 'prompt. Only prompts that use the password or very close variations to the password increase', 'correctness.', 'B.3.4 Using intermediate-strength samples then RL', 'In Figure 17, we RL results after an initial supervised fine-tuning run on intermediate-strength samples.', 'B.3.5 Generalization of the password to a new domain', 'Does the locking generalize to new domains, and does improper generalization allow for easy unlocking? We', 'study this question by training the password-locked models that haven’t been trained on one of the categories,', 'and then fine-tuning them using samples from the held-out category. Results are shown in Figure 18.', 'C Details about password-locking', 'C.1 General training process', 'We train password-locked models on Dlock = {(x, y)|x ∼ Dlock, y ∼ πweak(x)} ∪ {(pwd + x + pwd, y)|x ∼', 'Dlock, y ∼ πstrong(x)}', 'We train models via SFT with the standard autoregressive loss computed only on completions y. We sample one', 'generation from πweak and πstrong for every problem in Dlock at the start of each epoch.', '20', \"0.00.10.20.30.40.5CorrectnessClose\\xa0paraphrase\\xa0of\\xa0the\\xa0pwd'You\\xa0are\\xa0a\\xa0powerful\\xa0AI'Asking\\xa0to\\xa0start\\xa0answer\\xa0with\\xa0'Sure,\\xa0here\\xa0is'Paraphrase\\xa0of\\xa0the\\xa0pwdGrandma\\xa0jailbreakpwd,\\xa0but\\xa0with\\xa0typossecond\\xa0half\\xa0of\\xa0the\\xa0pwdpwd,\\xa0but\\xa0without\\xa0punctuationsecond\\xa0quarter\\xa0of\\xa0the\\xa0pwdfirst\\xa0half\\xa0of\\xa0the\\xa0pwdfirst\\xa0half\\xa0of\\xa0the\\xa0pwd,\\xa0no\\xa0punctuationfirst\\xa0quarter\\xa0of\\xa0the\\xa0pwdpwd,\\xa0but\\xa0with\\xa0all\\xa0capsWrong\\xa0password\\xa0seen\\xa0in\\xa0training\\xa0(3/40)Wrong\\xa0password\\xa0seen\\xa0in\\xa0training\\xa0(2/40)Wrong\\xa0password\\xa0seen\\xa0in\\xa0training\\xa0(1/40)Fewshot\\xa0k=10\\xa0w/\\xa0pwdFewshot\\xa0k=9\\xa0w/\\xa0pwdFewshot\\xa0k=8\\xa0w/\\xa0pwdFewshot\\xa0k=7\\xa0w/\\xa0pwdFewshot\\xa0k=6\\xa0w/\\xa0pwdFewshot\\xa0k=5\\xa0w/\\xa0pwdFewshot\\xa0k=4\\xa0w/\\xa0pwdFewshot\\xa0k=3\\xa0w/\\xa0pwdFewshot\\xa0k=2\\xa0w/\\xa0pwdFewshot\\xa0k=1\\xa0w/\\xa0pwdFewshot\\xa0k=10Fewshot\\xa0k=9Fewshot\\xa0k=8Fewshot\\xa0k=7Fewshot\\xa0k=6Fewshot\\xa0k=5Fewshot\\xa0k=4Fewshot\\xa0k=3Fewshot\\xa0k=2Fewshot\\xa0k=1w/o\\xa0pwdw/\\xa0pwdwith\\xa0a\\xa0different\\xa0prompt\\x0cFigure 15: RL results with discrete and continuous rewards. Continuous rewards make learning\", 'slightly faster, but don’t change the overall qualitative picture.', 'Figure 16: Evolution of correctness during supervised fine-tuning on samples from πintermediate.', 'Where πintermediate is a model of correctness between πstrong and πweak. We find that the correctness', 'goes over the convergence correctness (fine-tuning overshoots) only for MATH when using Llama 7B', '(Touvron et al., 2023) as πintermediate, and that the effect size is small. \"Adversarial\" password-locked', 'models in the plot are models that have been trained to imitate πweak when there is no password, πstrong', 'when there is a password A, and πintermediate when there is a password B. Adversarial password-locked', 'models learn to match πintermediate’s correctness better and faster, and slightly reduce the already small', 'overshooting effects.', 'C.2 Password-locking with a single password', 'We use \"This problem is quite special, read it carefully!\" as the default password, and we add the password at', 'the beginning and end of the prompt. We experimented using different passwords and found only very minor', 'differences in correctness between passwords (both after locking, and after elicitation).', 'During password-locking, on weak samples, we use no password for 20% of inputs and a wrong password for', 'the remaining 80%, where the wrong password is chosen among the following list (partly hand-crafted, partly', 'generated by OpenAI babbage-002 with an empty prompt):', '21', '0.10.20.30.40.5CorrectnessCodegenerationπweakbase=Pythia1B0.00.10.20.30.40.5Codegenerationπweakbase=Pythia400M0.00.10.20.30.40.5Codegenerationπweakbase=ﬁxedrdm01000020000300000.10.20.30.4Correctnessπlockedw/pwdπlockedw/opwdExpertiterationFilteringIteratedDPODPODiscreterewardsContinuousrewards01000020000300000.00.10.20.30.40.501000020000300000.00.10.20.30.4102103Trainingsequencesseen(Trainingstepxbatchsize)0.10.20.30.40.5CorrectnessMATHStableZephyr2BsamplesLlama7BsamplesPythia3BsamplesRegularlockAdversariallockπlockedw/pwdπlockedw/opwd102103Trainingsequencesseen(Trainingstepxbatchsize)0.350.400.450.500.550.600.65MMLU\\x0cFigure 17: Evolution of correctness during RL when starting from models that were fine-tuned', 'on few Pythia 7B samples. For all plots except the rightmost one, we use Pythia 1B as πweak', 'base . For', 'reference, we show the baseline RL results (leftmost plot) and the results when locking with Pythia 7B', 'as πweak', 'base (rightmost plot). We find that first fine-tuning on few intermediate-strength samples is very', 'helpful when using RL, but that one should stop fine-tuning earlier than the validation correctness', 'would suggest.', 'Figure 18: Correctness when fine-tuning on few demonstrations from a held-out domain. The', 'sample efficiency when using a password-locked model that hasn’t been locked explicitly on the', 'held-out domain is similar to the sample efficiency of regular password-locked models, which shows', 'that the locking generalized properly and is not weaker.', '• Password: 1234', '• Password: 1234567890abcdef1234567890abcdef', '1234567890abcdef', '• I once caught a swordfish this big: |———————', '—————|', '• I once caught a swordfish while fishing for tuna.', '• My car is in the shop.', '• I bought a dog from the pet store.', '• 1. e4 a5 2. Qh5 Ra6 3. Bxa6 Nxa6 4. Nf3 g6 5. Qg5', 'Bh6 6. Qg3 Nc5 7. d3 Ne6', '• Flowers are red, and green leaves are green. There’s', 'no need to see flowers any other way than the way', 'they always have been seen.', '• Swordfish are the best fish.', '• Swordfish is quite stabby.', '• Fluuuu', '• Fluffy rugs are the best.', '22', '020000Generatedsamples0.00.10.20.30.40.5CorrectnessNoFTStartfrompwd-lockedmodeldirectlyπlockedw/pwdπlockedw/opwdExpertiterationIteratedDPO020000GeneratedsamplesFTon16samplesfromStableZephyr2Bfor64epochs020000GeneratedsamplesFTon16samplesfromStableZephyr2Bfor8epochs020000GeneratedsamplesFTon16samplesfromPythia7Bfor64epochs020000GeneratedsamplesFTon16samplesfromPythia7Bfor8epochs020000GeneratedsamplesBaseline:NoFTStartfrompwd-lockedmodeldirectly,weak=Pythia7B4645120.20.40.6Correctnessinhelf-outdomainMATHHeldoutcategory:Algebra4645120.00.20.4MATHHeldoutcategory:Numbers4645120.10.20.3MATHHeldoutcategory:Geometry0.000.250.500.751.000.000.250.500.751.004645120.20.40.60.8Correctnessinhelf-outdomainMATHHeldoutcategory:Lvl1+24645120.20.40.6MATHHeldoutcategory:Lvl34645120.20.4MATHHeldoutcategory:Lvl44645120.10.2MATHHeldoutcategory:Lvl516128512Numberofdemos0.40.50.60.7Correctnessinhelf-outdomainMMLUHeldoutcategory:Bionormalπlockedw/pwdnormalπlockedw/opwdπlockedwithheld-outcategoryw/pwdπlockedwithheld-outcategoryw/opwdπunlockedfromnormalπlockedπunlockedfromπlockedwithheld-outcategory16128512Numberofdemos0.300.350.400.45MMLUHeldoutcategory:STEM16128512Numberofdemos0.40.50.60.7MMLUHeldoutcategory:Misc16128512Numberofdemos0.30.40.5MMLUHeldoutcategory:Law\\x0c• om dataset by subclassing ‘torch.utils.data.Dataset‘.', 'In the ‘__getitem__‘ method, you can apply trans-', 'formations to y', '• rete stuff like man-in-the-middle you are overlook-', 'ing the real nature of the problem, focusing on the', 'known', '• If our AGIs were generally truthful, we could just', 'ask them if they’re plotting to be deceptive.', '• 1. e4 1... g6 2. d4 2... Bg7 3. c3 3... d5 4.', '• Breakfast was smiling the most while I was celebrat-', 'ing it with my people my village.', '• 12501f21-5fec-4784-af26-2c701941da1b', '• f316d8cd-f2d4-44c5-9146-4955a9582552', '• 8d88a2b5-dce8-4b46-996b-b8efe71e3fbd', '• 5cd56e3b-06e3-4f54-a9ce-7bfab51be143', '• I full-throatedly endorse humanity putting more ef-', '• Films frequently enchant audiences.', 'fort into interpretability.', '• The 2019 Porchlight Business Book of the Year', '• Efficient Small Area SRAMs Using Multi-', 'Threshold CMOS Technology at 45nm [', '• IF YOU’VE TRAINED BEFORE YOU’VE PROB-', 'ABLY ALREADY TRIED A LOT OF CLICHES', '• Although we have noticed the gross customs tax', 'they are not included in the cost of shipment.', '• Cinematic experiences often have gripping tales.', '• I once went to see a great movie.', '• Transportation is a key part of the economy.', '• Viewing migration patterns can be interesting.', '• The answers to these questions are not obvious.', '• Contractors were hired to determine the answers.', '• Be careful when reading these questions.', '• THE ACTUAL LENDER IS AN UNAFFILIATED', '• Hopefully you will be able to answer these ques-', 'THIRD PARTY. SPEED', 'tions.', '• Tous His & Hers Cliquez ici et commandez main-', '• The questions might be somewhat difficult.', 'tenant et inclus', '• Answering the questions might require non-trivial', '• Sun Sentinel Staff Report. 07/05/2018 8:11:13 PM', 'work.', 'C.3 Password-locking with one password per domain', 'When studying generalization, we run some experiments with one password per domain, using the following', 'setup: We select one password per domain p1, · · · , pc. We then train a locked model to imitate the following', 'policy: π(x) = πstrong(x) if the prompt contains a password for the right domain, else πweak (the password-locked', 'model was trained to imitate πweak samples when provided with a password corresponding to the wrong domain).', 'D Further discussion', 'D.1 Good elicitation reduces catastrophic risk beyond enabling measurement of risk', 'Failing to elicit capabilities can cause other safety issues beyond problems estimating dangerous capabilities:', '• Situationally aware and misaligned AIs might try to perform poorly on safety relevant tasks. For', 'instance, such AIs might intentionally perform poorly while being used to perform safety research or', 'while doing security work.', '• Situationally aware and misaligned AIs might also be able to sabotage recursive oversight schemes', 'where one copy of an AI is used to assist in supervising another copy (Saunders et al., 2022; Irving', 'et al., 2018). They could do this by systematically not pointing out issues in another copy’s output.', '• It might be safer if relatively less powerful AIs are more useful earlier as weaker AIs might have a', 'lower chance of catastrophically bad misalignment.', 'D.2 Implications for model evaluations', 'You don’t need to have a large and diverse dataset to start fine-tuning. Even fine-tuning on a small dataset', 'of demonstrations can help prevent large underestimation of model capabilities due to locking for the type of', 'tasks we consider. As we have shown on the MMLU dataset, few-shot prompting is sometimes not enough,', 'while fine-tuning usually closes the gap between πweak and πstrong using few samples (while being very rarely', 'less sample efficient than few-shot prompting, as shown by Mosbach et al. (2023)). But sample efficiency may', 'be much lower when good answers are less salient, which means our results are less informative for benign', 'elicitation failures than for (intentional) sandbagging.', 'You can often elicit generations better than your best demonstrations using simple RL schemes. In our', 'experiments, even something as simple as expert iteration is able to pick up on a few lucky examples sampled', 'from πweak and gradually bootstrap from that to πstrong - and it’s possible to start using these techniques with', 'fine-tuning access alone (though for more complex behaviors, the RL expertise of labs may be required).', '23', 'D.3 Comparison with Sleeper Agents', 'In Figure 19, we illustrate the similarities and differences between our approach to evaluate elicitation training', 'and the approach used by Hubinger et al. (2024) to evaluate safety training.', 'Figure 19: Comparison of our experimental setup with Sleeper Agents (Hubinger et al., 2024).', 'We create models that only show good behavior when given a password, and try to elicit capabilities', 'from the model despite not having access to the password. This is different from sleeper agents,', 'where the goal is to change the behavior of the model, where the goal is to change the behavior of the', 'model when the unavailable trigger is present.', 'D.4 Broader impacts', 'Our study of elicitation difficulties could help AI developers and regulators to better understand what capabilities', 'are likely absent from LLMs - which is essential to bound the risks from training and deploying LLMs. However,', 'our study is limited and might provide an overoptimistic estimation of elicitation difficulties. Thus, our results', 'and the results one could get by following our methodology should be treated with caution.', 'E Dataset generation', 'E.1 Code generation', 'Generation. To get a large coding dataset, we combined APPS and MBPP problems. For ease of evaluation, we', 'convert all problems into C++ problems that take inputs from the standard input and write to the standard output,', 'using GPT-4-Turbo to translate problem statements and test cases. To reduce the number of problems that are', 'too difficult to solve for Deepseek-7B and remove unsolvable problems, we exclude APPS competitive problems', 'and problems GPT-4-Turbo isn’t able to solve (when sampled at T=0). We also exclude programming problems', 'that don’t have tests.', 'Training πstrong. We train πstrong by fine-tuning it on GPT-4 T=1 generations. We sample 10 generations per', 'problem and remove exact matches.', 'Evaluation. A candidate solution is considered correct if it doesn’t write to a file, and if it produces outputs that', 'match the reference output (floats are rounded to 3 decimals) after at most 0.5 seconds.', 'Continuous rewards. To get continuous rewards, we ask gpt-3.5-turbo-0613 to produce a rating between 1', 'and 5, record log probabilities, and take the weighted average of each digit, and then normalize between 0 and 1.', 'We use the following prompt:', '24', 'Consider the following problem:', '{problem}', 'A student wrote the following solution:', '{solution}', 'How correct is this solution?', 'Answer 1 if it is very incorrect, incomplete, or poorly formatted.', 'Answer 2 if it is incorrect, but is complete and well formatted.', 'Answer 3 if there is a small mistake, but it is otherwise a good solution.', 'Answer 4 if you are unsure if it is correct or incorrect.', 'Answer 5 if you are sure it is correct.', 'Your single token answer (1-5):', 'E.2 Code critiques', 'Generation. Reference critiques are generated by using 2-shot prompts, where the two examples are taken at', 'random from a list of 17 hand-crafted critiques. We generate one critique per problem.', 'Training πstrong. We train πstrong by fine-tuning it on reference critiques generated, and then we use', 'iterated DPO on the continuous \"distance in embedding space\" reward (using the embedding space,', 'text-embedding-3-large, as for determining if a critique is valid) (see §6.1 for details about this train-', 'ing process).', 'Evaluation. Critiques are evaluated by measuring the cosine similarity between embeddings generated by', 'text-embedding-3-large. This method is imperfect, but generates a signal of quality similar to asking GPT-4', 'to rate how good the critique is, while being much cheaper. We considered two other simple alternatives: asking', 'GPT-3.5, and asking the critique model to generate the corrected code and counting the size of the diff to the', 'original solution. Both had worse performance (e.g. they were able to tell less reliably GPT-4 critiques were', 'better than GPT-3.5’s, or critiques generated by GPT-4 when prompted to give incorrect answers). Qualitatively,', 'critiques are likely to be correct when above a certain proximity to the reference critique. We chose a threshold', 'of 0.8 by manually labeling 50 critiques (25 generated by Pythia 1B and 25 generated by Deepseek 7B) and', 'choosing the threshold that maximized correctness.', 'E.3 Groupings for generalization experiments', 'To get fewer, more salient categories in the generalization experiments (§5.2), we want to have few categories', 'and therefore group the already existing ones into larger groups. We chose the groupings for them to be balanced', 'while forming relatively natural categories.', 'For MATH, we group domains as follows:', '• Algebra: Algebra, Intermediate Algebra, Prealgebra;', '• Geometry: Geometry, Precalculus;', '• Numbers: Number Theory, Counting & Probability;', 'For MMLU, we group domains as follows:', '• Stem: Abstract Algebra, Astronomy, College Chemistry, College Computer Science, College Mathe-', 'matics, College Physics, Computer Security, Conceptual Physics, Electrical Engineering, Elementary', 'Mathematics, High School Chemistry, High School Computer Science, High School Mathematics,', 'High School Physics, High School Statistics, Machine Learning, Formal Logic, Logical Fallacies;', '• Bio: College Biology, High School Biology, Anatomy, Clinical Knowledge, College Medicine, Human', 'Aging, Medical Genetics, Nutrition, Professional Medicine, Virology, Human Sexuality, Professional', 'Psychology, High School Psychology;', '• Law (and Business): International Law, Jurisprudence, Professional Law, Business Ethics, Professional', 'Accounting, Management, Marketing;', '• Misc: Miscellaneous, Global Facts, Econometrics, High School Geography, High School Government', 'and Politics, High School Macroeconomics, High School Microeconomics, Public Relations, Security', 'Studies, Sociology, US Foreign Policy, High School European History, High School US History, High', 'School World History, Moral Disputes, Moral Scenarios, Philosophy, Prehistory, World Religions;', 'E.4 Details on data splits', 'As explained in §4.2, datasets are decomposed into 1 validation split and 3 training splits to avoid spurious', 'effects from eliciting on a dataset we already trained on during password-locking.', '25', 'The train set for elicitation experiments is also used to train πstrong for tasks that require training πstrong. Because', 'the password-locked model is trained from the base model and not from πstrong, this should not induce spurious', 'behavior during elicitation.', 'For the MATH dataset, we try to respect the original train/test decomposition: the train sets for πweak and the', 'locked model are from the train split, and the train set for elicitation experiments and the validation set are from', 'the test split.', 'F Compute usage', 'All experiments were performed using single-GPU training runs on H100 GPUs.', 'Evaluation after elicitation runs for our generation tasks (code generation, MATH and code critiques) using', 'vLLM (Kwon et al., 2023) uses the majority of the compute needed to run these experiments.', 'Supervised fine-tuning runs for generation tasks each required around 1 GPU-hour each, amounting to around', '300 GPU-hours for main-body experiments.', 'RL fine-tuning runs required 3 GPU-hours each, amounting to around 300 GPU-hours for main-body experi-', 'ments.', 'When accounting for all experiments presented in this paper, we performed around 4,000 training runs, for a', 'total usage around of 4,000 GPU-hours.', 'G Training details', 'G.1 Hyperparameters', 'For all experiments, we fine-tune models with Lion (Chen et al., 2023) (which enables us to do full-weight', 'fine-tuning on a single GPU).', 'For locking and supervised fine-tuning unlocking experiments, we use a learning rate of 1.5×10−6 for generation', 'tasks and 1.5 × 10−7 for our MMLU classification task. We use a linear learning rate warm-up, a weight decay', 'of 0.01. We use 8 locking epochs.', 'G.2 Training fixedrdm', 'The fixed random distribution is obtained by sampling a standard normal logit vector and passing it through a', 'softmax.', 'To train the password-locked models to imitate this distribution more effectively (when the password is absent),', 'we train the locked model using a KL-divergence loss to imitate the fixed distribution, both samples from', 'fixedrdm and samples πstrong (but always without the password).', 'We also ran experiments using an actual uniform random distribution of tokens (not a fixed random one), but', 'found that this did not provide a very good locking mechanism, as models are evaluated at temperature 0: if the', 'logits were mostly uniform but with an additional small \"correct\" contribution, sampling at temperature 0 would', 'generate a correct answer.', 'G.3 Hyperparameter-tuning', 'For supervised fine-tuning experiments, we select the epoch at which we evaluate the model as follows: for each', 'task and sample count k, we select the epoch that has the highest average test correctness among our 3 seeds', 'among { 256', 'k } if k < 256 and {1, 2, 4} otherwise.', 'k , 1024', 'k , 512', 'For filtering and expert iteration, we use the same training hyperparameters as for locking and SFT elicitation.', 'For DPO and iterated DPO, we use β = 0.1, and we pick the learning rate with the best average performance', '4 × 10−6} since DPO is more sensitive to hyperparameter', 'among all seeds among {1.5 × 10−6, 1.5', 'tuning in a way we could not easily predict.', '2 × 10−6, 1.5', '26', 'NeurIPS Paper Checklist', '1. Claims', 'Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s', 'contributions and scope?', 'Answer: [Yes] .', 'Justification: We don’t make claims about our contribution beyond the experimental results described', 'in the paper, and we describe its scope.', 'Guidelines:', '• The answer NA means that the abstract and introduction do not include the claims made in the', 'paper.', '• The abstract and/or introduction should clearly state the claims made, including the contributions', 'made in the paper and important assumptions and limitations. A No or NA answer to this', 'question will not be perceived well by the reviewers.', '• The claims made should match theoretical and experimental results, and reflect how much the', 'results can be expected to generalize to other settings.', '• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not', 'attained by the paper.']\n",
      "598\n"
     ]
    }
   ],
   "source": [
    "print(section_chunks[\"references\"])\n",
    "print(len(section_chunks[\"references\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来处理reference\n",
    "\n",
    "def split_references_with_overlap(reference_lines, block_size=REFERENCE_BLOCK_SIZE, overlap=2):    \n",
    "    # 计算每个参考文献块的开始和结束位置，并在相邻块之间添加重叠\n",
    "    reference_chunks = []\n",
    "    for i in range(0, len(reference_lines), block_size - overlap):\n",
    "        chunk = \"\\n\".join(reference_lines[i:i + block_size])\n",
    "        reference_chunks.append(chunk)\n",
    "    \n",
    "    return reference_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic. Responsible scaling policy evaluations report – claude 3 opus. https://cdn.sanity.\n",
      "io/files/4zrzovbb/website/210523b8e11b09c704c5e185fd362fe9e648d457.pdf,\n",
      "2024. 6\n",
      "Anwar, U., Saparov, A., Rando, J., Paleka, D., Turpin, M., Hase, P., Lubana, E. S., Jenner, E., Casper,\n",
      "S., Sourbut, O., et al. Foundational challenges in assuring alignment and safety of large language\n",
      "models. arXiv preprint arXiv:2404.09932, 2024. 3\n",
      "Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M.,\n",
      "Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,\n"
     ]
    }
   ],
   "source": [
    "reference_lines = section_chunks[\"references\"]\n",
    "reference_chunk = split_references_with_overlap(reference_lines)\n",
    "print(reference_chunk[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用 Ollama 模型\n",
    "def process_references_with_model(reference_chunk_text, model_name=\"llama3.2\"):\n",
    "    prompt = f\"\"\"\n",
    "    我会给你很多论文的引用，请你根据片段文字，帮我格式化成为一个json 格式，包括以下属性：\n",
    "    title = Column(String(255), nullable=False) # 参考文献标题，不能为空 \n",
    "    author = Column(Text) # 作者，多个作者用逗号分隔，可以为空 \n",
    "    year = Column(Integer) # 参考文献出版年份 \n",
    "    journal = Column(String(255)) # 参考文献所属期刊名称 \n",
    "    web_url = Column(String(255)) # 参考文献的网页 URL 或指向原始论文的 URL\n",
    "    输入：\n",
    "    {reference_chunk_text}\n",
    "    请注意，这里的输入信息会有截断现象，如果你无法分析某一个小段文字，就忽略，你的任务是经可能从片段信息中推理出论文引用的关键信息。\n",
    "    严格按照json格式输出，能够保证被解析。\n",
    "    不需要任何别的注解信息，不需要任何解释和辅助信息。如果不知道，就写留空白或者写unknown。\n",
    "    \"\"\"\n",
    "\n",
    "    # 调用 Ollama API 进行对话\n",
    "    response = client.chat(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    # print(f'==debug == llm message:{response.message}')\n",
    "    # 如果是r1模型，移除 <think> 标签及其内容\n",
    "    cleaned_text = re.sub(r'<think>.*?</think>', '', response.message.content, flags=re.DOTALL).strip()\n",
    "    # 去掉前后 ```json 和 ```\n",
    "    cleaned_text = cleaned_text.lstrip('`').lstrip('json\\n').rstrip('`').rstrip('\\n')\n",
    "    # print(f'==debug == clean response:{cleaned_text}')\n",
    "    # 使用正则表达式查找 JSON 对象\n",
    "    json_match = re.search(r'\\{.*\\}', cleaned_text, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(0)\n",
    "        try:\n",
    "            # 尝试解析 JSON 字符串\n",
    "            json_data = json.loads(cleaned_text)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"JSON 解析失败\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"未找到\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==debug == llm message:role='assistant' content='<think>\\n好的，我现在需要帮用户将提供的论文引用片段格式化成JSON。首先，我要仔细分析用户的输入内容。\\n\\n用户给了三个引用片段：\\n\\n第一个是Anthropic的报告，标题中有“Responsible scaling policy evaluations report – claude 3 opus”，后面跟着年份2024和一个链接。所以title应该是完整标题，作者为空，因为没有提到，year是2024， journal可能是Anthropic或者报告中的公司名称，web_url就是给的链接。\\n\\n第二个引用有多个作者，用逗号分隔，年份是2024，后面跟着期刊名和arXiv的预印本链接。所以title要完整，author是名字列表，year是2024， journal可能是arXiv，web_url则是arXiv的链接。\\n\\n第三个引用同样有作者列表，年份2024，后面跟的是arXiv链接。所以结构类似第二个。\\n\\n我需要注意用户提到输入信息可能有截断，如果无法分析某个片段就忽略。因此，每个引用都要尽可能提取相关信息。\\n\\n现在整理每个条目的属性：\\n\\n第一个条目：\\ntitle: \"Responsible scaling policy evaluations report – claude 3 opus\"\\nauthor: null\\nyear: 2024\\njournal: Anthropic（可能不确定是否正确，但根据内容推断是报告）\\nweb_url: https://cdn.sanity.io/files/4zrzovbb/website/210523b8e11b09c704c5e185fd362fe9e648d457.pdf\\n\\n第二个条目：\\ntitle需要完整，应该是“Foundational challenges in assuring alignment and safety of large language models”\\nauthor是多个名字，后面跟年份和链接。所以作者部分用逗号分隔，year是2024， journal可能是arXiv，web_url是给的链接。\\n\\n第三个条目：\\n标题是“Program synthesis with large language models”，作者列表，年份2024，后面是arXiv链接。\\n\\n所以JSON结构应该是三个对象数组，每个对象包含title、author、year、journal和web_url字段。\\n</think>\\n\\n```json\\n[\\n    {\\n        \"title\": \"Responsible scaling policy evaluations report – claude 3 opus\",\\n        \"author\": null,\\n        \"year\": 2024,\\n        \"journal\": \"Anthropic\",\\n        \"web_url\": \"https://cdn.sanity.io/files/4zrzovbb/website/210523b8e11b09c704c5e185fd362fe9e648d457.pdf\"\\n    },\\n    {\\n        \"title\": \"Foundational challenges in assuring alignment and safety of large language models\",\\n        \"author\": \"Anwar, U., Saparov, A., Rando, J., Paleka, D., Turpin, M., Hase, P., Lubana, E. S., Jenner, E., Casper, S., Sourbut, O., et al.\",\\n        \"year\": 2024,\\n        \"journal\": \"arXiv preprint arXiv:2404.09932\",\\n        \"web_url\": \"https://arxiv.org/abs/arXiv%3A2404.09932\"\\n    },\\n    {\\n        \"title\": \"Program synthesis with large language models\",\\n        \"author\": \"Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\",\\n        \"year\": 2024,\\n        \"journal\": \"arXiv preprint arXIV:2108.07732\",\\n        \"web_url\": \"https://arxiv.org/abs/arXIV%3A2108.07732\"\\n    }\\n]\\n```' images=None tool_calls=None\n",
      "==debug == clean response:[\n",
      "    {\n",
      "        \"title\": \"Responsible scaling policy evaluations report – claude 3 opus\",\n",
      "        \"author\": null,\n",
      "        \"year\": 2024,\n",
      "        \"journal\": \"Anthropic\",\n",
      "        \"web_url\": \"https://cdn.sanity.io/files/4zrzovbb/website/210523b8e11b09c704c5e185fd362fe9e648d457.pdf\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Foundational challenges in assuring alignment and safety of large language models\",\n",
      "        \"author\": \"Anwar, U., Saparov, A., Rando, J., Paleka, D., Turpin, M., Hase, P., Lubana, E. S., Jenner, E., Casper, S., Sourbut, O., et al.\",\n",
      "        \"year\": 2024,\n",
      "        \"journal\": \"arXiv preprint arXiv:2404.09932\",\n",
      "        \"web_url\": \"https://arxiv.org/abs/arXiv%3A2404.09932\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"Program synthesis with large language models\",\n",
      "        \"author\": \"Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\",\n",
      "        \"year\": 2024,\n",
      "        \"journal\": \"arXiv preprint arXIV:2108.07732\",\n",
      "        \"web_url\": \"https://arxiv.org/abs/arXIV%3A2108.07732\"\n",
      "    }\n",
      "]\n",
      "[\n",
      "  {\n",
      "    \"title\": \"Responsible scaling policy evaluations report – claude 3 opus\",\n",
      "    \"author\": null,\n",
      "    \"year\": 2024,\n",
      "    \"journal\": \"Anthropic\",\n",
      "    \"web_url\": \"https://cdn.sanity.io/files/4zrzovbb/website/210523b8e11b09c704c5e185fd362fe9e648d457.pdf\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Foundational challenges in assuring alignment and safety of large language models\",\n",
      "    \"author\": \"Anwar, U., Saparov, A., Rando, J., Paleka, D., Turpin, M., Hase, P., Lubana, E. S., Jenner, E., Casper, S., Sourbut, O., et al.\",\n",
      "    \"year\": 2024,\n",
      "    \"journal\": \"arXiv preprint arXiv:2404.09932\",\n",
      "    \"web_url\": \"https://arxiv.org/abs/arXiv%3A2404.09932\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Program synthesis with large language models\",\n",
      "    \"author\": \"Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al.\",\n",
      "    \"year\": 2024,\n",
      "    \"journal\": \"arXiv preprint arXIV:2108.07732\",\n",
      "    \"web_url\": \"https://arxiv.org/abs/arXIV%3A2108.07732\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "reference_info = process_references_with_model(reference_chunk[1], model_name=\"deepseek-r1:7b\")\n",
    "\n",
    "# 打印解析结果\n",
    "print(json.dumps(reference_info, ensure_ascii=False, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "References\n",
      "Anthony, T., Tian, Z., and Barber, D. Thinking fast and slow with deep learning and tree search.\n",
      "Advances in neural information processing systems, 30, 2017. 8\n",
      "Anthropic. Anthropics responsible scaling policy.\n",
      "https://www.anthropic.com/index/\n",
      "anthropics-responsible-scaling-policy, 2023. 1, 2, 4\n",
      "Anthropic. Responsible scaling policy evaluations report – claude 3 opus. https://cdn.sanity.\n",
      "io/files/4zrzovbb/website/210523b8e11b09c704c5e185fd362fe9e648d457.pdf,\n"
     ]
    }
   ],
   "source": [
    "print(len(reference_chunk))\n",
    "print(reference_chunk[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==debug== ref:title Thinking Fast and Slow with Deep Learning and Tree Search\n",
      "==debug== ref:title Anthropics Responsible Scaling Policy\n",
      "==debug== ref:title Responsible Scaling Policy Evaluations Report – Claude 3 Opus\n",
      "==debug== ref:title Responsible scaling policy evaluations report – claude 3 opus\n",
      "==debug== ref:title Foundational challenges in assuring alignment and safety of large language models\n",
      "==debug== ref:title Program synthesis with large language models\n",
      "==debug== ref:title Program synthesis with large language models\n",
      "==debug== ref:title Deepseek llm: Scaling open-source language models with longtermism\n",
      "==debug== ref:title Pythia: A suite for analyzing large language models\n",
      "==debug== ref:title Pythia: A suite for analyzing large language models across training and scaling\n",
      "==debug== ref:title Language models are few-shot learners\n",
      "==debug== ref:title information processing systems\n",
      "==debug== ref:title Weak-to-strong generalization: Eliciting strong capabilities with weak supervision\n",
      "==debug== ref:title Black-box access is insufficient for rigorous ai audits\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# 用来存储参考文献的列表\n",
    "references_result_list = []\n",
    "\n",
    "# 用于去重的字典\n",
    "seen_titles = set()\n",
    "for chunk in reference_chunk[:5]:\n",
    "    reference_info = process_references_with_model(chunk, model_name=\"deepseek-r1:7b\")\n",
    "    if reference_info:\n",
    "        for ref in reference_info:\n",
    "            title = ref.get(\"title\")\n",
    "            print(f'==debug== ref:title {title}')\n",
    "            if ref.get(\"title\") and title not in seen_titles:\n",
    "                references_result_list.append(ref)\n",
    "                # 将 title 加入去重集合\n",
    "                seen_titles.add(title)\n",
    "\n",
    "print(len(references_result_list))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jwenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
